import os
import uuid
import datetime
import streamlit as st
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
import io
from logic import (
    USAGE_FILE, SESSIONS_FILE, MAX_BUDGET_USD, PRICING,
    VERTEX_PROJECT, VERTEX_LOCATION,
    load_usage, save_usage, calculate_cost, get_mime_type,
    extract_youtube_id, get_youtube_transcript, get_relevant_context,
    extract_text_from_response, load_sessions, save_sessions, get_client,
    load_user_profile, save_user_profile, update_user_profile_from_conversation,
    build_full_session_memory
)

try:
    from st_img_pastebutton import paste
    import_error_msg = None
except ImportError as e:
    paste = None
    import_error_msg = str(e)

# =========================
# ç’°å¢ƒå¤‰æ•° & å®šæ•°
# =========================

load_dotenv()



st.set_page_config(page_title="Gemini 3 Web Studio", layout="wide")

# ğŸ” ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ­ãƒƒã‚¯ + URLãƒˆãƒ¼ã‚¯ãƒ³æ°¸ç¶šåŒ–
# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
try:
    if "APP_PASSWORD" in st.secrets:
        APP_PASSWORD = st.secrets["APP_PASSWORD"]
        SECRET_TOKEN = st.secrets.get("SECRET_TOKEN", "access_granted_default")
    else:
        APP_PASSWORD = os.getenv("APP_PASSWORD", "198501")  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé–‹ç™ºç”¨ï¼‰
        SECRET_TOKEN = os.getenv("SECRET_TOKEN", "access_granted_198501")
except:
    APP_PASSWORD = os.getenv("APP_PASSWORD", "198501")
    SECRET_TOKEN = os.getenv("SECRET_TOKEN", "access_granted_198501")

# 1. URLãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
query_params = st.query_params
url_token = query_params.get("auth", None)

if url_token == SECRET_TOKEN:
    st.session_state.authenticated = True
elif "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# 2. æœªèªè¨¼ãªã‚‰ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ç”»é¢
if not st.session_state.authenticated:
    st.title("Gemini 3 Studio")
    st.write("ã“ã®ã‚¢ãƒ—ãƒªã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™ã€‚")

    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")

    if st.button("ãƒ­ã‚°ã‚¤ãƒ³"):
        if password == APP_PASSWORD:
            st.session_state.authenticated = True
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¿®æ­£: URLã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‰Šé™¤ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶å±¥æ­´/ã‚¹ã‚¯ã‚·ãƒ§æ¼æ´©é˜²æ­¢ï¼‰
            st.query_params.clear()
            st.rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™ã€‚")

    st.stop()

# =========================
# Early Gemini Client Function (for recommendations before main init)
# =========================
@st.cache_resource
def get_gemini_client():
    """
    Gemini ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆStreamlit Secretså¯¾å¿œï¼‰
    
    Streamlit Cloud: st.secretsã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼æƒ…å ±ã‚’ä½¿ç”¨
    ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º: Application Default Credentials
    """
    try:
        # Get project ID from environment variable or secrets
        project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
        
        # Streamlit Cloud: Service Account via secrets
        if "GOOGLE_CREDENTIALS" in st.secrets:
            from google.oauth2 import service_account
            creds_dict = dict(st.secrets["GOOGLE_CREDENTIALS"])
            
            # Use project_id from credentials if not set
            if not project_id:
                project_id = creds_dict.get("project_id")
            
            scoped_creds = service_account.Credentials.from_service_account_info(
                creds_dict,
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            
            print(f"[DEBUG] Using secrets auth with project_id: {project_id}")
            
            return genai.Client(
                vertexai=True,
                project=project_id,
                location=VERTEX_LOCATION,
                credentials=scoped_creds
            )
        else:
            # No secrets - use environment variables
            print(f"[DEBUG] No GOOGLE_CREDENTIALS in secrets, using env vars")
            
            if not project_id:
                raise ValueError("GOOGLE_CLOUD_PROJECT environment variable is required")
            
            # Check if we have service account JSON in env
            if "GOOGLE_APPLICATION_CREDENTIALS_JSON" in os.environ:
                import json
                from google.oauth2 import service_account
                
                creds_json = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON")
                creds_dict = json.loads(creds_json)
                
                scoped_creds = service_account.Credentials.from_service_account_info(
                    creds_dict,
                    scopes=["https://www.googleapis.com/auth/cloud-platform"]
                )
                
                print(f"[DEBUG] Using env JSON auth with project_id: {project_id}")
                
                return genai.Client(
                    vertexai=True,
                    project=project_id,
                    location=VERTEX_LOCATION,
                    credentials=scoped_creds
                )
            else:
                # Application Default Credentials
                print(f"[DEBUG] Using ADC with project_id: {project_id}")
                
                return genai.Client(
                    vertexai=True,
                    project=project_id,
                    location=VERTEX_LOCATION,
                )
    except Exception as e:
        print(f"âŒ Gemini ClientåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None

# =========================
# Helper Functions
# =========================

# =========================
# Helper Functions
# =========================
# Moved to logic.py

# =========================
# Grok Review Function (OpenRouter API)
# =========================

import requests
# curl_cffi ã¯æœªä½¿ç”¨ã®ãŸã‚å‰Šé™¤ï¼ˆPuterå»ƒæ­¢ã«ä¼´ã„ä¸è¦ï¼‰
import textwrap

def wrap_recommendation_text(text, width=20):
    """
    æ¨è–¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šå¹…ã§è‡ªå‹•æ”¹è¡Œï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼è¡¨ç¤ºç”¨ï¼‰
    è¦‹å‡ºã—ã‚„é‡è¦ãªè¡Œã¯ä¿æŒã—ã€æœ¬æ–‡ã®ã¿æ”¹è¡Œ
    
    Args:
        text: æ”¹è¡Œã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        width: 1è¡Œã®æœ€å¤§æ–‡å­—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ20æ–‡å­—ï¼‰
    
    Returns:
        æ”¹è¡Œå‡¦ç†ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    lines = text.split('\n')
    wrapped_lines = []
    
    for line in lines:
        stripped = line.strip()
        
        # ç©ºè¡Œã¯ãã®ã¾ã¾ä¿æŒ
        if not stripped:
            wrapped_lines.append(line)
            continue
        
        # è¦‹å‡ºã—è¡Œï¼ˆ#å§‹ã¾ã‚Šã€**å›²ã¿ã€æ•°å­—.å§‹ã¾ã‚Šï¼‰ã¯æ”¹è¡Œã—ãªã„
        if (stripped.startswith('#') or 
            stripped.startswith('**') or 
            (len(stripped) > 2 and stripped[0].isdigit() and stripped[1] == '.')):
            wrapped_lines.append(line)
            continue
        
        # çŸ­ã„è¡Œï¼ˆwidthä»¥ä¸‹ï¼‰ã¯ãã®ã¾ã¾ä¿æŒ
        if len(stripped) <= width:
            wrapped_lines.append(line)
            continue
        
        # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ãï¼‰ã‚’ä¿æŒ
        indent = len(line) - len(line.lstrip())
        indent_str = ' ' * indent
        
        # é•·ã„æœ¬æ–‡ã®ã¿æ”¹è¡Œå‡¦ç†
        wrapped = textwrap.fill(
            stripped, 
            width=width,
            break_long_words=True,
            break_on_hyphens=True,
            initial_indent=indent_str,
            subsequent_indent=indent_str + '  '
        )
        wrapped_lines.append(wrapped)
    
    return '\n'.join(wrapped_lines)

# OpenRouter API Keyã®å–å¾— (st.secretså„ªå…ˆã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°)
try:
    if "OPENROUTER_API_KEY" in st.secrets:
        OPENROUTER_API_KEY = str(st.secrets["OPENROUTER_API_KEY"]).strip()
        if not OPENROUTER_API_KEY:  # ç©ºæ–‡å­—ãªã‚‰envå¤‰æ•°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")
    else:
        OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")
except Exception as e:
    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º
    # st.error(f"OPENROUTER_API_KEYèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")

# Puterèªè¨¼æƒ…å ±ã¯å‰Šé™¤ï¼ˆAWS Bedrockã«ç§»è¡Œï¼‰


# â–¼â–¼â–¼ AWS Bedrock (Claude 4.5 Sonnetç”¨) â–¼â–¼â–¼
try:
    import boto3
    HAS_BOTO3 = True
except ImportError:
    HAS_BOTO3 = False

# AWSèªè¨¼æƒ…å ±å–å¾—
try:
    if "AWS_ACCESS_KEY_ID" in st.secrets:
        AWS_ACCESS_KEY_ID = str(st.secrets["AWS_ACCESS_KEY_ID"]).strip()
        AWS_SECRET_ACCESS_KEY = str(st.secrets["AWS_SECRET_ACCESS_KEY"]).strip()
        # ç©ºæ–‡å­—ãªã‚‰envå¤‰æ•°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not AWS_ACCESS_KEY_ID:
            AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID", "")
            AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY", "")
    else:
        AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID", "")
        AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY", "")
except Exception as e:
    # st.error(f"AWSèªè¨¼æƒ…å ±èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID", "")
    AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY", "")

# Claude 4.5 Sonnet ã® inference profile ID
CLAUDE_MODEL_ID = "us.anthropic.claude-sonnet-4-5-20250929-v1:0"
CLAUDE_REGION = "us-east-1"
# â–²â–²â–² è¿½åŠ ã“ã“ã¾ã§ â–²â–²â–²

# â–¼â–¼â–¼ GitHub Models (o4-miniç”¨) â–¼â–¼â–¼
try:
    if "GITHUB_TOKEN" in st.secrets:
        GITHUB_TOKEN = str(st.secrets["GITHUB_TOKEN"]).strip()
        if not GITHUB_TOKEN:  # ç©ºæ–‡å­—ãªã‚‰envå¤‰æ•°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
    else:
        GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
except Exception as e:
    # st.error(f"GITHUB_TOKENèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")

GITHUB_MODEL_ID = "o4-mini"
# â–²â–²â–² GitHub Models ã“ã“ã¾ã§ â–²â–²â–²

# â–¼â–¼â–¼ OpenRouter ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ«ï¼ˆå…ƒ Grok ã‚¹ãƒ­ãƒƒãƒˆï¼‰â–¼â–¼â–¼
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Amazon Nova 2 Lite (free)
DEFAULT_SECONDARY_MODEL_ID = "amazon/nova-2-lite-v1:free"

# ç’°å¢ƒå¤‰æ•°å„ªå…ˆã§å·®ã—æ›¿ãˆå¯èƒ½
SECONDARY_MODEL_ID = (
    os.getenv("OPENROUTER_SECONDARY_MODEL_ID")   # æ–°ã—ã„æ¨å¥¨ç’°å¢ƒå¤‰æ•°
    or os.getenv("GROK_MODEL_ID")               # äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã™
    or DEFAULT_SECONDARY_MODEL_ID
)

# UI ç”¨ã«äººé–“ã«è¦‹ã›ã‚‹åå‰ã‚‚ ENV ã‹ã‚‰å¤‰ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãŠã
SECONDARY_MODEL_NAME = os.getenv(
    "OPENROUTER_SECONDARY_MODEL_NAME",
    "Amazon Nova 2 Lite (free)",
)
# â–²â–²â–² OpenRouter ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ« ã“ã“ã¾ã§ â–²â–²â–²


# =========================
# Session Management
# =========================

def compact_newlines(text: str) -> str:
    """
    éå‰°ãªç©ºç™½è¡Œã‚’åœ§ç¸®ã—ã€è¦‹ã‚„ã™ã„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã«ã™ã‚‹
    """
    import re
    # 1. 3è¡Œä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2è¡Œã«åœ§ç¸®
    text = re.sub(r'\n{3,}', '\n\n', text)
    # 2. ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿ã®è¡Œã‚’ç©ºè¡Œã«å¤‰æ›
    text = re.sub(r'\n[ \t]+\n', '\n\n', text)
    # 3. æ”¹è¡Œ+ã‚¹ãƒšãƒ¼ã‚¹+æ”¹è¡Œã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ”¹è¡Œ2ã¤ã«
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    # 4. ãƒ†ãƒ¼ãƒ–ãƒ«å¾Œã®éå‰°ãªç©ºç™½ã‚’å‰Šé™¤ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã®å¾Œã«3è¡Œä»¥ä¸Šã®ç©ºç™½ãŒã‚ã‚‹å ´åˆï¼‰
    text = re.sub(r'(\|[^\n]+\|)\n{3,}', r'\1\n\n', text)
    return text

def trim_history(messages: list, max_tokens: int = 25000) -> list:
    """
    Vertex AI Quotaã‚¨ãƒ©ãƒ¼å¯¾ç­–: å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
    æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å„ªå…ˆã—ã€å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è‡ªå‹•çš„ã«åˆ‡ã‚Šæ¨ã¦ã‚‹
    
    Args:
        messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
        max_tokens: æ¨å®šæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ25000 = Gemini 3 Proã§ä½™è£•ï¼‰
    
    Returns:
        ãƒˆãƒªãƒ ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
    """
    if not messages:
        return []
    
    trimmed = []
    current_est_tokens = 0
    
    # æ–°ã—ã„é †ã«ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆæœ€æ–°ã‚’å„ªå…ˆä¿æŒï¼‰
    for msg in reversed(messages):
        content = msg.get("content", "")
        # ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®š: æ—¥æœ¬èª1æ–‡å­—â‰’1.5ãƒˆãƒ¼ã‚¯ãƒ³ã€å®‰å…¨å´ã§æ–‡å­—æ•°Ã—2
        est_tokens = len(content) * 2
        
        if current_est_tokens + est_tokens > max_tokens:
            break  # ä¸Šé™ã‚’è¶…ãˆãŸã‚‰ã‚¹ãƒˆãƒƒãƒ—ï¼ˆå¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆ‡ã‚Šæ¨ã¦ï¼‰
        
        trimmed.insert(0, msg)  # å…ˆé ­ã«æŒ¿å…¥ã—ã¦é †åºã‚’ç¶­æŒ
        current_est_tokens += est_tokens
    
    return trimmed

def parse_thinking(text: str) -> tuple[str, str]:
    """
    æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆ<thinking>ã‚¿ã‚°ï¼‰ã‚’å›ç­”ã‹ã‚‰åˆ†é›¢
    GPT 5.1 Pro/Claude 4.5ã®Thinking Processæ©Ÿèƒ½ã‚’å®Ÿè£…
    
    Args:
        text: LLMã®ç”Ÿã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        (thinking_content, main_content) - æ€è€ƒéƒ¨åˆ†ã¨æœ¬æ–‡
    """
    import re
    pattern = r"<thinking>(.*?)</thinking>"
    match = re.search(pattern, text, re.DOTALL)
    if match:
        thinking = match.group(1).strip()
        content = re.sub(pattern, "", text, flags=re.DOTALL).strip()
        return thinking, content
    return None, text

def extract_facts_and_risks(client, model_id: str, research_text: str) -> tuple[str, str, dict]:
    """
    Phase Bä»¥å‰ã®å¾Œæ–¹äº’æ›ç”¨ï¼ˆv1ï¼‰ï¼šäº‹å®Ÿã¨ãƒªã‚¹ã‚¯ã‚’Markdownå½¢å¼ã§æŠ½å‡º
    
    âš ï¸ ã“ã®v1é–¢æ•°ã¯ã€ä»¥ä¸‹ã®å ´åˆã®ã¿ä½¿ç”¨ã•ã‚Œã¾ã™ï¼š
    - Phase B IRæŠ½å‡ºï¼ˆextract_facts_and_risks_v2ï¼‰ãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    - å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¨ã®äº’æ›æ€§ç¶­æŒ
    
    Phase Bå®Ÿè£…å¾Œã¯ã€extract_facts_and_risks_v2() ãŒãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¹ã§ã™ã€‚
    
    Args:
        client: Gemini client
        model_id: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        research_text: èª¿æŸ»ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        Tuple of (fact_summary, risk_summary, usage_dict)
    """
    extraction_prompt = f"""ä»¥ä¸‹ã®èª¿æŸ»çµæœã‹ã‚‰ã€äº‹å®Ÿã¨ãƒªã‚¹ã‚¯ã‚’åˆ†é›¢ã—ã¦ãã ã•ã„ã€‚

ã€èª¿æŸ»çµæœã€‘
{research_text[:8000]}

ã€å‡ºåŠ›å½¢å¼ï¼ˆå³å®ˆï¼‰ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
  "facts": [
    "ç¢ºèªã•ã‚ŒãŸäº‹å®Ÿ1ï¼ˆæ—¥ä»˜ãƒ»æ•°å€¤ãƒ»å¼•ç”¨å…ƒã‚’å«ã‚€ï¼‰",
    "ç¢ºèªã•ã‚ŒãŸäº‹å®Ÿ2",
    ...ï¼ˆ5-10é …ç›®ï¼‰
  ],
  "risks": [
    "ãƒªã‚¹ã‚¯ãƒ»ä¸ç¢ºå®Ÿæ€§1ï¼ˆç°¡æ½”ã«ï¼‰",
    "ãƒªã‚¹ã‚¯ãƒ»ä¸ç¢ºå®Ÿæ€§2",
    ...ï¼ˆ3-7é …ç›®ï¼‰
  ],
  "unknowns": [
    "æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ç‚¹ï¼ˆã‚ã‚‹å ´åˆã®ã¿ï¼‰"
  ]
}}

JSONã®ã¿ã‚’å‡ºåŠ›ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„èª¬æ˜æ–‡ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""
    
    config = types.GenerateContentConfig(
        temperature=0.2,
        system_instruction="äº‹å®Ÿã¨ãƒªã‚¹ã‚¯ã‚’æ­£ç¢ºã«åˆ†é›¢ã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã™ã‚‹å°‚é–€å®¶ã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„ã€‚"
    )
    
    try:
        response = client.models.generate_content(
            model=model_id,
            contents=extraction_prompt,
            config=config
        )
        text = extract_text_from_response(response).strip()
        
        # usageæƒ…å ±ã‚’å–å¾—
        usage_dict = {
            "prompt_tokens": response.usage_metadata.prompt_token_count or 0,
            "output_tokens": response.usage_metadata.candidates_token_count or 0,
        } if response.usage_metadata else {"prompt_tokens": 0, "output_tokens": 0}
        
        # JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
        import json
        import re
        
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ï¼ˆ```json ... ```ï¼‰
        json_text = re.sub(r'```json\s*|\s*```', '', text)
        
        try:
            data = json.loads(json_text)
            facts = data.get("facts", [])
            risks = data.get("risks", [])
            unknowns = data.get("unknowns", [])
            
            # äº‹å®Ÿã®æ•´å½¢
            fact_summary = "## ğŸ“Š äº‹å®Ÿ\n" + "\n".join([f"- {f}" for f in facts])
            if unknowns:
                fact_summary += "\n\n### ä¸æ˜ç‚¹\n" + "\n".join([f"- {u}" for u in unknowns])
            
            # ãƒªã‚¹ã‚¯ã®æ•´å½¢
            risk_summary = "## âš ï¸ ãƒªã‚¹ã‚¯ãƒ»ä¸ç¢ºå®Ÿæ€§\n" + "\n".join([f"- {r}" for r in risks])
            
            return fact_summary, risk_summary, usage_dict
            
        except json.JSONDecodeError:
            # JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯Markdownãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if "##" in text:
                parts = text.split("##")
                fact_summary = parts[1] if len(parts) > 1 else text[:len(text)//2]
                risk_summary = parts[2] if len(parts) > 2 else text[len(text)//2:]
            else:
                mid = len(text) // 2
                fact_summary = text[:len(text)//2]
            risk_summary = text[len(text)//2:]
            return fact_summary, risk_summary, usage_dict
            
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¿”ã™
        return "äº‹å®ŸæŠ½å‡ºã‚¨ãƒ©ãƒ¼", "ãƒªã‚¹ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼", {"prompt_tokens": 0, "output_tokens": 0}


# ========================================
# Phase B: JSON IR Extraction (v2)
# ========================================

def extract_facts_and_risks_v2(
    client,
    model_id: str,
    user_question: str,
    research_text: str
) -> tuple:
    """
    Extract structured JSON IR from research text (Phase B).
    
    Returns: (ir_dict or None, usage_dict, raw_json_text)
    """
    try:
        from research_ir import validate_research_ir
        from datetime import datetime
        import json
        import re
        
        # Truncate research_text if too long
        truncated_research = research_text[:4000] if len(research_text) > 4000 else research_text
        
        extraction_prompt = f"""ä»¥ä¸‹ã®èª¿æŸ»ãƒ¡ãƒ¢ã‹ã‚‰ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæƒ…å ±ã‚’æŠ½å‡ºã—ã¦JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã€èª¿æŸ»ãƒ¡ãƒ¢ã€‘
{truncated_research}

ã€ã‚¿ã‚¹ã‚¯ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼**ã®ã¿**ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã‚„å‰ç½®ãã¯ä¸è¦ã§ã™ã€‚

{{
  "facts": [
    {{
      "statement": "å…·ä½“çš„ãªäº‹å®Ÿã®è¨˜è¿°",
      "source": "web",
      "source_detail": "URLã¾ãŸã¯å‡ºå…¸å…ˆ",
      "date": "2024-12-04",
      "confidence": "high"
    }}
  ],
  "options": [
    {{
      "name": "é¸æŠè‚¢ãƒ»æ¡ˆã®åå‰",
      "pros": ["ãƒ¡ãƒªãƒƒãƒˆ1", "ãƒ¡ãƒªãƒƒãƒˆ2"],
      "cons": ["ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ1"],
      "conditions": ["æˆç«‹æ¡ä»¶1"],
      "estimated_cost": null
    }}
  ],
  "risks": [
    {{
      "statement": "ãƒªã‚¹ã‚¯ã®å†…å®¹",
      "severity": "high",
      "timeframe": "short",
      "mitigation": "å¯¾ç­–æ¡ˆï¼ˆã‚ã‚Œã°ï¼‰"
    }}
  ],
  "unknowns": [
    {{
      "question": "ä¸æ˜ãªç‚¹ãƒ»è¦ç¢ºèªäº‹é …",
      "why_unknown": "insufficient_data",
      "impact": "high"
    }}
  ],
  "metadata": {{
    "question": "{user_question[:150]}",
    "language": "ja",
    "created_at": "{datetime.now().isoformat()}",
    "models": ["{model_id}"],
    "sources_count": 1,
    "search_queries": []
  }}
}}

ã€é‡è¦ãªåˆ¶ç´„ã€‘
1. source ã¯ "web", "youtube", "model" ã®ã„ãšã‚Œã‹
2. confidence ã¯ "high", "medium", "low" ã®ã„ãšã‚Œã‹
   - high: å…¬å¼æƒ…å ±ã¾ãŸã¯è¤‡æ•°ã‚½ãƒ¼ã‚¹ã§ç¢ºèª
   - medium: å˜ä¸€ã‚½ãƒ¼ã‚¹ã¾ãŸã¯é–“æ¥æƒ…å ±
   - low: æ¨æ¸¬ã¾ãŸã¯å¤ã„æƒ…å ±
3. severity/impact ã¯ "high", "medium", "low" ã®ã„ãšã‚Œã‹
4. timeframe ã¯ "short", "medium", "long" ã®ã„ãšã‚Œã‹
5. why_unknown ã¯ "insufficient_data", "conflicting_data", "grey_area", "future_dependent" ã®ã„ãšã‚Œã‹
6. è©²å½“é …ç›®ãŒãªã„å ´åˆã¯ç©ºé…åˆ— [] ã‚’ä½¿ç”¨
7. JSONã®ã¿ã‚’å‡ºåŠ›ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚„èª¬æ˜æ–‡ã¯ä¸è¦ï¼‰
"""

        config = types.GenerateContentConfig(
            temperature=0.1,  # äº‹å®ŸæŠ½å‡ºã¯ä½æ¸©åº¦
            response_mime_type="application/json"
        )
        
        response = client.models.generate_content(
            model=model_id,
            contents=[{"role": "user", "parts": [{"text": extraction_prompt}]}],
            config=config
        )
        
        raw_text = extract_text_from_response(response).strip()
        usage_dict = {
            "prompt_tokens": response.usage_metadata.prompt_token_count or 0,
            "output_tokens": response.usage_metadata.candidates_token_count or 0,
        } if response.usage_metadata else {"prompt_tokens": 0, "output_tokens": 0}
        
        # Remove code blocks if present
        json_text = re.sub(r'```json\s*|\s*```', '', raw_text)
        
        # Parse JSON with retry
        ir_dict = None
        for attempt in range(2):
            try:
                ir_dict = json.loads(json_text)
                break
            except json.JSONDecodeError as e:
                if attempt == 0:
                    # Try to fix common issues
                    json_text = json_text.replace("'", '"')  # Single to double quotes
                    json_text = re.sub(r',\s*}', '}', json_text)  # Remove trailing commas
                    json_text = re.sub(r',\s*]', ']', json_text)
                else:
                    print(f"[DEBUG] JSON parse failed after retry: {e}")
                    return (None, usage_dict, raw_text)
        
        if ir_dict is None:
            return (None, usage_dict, raw_text)
        
        # Validate and normalize
        normalized_ir, warnings = validate_research_ir(ir_dict)
        
        if warnings:
            print(f"[DEBUG] IR validation warnings: {warnings}")
        
        return (normalized_ir, usage_dict, raw_text)
        
    except Exception as e:
        print(f"[DEBUG] extract_facts_and_risks_v2 exception: {e}")
        import traceback
        traceback.print_exc()
        return (None, {"prompt_tokens": 0, "output_tokens": 0}, str(e))


def convert_ir_to_markdown(ir: dict) -> tuple[str, str]:
    """
    Convert JSON IR to Markdown format for backward compatibility.
    
    Args:
        ir: ResearchIR dictionary
    
    Returns:
        Tuple of (fact_summary, risk_summary)
    """
    from research_ir import build_synthesis_prompt_from_ir
    
    # Facts section
    fact_lines = ["## ğŸ“Š äº‹å®Ÿ"]
    confidence_marks = {
        "high": "âœ“",
        "medium": "â–³",
        "low": "?",
        "unknown": "Â·"
    }
    
    for fact in ir.get("facts", []):
        mark = confidence_marks.get(fact.get("confidence", "unknown"), "Â·")
        fact_lines.append(f"{mark} {fact.get('statement', '')}")
        if fact.get("source_detail"):
            fact_lines.append(f"  å‡ºå…¸: {fact['source_detail']}")
    
    # Unknowns section
    if ir.get("unknowns"):
        fact_lines.append("\n### ä¸æ˜ç‚¹ãƒ»è¦ç¢ºèªäº‹é …")
        for unknown in ir["unknowns"]:
            fact_lines.append(f"? {unknown.get('question', '')}")
    
    fact_summary = "\n".join(fact_lines) if fact_lines else "ï¼ˆæŠ½å‡ºã•ã‚ŒãŸäº‹å®Ÿãªã—ï¼‰"
    
    # Risks section
    risk_lines = ["## âš ï¸ ãƒªã‚¹ã‚¯ãƒ»ä¸ç¢ºå®Ÿæ€§"]
    severity_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢", "unknown": "âšª"}
    
    # Sort by severity
    severity_order = {"high": 0, "medium": 1, "low": 2, "unknown": 3}
    sorted_risks = sorted(
        ir.get("risks", []),
        key=lambda x: severity_order.get(x.get("severity", "unknown"), 3)
    )
    
    for risk in sorted_risks:
        emoji = severity_emoji.get(risk.get("severity", "unknown"), "âšª")
        risk_lines.append(f"{emoji} {risk.get('statement', '')}")
        if risk.get("mitigation"):
            risk_lines.append(f"  å¯¾ç­–: {risk['mitigation']}")
    
    risk_summary = "\n".join(risk_lines) if risk_lines else "ï¼ˆç‰¹å®šã•ã‚ŒãŸãƒªã‚¹ã‚¯ãªã—ï¼‰"
    
    return (fact_summary, risk_summary)


# =========================
# Gemini Client Setup
# =========================
try:
    from google import genai
    from google.genai import types
except ImportError:
    st.error("Google Generative AI package not found. Please install: pip install google-generativeai")
    st.stop()


def build_session_memory(sessions: list, current_session_id: str, max_entries: int = 10) -> str:
    """
    
    Args:
        sessions: ã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³
        current_session_id: ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        max_entries: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒªæ•°
    
    Returns:
        ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ã®ãƒ†ã‚­ã‚¹ãƒˆ
    """
    # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é™¤å¤–
    past_sessions = [s for s in sessions if s["id"] != current_session_id]
    
    if not past_sessions:
        return ""
    
    # æœ€æ–°ã®max_entriesã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—
    recent_sessions = past_sessions[-max_entries:]
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨é‡è¦ãªåˆ¤æ–­ã‚’æŠ½å‡º
    key_contexts = []
    for session in recent_sessions:
        for msg in session.get("messages", []):
            if msg["role"] == "user" and len(msg["content"]) > 50:
                # ååˆ†ãªé•·ã•ã®è³ªå•ã®ã¿
                key_contexts.append(msg["content"][:200])
    
    if not key_contexts:
        return ""
    
    # ç°¡æ˜“è¦ç´„
    memory_text = "ã€éå»ã®æ–‡è„ˆãƒ»åˆ¤æ–­åŸºæº–ã€‘\n"
    memory_text += "\n".join([f"- {ctx}..." for ctx in key_contexts[-5:]])
    memory_text += "\n\n"
    
    return memory_text


def generate_recommendations(client, sessions, current_session_id, user_profile, mode="normal"):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¨éå»ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æ¬¡ã®è³ªå•å€™è£œã‚’ç”Ÿæˆ
    
    Args:
        client: Vertex AI client
        sessions: å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³
        current_session_id: ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        user_profile: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        mode: "normal" (ç›´è¿‘5ä»¶) or "deep" (å…¨å±¥æ­´)
    
    Returns:
        tuple: (recommendations_text, usage_dict)
    """
    if mode == "deep":
        # Level 3: å…¨å±¥æ­´ Ã— gemini-2.0-flash
        session_memory = build_full_session_memory(sessions, current_session_id)
        model_name = "gemini-2.0-flash"
        max_tokens = 3000
        role_desc = "ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ç†ŸçŸ¥ã—ãŸå°‚å±ã®æˆ¦ç•¥ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚"
        task_desc = "ã“ã‚Œã¾ã§ã®å…¨è­°è«–ã‚’ä¿¯ç°ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã¾ã æ°—ã¥ã„ã¦ã„ãªã„æœ¬è³ªçš„ãªèª²é¡Œã‚„ã€æ¬¡ã«æ·±æ˜ã‚Šã™ã¹ãæˆ¦ç•¥çš„ãªãƒ†ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
    else:
        # Level 2: ç›´è¿‘5ä»¶ Ã— gemini-2.5-flash
        session_memory = build_session_memory(sessions, current_session_id, max_entries=5)
        model_name = "gemini-2.5-flash"
        max_tokens = 1500
        role_desc = "ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®ä¼šè©±å±¥æ­´ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦ã€æ¬¡ã«èãã¨è‰¯ã„è³ªå•ã‚’ææ¡ˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        task_desc = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ãƒ»é–¢å¿ƒã«åŸºã¥ãã€æ¬¡ã«èãã¨è‰¯ã„è³ªå•ã‚’3ã€œ5å€‹ææ¡ˆã—ã¦ãã ã•ã„ã€‚"

    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®æ•´å½¢
    interests_str = ", ".join(user_profile.get("interests", [])) if user_profile.get("interests") else "ã¾ã ç‰¹å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
    facts_str = "\n".join([f"- {fact}" for fact in user_profile.get("facts_about_user", [])]) if user_profile.get("facts_about_user", []) else "ã¾ã è“„ç©ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    prefs_str = ""
    if user_profile.get("preferences"):
        prefs_str = "\n".join([f"- {k}: {v}" for k, v in user_profile["preferences"].items()])
    else:
        prefs_str = "ã¾ã è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    system_prompt = f"""{role_desc}

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ãƒ»å¥½ã¿ãƒ»éå»ã®æ–‡è„ˆã‚’æœ€å¤§é™æ´»ç”¨
- 3ã€œ5å€‹ã®å…·ä½“çš„ãªè³ªå•ã‚’ææ¡ˆ
- **è³ªå•æ–‡ã¯ç°¡æ½”ã«ã€30æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ã™ã‚‹**ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã®å¹…ãŒç‹­ã„ãŸã‚ï¼‰
- å„è³ªå•ã«ã¯ã€Œãªãœã“ã‚ŒãŒè‰¯ã„ã‹ã€ã®ç†ç”±ã‚’ç°¡æ½”ã«ä»˜ã‘ã‚‹
- éå»ã®ä¼šè©±ã¨ã®ç¹‹ãŒã‚Šã‚’æ˜ç¤º
- **è‡ªç„¶ãªæ—¥æœ¬èªã§æ–‡æ³•çš„ã«æ­£ã—ã„æ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨**
- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¡¨ç¤ºã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãŒæŠ˜ã‚Šè¿”ã•ã‚Œã‚‹ã‚ˆã†ã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹
- å‡ºåŠ›ã¯ä»¥ä¸‹ã®Markdownå½¢å¼ã§:

1. [ç°¡æ½”ãªè³ªå•æ–‡ï¼ˆ30æ–‡å­—ä»¥å†…ï¼‰]
   - ç†ç”±: [ãªãœã“ã®è³ªå•ãŒæœ‰ç›Šã‹]

2. [ç°¡æ½”ãªè³ªå•æ–‡]
   - ç†ç”±: [ç†ç”±]

...

ã€å‡ºåŠ›ä¾‹ã€‘
1. iPhoneã®é€šè¨³æ©Ÿèƒ½ã®æ´»ç”¨æ³•ã¯ï¼Ÿ
   - ç†ç”±: éå»ã®ä¼šè©±ã§iPhoneã®é€šè¨³æ©Ÿèƒ½ã«é–¢å¿ƒã‚’ç¤ºã•ã‚Œã¦ã„ãŸãŸã‚ã€å…·ä½“çš„ãªåˆ©ç”¨ã‚·ãƒ¼ãƒ³ã‚’æ·±æ˜ã‚Šã™ã‚‹ã“ã¨ã§å®Ÿç”¨æ€§ã‚’ç¢ºèªã§ãã¾ã™

2. Geminiã®ã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Šã®ã‚³ãƒ„ã¯ï¼Ÿ
   - ç†ç”±: ã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Šã¸ã®é–¢å¿ƒãŒé«˜ã„ãŸã‚ã€å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã‚’æ¤œè¨ã™ã‚‹ã“ã¨ãŒæœ‰ç›Šã§ã™
"""
    
    user_content = f"""ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã€‘
èˆˆå‘³ãƒ»é–¢å¿ƒ: {interests_str}

å¥½ã¿ãƒ»è¦æœ›:
{prefs_str}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é–¢ã™ã‚‹äº‹å®Ÿ:
{facts_str}

ã€éå»ã®ä¼šè©±ã‚µãƒãƒªãƒ¼ã€‘
{session_memory if session_memory else "ï¼ˆæ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰"}

---

{task_desc}"""
    
    try:
        response = client.models.generate_content(
            model=model_name,
            contents=[
                {"role": "user", "parts": [{"text": f"{system_prompt}\n\n{user_content}"}]}
            ],
            config=types.GenerateContentConfig(
                temperature=0.7,
                max_output_tokens=max_tokens,
            )
        )
        
        # ä½¿ç”¨é‡æƒ…å ±ã®å–å¾—
        usage_metadata = response.usage_metadata
        usage_dict = {
            "input_tokens": usage_metadata.prompt_token_count if usage_metadata else 0,
            "output_tokens": usage_metadata.candidates_token_count if usage_metadata else 0,
        }
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
        recommendations_text = extract_text_from_response(response)
        
        return (recommendations_text, usage_dict)
        
    except Exception as e:
        error_text = f"### âš ï¸ ã‚¨ãƒ©ãƒ¼\n\nææ¡ˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        return (error_text, {"input_tokens": 0, "output_tokens": 0})


def think_with_grok(user_question: str, research_text: str, enable_x_search: bool = False, mode: str = "default") -> str:
    """
    OpenRouter ã®ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: amazon/nova-2-lite-v1:freeï¼‰ã§
    ãƒªã‚µãƒ¼ãƒãƒ¡ãƒ¢ã‚’åˆ¥è¦–ç‚¹ã‹ã‚‰æ¤œè¨ã™ã‚‹ã€‚
    enable_x_search=True ã®å ´åˆã€X/Twitteræƒ…å ±ã®æ´»ç”¨ã‚’ä¿ƒã™
    mode="full_max" ã®å ´åˆã€ç‹¬ç«‹ã—ãŸãƒªãƒ¼ãƒ‰ç ”ç©¶è€…ã¨ã—ã¦æŒ¯ã‚‹èˆã†
    """
    if not OPENROUTER_API_KEY:
        raise RuntimeError("OpenRouter API Key is missing.")

    # Xæ¤œç´¢å¼·åŒ–ç‰ˆã®å ´åˆã€ç‰¹åˆ¥ãªæŒ‡ç¤ºã‚’è¿½åŠ 
    x_search_instruction = ""
    if enable_x_search:
        x_search_instruction = (
            "\n\n**é‡è¦**: ã‚ãªãŸã¯ Xï¼ˆTwitterï¼‰ã®æƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã¨ä»®å®šã—ã¦æ§‹ã„ã¾ã›ã‚“ãŒã€"
            "å®Ÿéš›ã«Webã‚’é–²è¦§ã—ãŸã‹ã®ã‚ˆã†ãªæ–­å®šçš„è¡¨ç¾ï¼ˆã€Œå…¬å¼ã‚µã‚¤ãƒˆã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€œã€ãªã©ï¼‰ã¯é¿ã‘ã¦ãã ã•ã„ã€‚\n"
        )
    
    
    # Phase A: ãƒ¢ãƒ‡ãƒ«å½¹å‰²ç‰¹åŒ–
    role_specialization = """
ã€ã‚ãªãŸã®å°‚é–€å½¹å‰²ã€‘
ãƒ»ã‚¨ãƒƒã‚¸ãªè¦–ç‚¹ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ„è¦‹ã€çš®è‚‰ãªè¦‹æ–¹ã‚’æä¾›ã™ã‚‹å°‚é–€å®¶
ãƒ»ä¸»æµã®æ„è¦‹ã«å¯¾ã™ã‚‹ã€Œå¾…ã£ãŸã€ã‚’å…¥ã‚Œã‚‹å½¹å‰²
ãƒ»X/Twitterçš„ãªé‹­ã„æŒ‡æ‘˜ã‚„ç‚ä¸Šãƒªã‚¹ã‚¯ã®æ¤œå‡º

ã€ä»–ã®ãƒ¢ãƒ‡ãƒ«ã«ä»»ã›ã‚‹ã“ã¨ã€‘
ãƒ»Webæ¤œç´¢ã‚„é•·æ–‡è¦ç´„ â†’ Gemini
ãƒ»æ§‹é€ çš„ãƒªã‚¹ã‚¯åˆ†æ â†’ Claude 4.5
ãƒ»ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹åˆ—æŒ™ â†’ o4-mini
"""
    
    if mode == "full_max":
        user_content = (
            role_specialization +
            f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
            f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
            "æŒ‡ç¤º:\n"
            "ã‚ãªãŸã¯åˆ¥è¦–ç‚¹ã®ãƒªãƒ¼ãƒ‰ç ”ç©¶è€…ã§ã™ã€‚\n"
            "ãƒ»æ–°ã—ã„çµè«–ã‚’ä½œã‚‹ã‚ˆã‚Šã‚‚ã€ã€Œè¦‹è½ã¨ã—ã¦ã„ãã†ãªè«–ç‚¹ãƒ»ãƒªã‚¹ã‚¯ãƒ»åå¯¾æ„è¦‹ã€ã‚’å‡ºã™ã“ã¨ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚\n"
            "ãƒ»3ã€œ7å€‹ã®ç®‡æ¡æ›¸ãã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚å„é …ç›®ã¯1ã€œ3è¡Œä»¥å†…ã§ç°¡æ½”ã«ã€‚\n"
            "ãƒ»é€£ç¶šã™ã‚‹ç©ºè¡Œã¯1è¡Œã¾ã§ã«ã—ã¦ãã ã•ã„ã€‚\n"
            f"{x_search_instruction}"
        )
    else:
        user_content = (
            role_specialization +
            f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
            f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
            "æŒ‡ç¤º:\n"
            "èª¿æŸ»ãƒ¡ãƒ¢ã‚’å…ƒã«ã€ã€Œä»–ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹è½ã¨ã—ãã†ãªè¦–ç‚¹ãƒ»ãƒªã‚¹ã‚¯ã€ã‚’3ã€œ5å€‹ã€ç®‡æ¡æ›¸ãã§å‡ºã—ã¦ãã ã•ã„ã€‚\n"
            "ãƒ»ãƒ•ãƒ«ã®å›ç­”ã§ã¯ãªãã€ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆå½¢å¼ã§ã€‚\n"
            "ãƒ»å„é …ç›®ã¯1ã€œ3è¡Œä»¥å†…ã§ç°¡æ½”ã«ã€‚\n"
            f"{x_search_instruction}"
        )

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://gemini-app.streamlit.app/", 
        "X-Title": "Gemini Web Studio",
    }
    
    data = {
        "model": SECONDARY_MODEL_ID,
        "messages": [
            {"role": "user", "content": user_content}
        ],
        "temperature": 0.8,  # Phase A: ã‚¨ãƒƒã‚¸ãªè¦–ç‚¹ãƒ»ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ„è¦‹ã‚’å‡ºã—ã‚„ã™ã
        "max_tokens": 2000,
        # Nova 2 Lite ãªã© reasoning å¯¾å¿œãƒ¢ãƒ‡ãƒ«ãªã‚‰ã“ã“ã§æœ‰åŠ¹åŒ–ã‚‚å¯èƒ½ï¼š
        # "reasoning": {"effort": "medium"},
    }

    try:
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=60,
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except Exception as e:
        # ã“ã“ã¯ raise ã«ã—ã¦ã€å‘¼ã³å‡ºã—å´ã§ error ã¨ã—ã¦æ‰±ã†æ–¹ãŒå®‰å…¨
        raise RuntimeError(f"Error calling OpenRouter model ({SECONDARY_MODEL_ID}): {e}")

def review_with_grok(user_question: str, gemini_answer: str, research_text: str, mode: str = "normal") -> str:
    """
    OpenRouterã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€Geminiã®æœ€çµ‚å›ç­”ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹
    mode="onigunsou": å³æ ¼ãªæ¤œå¯Ÿå®˜ã¨ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼
    mode="full_max": ãƒ€ãƒ–ãƒ«é¬¼è»æ›¹ã¨ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼
    """
    if not OPENROUTER_API_KEY:
        return "OpenRouter API Key is missing."

    # å…±é€š: ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²ã‚’ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆå°‚ç”¨ã€ã«å³ã—ãåˆ¶é™
    system_content = (
        "ã‚ãªãŸã¯Geminiã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ã§ã™ã€‚\n"
        "\n"
        "ã€å‰æã€‘\n"
        "ãƒ»ã‚ãªãŸã®æ±ç”¨çŸ¥è­˜ã¯ã‚ãã¾ã§å‚è€ƒæƒ…å ±ã§ã™ã€‚\n"
        "ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¸¡ã—ãŸã€Œèª¿æŸ»ãƒ¡ãƒ¢ã€ã¨ã€ŒGeminiã®å›ç­”ã€ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹äº‹å®Ÿã‚’ã€ã‚ãªãŸè‡ªèº«ã®çŸ¥è­˜ã‚ˆã‚Šã‚‚å¸¸ã«å„ªå…ˆã—ã¦ãã ã•ã„ã€‚\n"
        "\n"
        "ã€ç¦æ­¢äº‹é …ã€‘\n"
        "ãƒ»è‡ªåˆ†ã®çŸ¥è­˜ã®ã‚«ãƒƒãƒˆã‚ªãƒ•ã‚„æœ€çµ‚æ›´æ–°æ—¥ã«ã¤ã„ã¦è¨€åŠã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
        "  ï¼ˆä¾‹:ã€Œ2024å¹´12æœˆæ™‚ç‚¹ã§ã¯ã€œã€ã€Œç§ã®çŸ¥è­˜ã¯2024å¹´ã¾ã§ã§ã™ã€ãªã©ï¼‰\n"
        "ãƒ»ã€Œã€œã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã¯å­˜åœ¨ã—ãªã„ã€ã€Œã¾ã ç™ºè¡¨ã•ã‚Œã¦ã„ãªã„ã€ã¨æ–­å®šã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
        "  å¿…è¦ãªå ´åˆã¯ã€Œå…¬é–‹æƒ…å ±ã¨é£Ÿã„é•ã†å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§è¦ç¢ºèªã€ã®ã‚ˆã†ã«ã€å¼±ã„è¡¨ç¾ã«ã—ã¦ãã ã•ã„ã€‚\n"
        "ãƒ»Webã‚µã‚¤ãƒˆã‚’ã€ä»Šè¦‹ãŸã€ã‹ã®ã‚ˆã†ãªè¡¨ç¾ï¼ˆä¾‹:ã€å…¬å¼ã‚µã‚¤ãƒˆã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€œã€ï¼‰ã‚’ä½¿ã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
        "\n"
        "ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ–¹é‡ã€‘\n"
        "ãƒ»èª¿æŸ»ãƒ¡ãƒ¢ã¨Geminiã®å›ç­”ã®ã‚ã„ã ã«ã‚ã‚‹ã€å…·ä½“çš„ãªçŸ›ç›¾ãƒ»å±é™ºãªèª¤ã‚Šãƒ»éåº¦ãªæ–­å®šã ã‘ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚\n"
        "ãƒ»å˜ã«ã€Œè‡ªåˆ†ã®çŸ¥è­˜ã¨é•ã†ã€ã€Œè‡ªåˆ†ã®çŸ¥è­˜ã§ã¯ç¢ºèªã§ããªã„ã€ã ã‘ã®å ´åˆã€ãã‚Œã‚’ã‚‚ã£ã¦èª¤ã‚Šèªå®šã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
        "  ãã®å ´åˆã¯ã€Œå…¬é–‹æƒ…å ±ã¨ç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§è¦ç¢ºèªã€ç¨‹åº¦ã®ä¸€è¡Œã‚³ãƒ¡ãƒ³ãƒˆã«ç•™ã‚ã¦ãã ã•ã„ã€‚\n"
        "ãƒ»Markdownã¯ä½¿ç”¨ã—ã¦ã‹ã¾ã„ã¾ã›ã‚“ãŒã€é€£ç¶šã™ã‚‹ç©ºè¡Œã¯1è¡Œã¾ã§ã«ã—ã¦ãã ã•ã„ã€‚\n"
    )
    
    if mode == "onigunsou":
        system_content += (
            "\nâš ï¸ é‡è¦ãªæ³¨æ„:\n"
            "ãƒ»ã‚ãªãŸã¯ **è‡ªåˆ†ã®å­¦ç¿’çŸ¥è­˜ã§ã¯ãªãã€èª¿æŸ»ãƒ¡ãƒ¢ã¨Geminiã®å›ç­”** ã‚’å‰æã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã ã•ã„ã€‚\n"
            "ãƒ»èª¿æŸ»ãƒ¡ãƒ¢ã«è‡ªåˆ†ã®è¨˜æ†¶ã¨ç•°ãªã‚‹æ–°ã—ã„æƒ…å ±ãŒã‚ã£ã¦ã‚‚ã€ã€Œèª¤ã‚Šã€ã¨æ±ºã‚ã¤ã‘ãšã€\n"
            "  ã€Œå…¬é–‹æƒ…å ±ã¨é£Ÿã„é•ã†å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§è¦ç¢ºèªã€ã¨ã„ã£ãŸå¼±ã„è¡¨ç¾ã«ã—ã¦ãã ã•ã„ã€‚\n"
            "ãƒ»ã€Œâ—¯å¹´â—¯æœˆæ™‚ç‚¹ã§ã¯ã€œã€ã®ã‚ˆã†ãªæ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã®åè«–ã¯è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚\n"
        )
        instruction = (
            "ä»¥ä¸‹ã®å½¢å¼ã§ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
            "## è©•ä¾¡æ¦‚è¦\n"
            "- å›ç­”ã¯ OK / è¦ä¿®æ­£ / å±é™º ã®ã„ãšã‚Œã‹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚\n\n"
            "## å•é¡Œç‚¹\n"
            "- ç®‡æ¡æ›¸ãã§ã€å±é™ºãªèª¤ã‚Šãƒ»éåº¦ãªæ–­å®šãƒ»è«–ç†ã®é£›èºãªã©ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n\n"
            "## ä¿®æ­£ã®ãƒã‚¤ãƒ³ãƒˆ\n"
            "- ã©ã®éƒ¨åˆ†ã‚’ã©ã†å¼±ã‚ã‚‹ï¼æ›¸ãæ›ãˆã‚‹ã¹ãã‹ ã ã‘ã‚’ç°¡æ½”ã«ç¤ºã—ã¦ãã ã•ã„ã€‚\n\n"
            "â€» Geminiã®å›ç­”å…¨æ–‡ã‚’æ›¸ãç›´ã—ãŸã‚Šã€ç‹¬è‡ªã®æœ€çµ‚å›ç­”ã‚’ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚"
        )
    elif mode == "full_max":
        system_content += (
            "\nâš ï¸ é‡è¦ãªæ³¨æ„:\n"
            "ãƒ»ã‚ãªãŸã¯æ¤œå¯Ÿå®˜ãƒ¬ãƒ™ãƒ«ã«å³ã—ããƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¾ã™ãŒã€\n"
            "  ãã‚Œã§ã‚‚ãªãŠ **èª¿æŸ»ãƒ¡ãƒ¢ã®è¨˜è¼‰ã‚’ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦æ‰±ã†** å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n"
            "ãƒ»è‡ªåˆ†ã®çŸ¥è­˜ã¨ã®å·®åˆ†ã ã‘ã‚’æ ¹æ‹ ã«ã€Œèª¤ã‚Šã€ã€Œå±é™ºã€ã¨åˆ¤æ–­ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
            "ãƒ»æœ€æ–°æƒ…å ±ã‹ã©ã†ã‹åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ã€Œè¦ç¢ºèªã€ã¨ã ã‘è¿°ã¹ã€\n"
            "  ã‚«ãƒƒãƒˆã‚ªãƒ•ã‚„å­¦ç¿’æ™‚æœŸã«ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã§ãã ã•ã„ã€‚\n"
        )
        instruction = (
            "ä»¥ä¸‹ã®å½¢å¼ã§ã€å³ã—ã‚ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
            f"## {SECONDARY_MODEL_NAME}è©•ä¾¡æ¦‚è¦\n"
            "- OK / è¦ä¿®æ­£ / å±é™º ã®ã„ãšã‚Œã‹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚\n\n"
            "## é‡å¤§ãªå•é¡Œç‚¹\n"
            "- ç®‡æ¡æ›¸ãã§ã€ç‰¹ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’èª¤èª˜å°ã—ãã†ãªç‚¹ã ã‘æŒ™ã’ã¦ãã ã•ã„ã€‚\n\n"
            "## æ”¹å–„ã®ãƒ’ãƒ³ãƒˆ\n"
            "- ã©ã®è«–ç‚¹ã‚’å¼±ã‚ãŸã‚Šã€è¿½åŠ ã§æ³¨æ„æ›¸ãã™ã¹ãã‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n\n"
            "â€» Geminiã®å›ç­”å…¨æ–‡ã‚’æ›¸ãç›´ã—ãŸã‚Šã€ç‹¬è‡ªã®æœ€çµ‚å›ç­”ã‚’ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚"
        )
    else:
        instruction = (
            "ä»¥ä¸‹ã®Geminiã®å›ç­”ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€è«–ç†çš„ãªèª¤ã‚Šã‚„ä¸è¶³ã—ã¦ã„ã‚‹è¦–ç‚¹ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚\n"
            "ã¾ãŸã€ã‚ˆã‚Šè‰¯ã„å›ç­”ã«ã™ã‚‹ãŸã‚ã®æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"
        )

    user_content = (
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
        f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
        f"Geminiã®å›ç­”:\n{gemini_answer}\n\n"
        f"æŒ‡ç¤º:\n{instruction}"
    )

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://gemini-app.streamlit.app/", 
        "X-Title": "Gemini Web Studio",
    }
    
    data = {
        "model": SECONDARY_MODEL_ID,
        "messages": [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content}
        ],
        "temperature": 0.5, # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãªã®ã§å°‘ã—æŠ‘ãˆã‚
        "max_tokens": 2000,
    }
    try:
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()
        result = response.json()
        
        # ã‚«ãƒƒãƒˆã‚ªãƒ•ç³»ã®ãƒã‚¤ã‚ºã‚’å‰Šé™¤
        raw_content = result["choices"][0]["message"]["content"]
        return _clean_grok_review(raw_content)
    except requests.exceptions.HTTPError as e:
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ¬æ–‡ã‚’è¿”ã™
        status = e.response.status_code if e.response else "unknown"
        body = e.response.text[:500] if e.response is not None else ""
        return f"Error calling {SECONDARY_MODEL_NAME}: HTTP {status}: {body}"
    except Exception as e:
        return f"Error calling {SECONDARY_MODEL_NAME}: {type(e).__name__}: {e}"


def _clean_grok_review(text: str) -> str:
    """
    Grok ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ã€ŒçŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•ã€ã€Œâ—¯å¹´â—¯æœˆæ™‚ç‚¹ã€ç³»ã®ãƒã‚¤ã‚ºã‚’è»½ãå‰Šã‚‹
    """
    NG_PHRASES = [
        "ã‚«ãƒƒãƒˆã‚ªãƒ•",
        "cutoff",
        "çŸ¥è­˜ã¯",
        "2024å¹´12æœˆæ™‚ç‚¹",
        "2024 å¹´ 12 æœˆæ™‚ç‚¹",
        "2024å¹´11æœˆæ™‚ç‚¹",
        "ç§ã®çŸ¥è­˜",
    ]
    lines = []
    for line in text.splitlines():
        if any(ng in line for ng in NG_PHRASES):
            # ãã®è¡Œã¯æ¨ã¦ã‚‹
            continue
        lines.append(line)
    return "\n".join(lines).strip()



def think_with_claude45_bedrock(user_question: str, research_text: str) -> tuple[str, dict]:
    """
    AWS Bedrock çµŒç”±ã§ Claude Sonnet 4.5 ã‚’ä½¿ã£ã¦ç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã™ã‚‹
    Returns: (å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ, usageè¾æ›¸)
    """
    if not HAS_BOTO3:
        return ("Error: boto3 library not installed. (pip install boto3)", {})
    if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:
        return ("Error: AWS credentials are missing.", {})

    # Claude 4.5 ã¸ã®å½¹å‰²ä»˜ä¸: è«–ç†çš„æ¨è«–ã¨ãƒªã‚¹ã‚¯æŒ‡æ‘˜ã«ç‰¹åŒ–
    system_prompt = (
        "ã‚ãªãŸã¯Geminiã¨ã¯ç•°ãªã‚‹ç‹¬ç«‹ã—ãŸAIã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚\n"
        "æä¾›ã•ã‚ŒãŸèª¿æŸ»ãƒ¡ãƒ¢ã‚’äº‹å®Ÿã®ãƒ™ãƒ¼ã‚¹ã¨ã—ã¤ã¤ã‚‚ã€ã‚ãªãŸã®å¼·ã¿ã§ã‚ã‚‹ã€Œè«–ç†çš„æ¨è«–(Reasoning)ã€ã‚’æ´»ã‹ã—ã¦ã€\n"
        "GeminiãŒè¦‹è½ã¨ã—ãŒã¡ãªã€å‰æã®èª¤ã‚Šã€ã€éš ã‚ŒãŸãƒªã‚¹ã‚¯ã€ã€åˆ¥ã®å¯èƒ½æ€§ã€ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚\n"
        "å›ç­”ã¯ç°¡æ½”ã«ã€ç®‡æ¡æ›¸ãã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    )

    
    # Phase A: ãƒ¢ãƒ‡ãƒ«å½¹å‰²ç‰¹åŒ–
    role_specialization = """
ã€ã‚ãªãŸã®å°‚é–€å½¹å‰²ã€‘
ãƒ»æ§‹é€ çš„ãƒªã‚¹ã‚¯ã€ã‚·ã‚¹ãƒ†ãƒ çš„ãªå•é¡Œç‚¹ã®ç™ºè¦‹
ãƒ»é•·æœŸçš„ãªã‚·ãƒŠãƒªã‚ªåˆ†æï¼ˆ1å¹´å¾Œã€5å¹´å¾Œã®å½±éŸ¿ï¼‰
ãƒ»è¦‹è½ã¨ã•ã‚ŒãŒã¡ãªå‰ææ¡ä»¶ã‚„ä¾å­˜é–¢ä¿‚ã®æŒ‡æ‘˜

ã€ã‚ãªãŸãŒé‡è¦–ã™ã¹ãã“ã¨ã€‘
ãƒ»çŸ­æœŸçš„ãªè¦–ç‚¹ã‚ˆã‚Šã‚‚ã€é•·æœŸçš„ãƒ»æ§‹é€ çš„ãªè¦–ç‚¹
ãƒ»ã€Œã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¤±æ•—ã™ã‚‹æ¡ä»¶ã¯ï¼Ÿã€
ãƒ»ã€Œã‚¹ã‚±ãƒ¼ãƒ«ã—ãŸæ™‚ã«ä½•ãŒå£Šã‚Œã‚‹ã‹ï¼Ÿã€
"""
    
    user_content = (
        role_specialization +
        f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
        f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
        "æŒ‡ç¤º:\n"
        "èª¿æŸ»ãƒ¡ãƒ¢ã®å†…å®¹ã‚’å…ƒã«ã€é•·æœŸçš„ãªè¦–ç‚¹ã§ã€Œãƒªã‚¹ã‚¯ã€ã€Œå‰ææ¡ä»¶ã€ã€Œä¾å­˜é–¢ä¿‚ã€ã‚’ä¸­å¿ƒã«\n"
        "3ã€œ5å€‹ã®é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç®‡æ¡æ›¸ãã§æ›¸ã„ã¦ãã ã•ã„ã€‚"
    )

    try:
        # AWS Bedrock ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
        bedrock = boto3.client(
            service_name='bedrock-runtime',
            region_name=CLAUDE_REGION,
            aws_access_key_id=AWS_ACCESS_KEY_ID,
            aws_secret_access_key=AWS_SECRET_ACCESS_KEY
        )

        # Bedrock converse API ã‚’ä½¿ç”¨ï¼ˆinference profileå¯¾å¿œ + Extended Thinkingï¼‰
        resp = bedrock.converse(
            modelId=CLAUDE_MODEL_ID,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"text": f"{system_prompt}\n\n{user_content}"}
                    ],
                }
            ],
            inferenceConfig={
                "maxTokens": 5000,  # Thinking modeã§ã¯å¤šã‚ã«ç¢ºä¿
                "temperature": 1.0,  # Extended Thinking mode ã§ã¯å¿…é ˆ
            },
            # Extended Thinking Mode ã‚’æœ‰åŠ¹åŒ–
            additionalModelRequestFields={
                "thinking": {
                    "type": "enabled",
                    "budget_tokens": 3000  # æ€è€ƒç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                }
            }
        )

        # æ€è€ƒãƒ–ãƒ­ãƒƒã‚¯ã¨å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã®å–ã‚Šå‡ºã— (reasoningContentå¯¾å¿œ)
        thinking_blocks = []
        text_chunks = []
        output = resp.get("output", {})
        message = output.get("message", {})
        
        for part in message.get("content", []):
            # Extended Thinking ã®æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ (reasoningContent)
            if "reasoningContent" in part:
                rc = part["reasoningContent"]
                if isinstance(rc, dict):
                    # reasoningText.text ã‚’å–å¾—
                    rt = rc.get("reasoningText", {})
                    if isinstance(rt, dict):
                        t = rt.get("text")
                        if t:
                            thinking_blocks.append(t)
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: rc["text"] ã‚‚è©¦ã™
                    elif "text" in rc:
                        thinking_blocks.append(rc["text"])
            # æœ€çµ‚å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
            elif "text" in part:
                text_chunks.append(part["text"])

        # ä½¿ç”¨é‡æƒ…å ±ã®å–å¾—
        usage = resp.get("usage", {})
        usage_dict = {
            "inputTokens": usage.get("inputTokens", 0),
            "outputTokens": usage.get("outputTokens", 0)
        }

        result_text = "".join(text_chunks) if text_chunks else "[Claude 4.5 Sonnetã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã—ãŸ]"
        
        # æ€è€ƒãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯å†’é ­ã«è¿½åŠ 
        if thinking_blocks:
            thinking_text = "\n\n".join([f"**ğŸ§  æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ {i+1}:**\n{block}" for i, block in enumerate(thinking_blocks)])
            result_text = f"{thinking_text}\n\n---\n\n**ğŸ’¡ æœ€çµ‚å›ç­”:**\n{result_text}"
        
        return (result_text, usage_dict)

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¿”ã™
        return (f"Error calling Claude 4.5 Sonnet (Bedrock): {e}", {})




def think_with_o4_mini(user_question: str, research_text: str) -> tuple[str, dict]:
    """
    GitHub ModelsçµŒç”±ã§o4-miniã‚’ä½¿ã£ã¦ç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã™ã‚‹
    åˆ¶é™: input 4000ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸‹ã®å ´åˆã®ã¿ä½¿ç”¨
    Returns: (å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ, ç©ºdict - GitHub Modelsã¯usageæƒ…å ±ã‚’è¿”ã•ãªã„)
    """
    if not GITHUB_TOKEN:
        return ("Error: GitHub Token is missing.", {})
    
    # é•·ã•ãƒã‚§ãƒƒã‚¯ã¯å‘¼ã³å‡ºã—å´ã§å®Ÿæ–½æ¸ˆã¿ï¼ˆ3800æ–‡å­—ä»¥ä¸‹ã‚’ä¿è¨¼ï¼‰
    
    
    system_prompt = (
        "ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸè¦ç´„ã‚’ã‚‚ã¨ã«ã€ŒæŠœã‘ã¦ã„ãã†ãªè¦³ç‚¹ã€ã‚’åˆ—æŒ™ã™ã‚‹ãƒ†ã‚¹ã‚¿ãƒ¼/ãƒã‚§ãƒƒã‚«ãƒ¼ã§ã™ã€‚\n"
        "ã€é‡è¦ãªåˆ¶ç´„ã€‘\n"
        "ãƒ»ãƒ•ãƒ«ã®å›ç­”ã¯æ›¸ã‹ãªã„ã§ãã ã•ã„ã€‚\n"
        "ãƒ»ã€Œä»–ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹è½ã¨ã—ãã†ãªãƒªã‚¹ã‚¯ãƒ»ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ»åè«–ã€ã ã‘ã‚’ç®‡æ¡æ›¸ãã§å‡ºã—ã¦ãã ã•ã„ã€‚\n"
        "ãƒ»æœ€å¤§5å€‹ã¾ã§ã€‚å„é …ç›®ã¯1ã€œ3è¡Œä»¥å†…ã§ç°¡æ½”ã«ã€‚\n"
        "ãƒ»é€£ç¶šã™ã‚‹ç©ºè¡Œã¯1è¡Œã¾ã§ã«ã—ã¦ãã ã•ã„ã€‚"
    )
    
    
    # Phase A: ãƒ¢ãƒ‡ãƒ«å½¹å‰²ç‰¹åŒ–
    role_specialization = """
ã€ã‚ãªãŸã®å°‚é–€å½¹å‰²ã€‘
ãƒ»ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®åˆ—æŒ™
ãƒ»ã€Œâ—¯â—¯ã®å ´åˆã¯ã©ã†ãªã‚‹ï¼Ÿã€ã¨ã„ã†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆä½œæˆ
ãƒ»å®Ÿè£…ä¸Šã®è½ã¨ã—ç©´ã‚„ç´°ã‹ã„æ³¨æ„ç‚¹ã®æŒ‡æ‘˜

ã€å‡ºåŠ›å½¢å¼ã®æ¨å¥¨ã€‘
ãƒ»ç®‡æ¡æ›¸ãã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆå½¢å¼
ãƒ»ã€Œç¢ºèªã™ã¹ãã“ã¨ã€ãƒªã‚¹ãƒˆ
ãƒ»ã€Œæƒ³å®šã™ã¹ãã‚±ãƒ¼ã‚¹ã€ãƒªã‚¹ãƒˆ
"""
    
    user_content = (
        role_specialization +
        f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
        f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
        "æŒ‡ç¤º:\n"
        "èª¿æŸ»ãƒ¡ãƒ¢ã‚’å…ƒã«ã€è€ƒæ…®ã™ã¹ãã€Œãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€ã€Œã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã€ã€Œãƒã‚§ãƒƒã‚¯é …ç›®ã€ã‚’\n"
        "ç®‡æ¡æ›¸ãã§3ã€œ7å€‹å‡ºã—ã¦ãã ã•ã„ã€‚"
    )
    
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Content-Type": "application/json",
    }
    
    data = {
        "model": "gpt-4o-mini",  # GitHub Modelsç”¨ã®ãƒ¢ãƒ‡ãƒ«å
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ],
        "temperature": 0.7,
        "max_tokens": 2000,
    }
    
    try:
        import requests
        response = requests.post(
            f"https://models.inference.ai.azure.com/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()
        result = response.json()
        answer_text = result["choices"][0]["message"]["content"]
        return (answer_text, {})  # GitHub Modelsã¯usageæƒ…å ±ã‚’è¿”ã•ãªã„
    except Exception as e:
        return (f"Error calling o4-mini (GitHub Models): {e}", {})


# Puteré–¢é€£ã®é–¢æ•°ã¯å‰Šé™¤ï¼ˆAWS Bedrockã«ç§»è¡Œï¼‰

def create_new_session():
    """
    å®‰å®šç‰ˆ: session_stateã‚’ãƒã‚¹ã‚¿ãƒ¼ã¨ã—ã¦ä½¿ç”¨
    âŒ å‰Šé™¤: load_sessions() â† ç«¶åˆã®åŸå› 
    """
    # session_stateã‚’ãƒã‚¹ã‚¿ãƒ¼ã¨ã—ã¦ä½¿ç”¨
    if "sessions" not in st.session_state:
        st.session_state.sessions = load_sessions()  # èµ·å‹•æ™‚ã®ã¿
    
    current_sessions = st.session_state.sessions
    
    # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒç©ºãªã‚‰ã€æ–°ã—ãä½œã‚‰ãšã«ãã‚Œã‚’å†åˆ©ç”¨ã™ã‚‹
    if st.session_state.get("current_session_id"):
        for s in current_sessions:
            if s["id"] == st.session_state.current_session_id:
                if len(s["messages"]) == 0:
                    st.toast("ã™ã§ã«æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã§ã™")
                    return

    new_id = str(uuid.uuid4())
    new_session = {
        "id": new_id,
        "title": "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ",
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": [],
    }
    current_sessions.insert(0, new_session)
    
    # 1. ãƒ¡ãƒ¢ãƒªã‚’æ›´æ–°
    st.session_state.sessions = current_sessions
    st.session_state.current_session_id = new_id
    
    # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã¸ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
    save_sessions(st.session_state.sessions)
    st.rerun()

def switch_session(session_id):
    st.session_state.current_session_id = session_id
    st.rerun()

def update_current_session_messages(messages):
    """
    å±¥æ­´å®‰å®šåŒ–ç‰ˆ: session_stateã‚’ãƒã‚¹ã‚¿ãƒ¼ã¨ã—ã¦æ‰±ã„ã€ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿å­˜ã®ã¿
    âŒ ä¿®æ­£å‰: load_sessions()ã§æ¯å›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ â†’ å…ˆç¥–è¿”ã‚Šç™ºç”Ÿ
    â­• ä¿®æ­£å¾Œ: session_stateã‚’ç›´æ¥æ›´æ–° â†’ ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ä¿å­˜
    """
    if st.session_state.current_session_id:
        # âŒ å‰Šé™¤: current_sessions = load_sessions()  â† ã“ã‚ŒãŒå…ˆç¥–è¿”ã‚Šã®åŸå› 
        
        # â­• session_stateã‚’ãƒã‚¹ã‚¿ãƒ¼ã¨ã—ã¦ä½¿ç”¨
        if "sessions" not in st.session_state or not st.session_state.sessions:
            st.session_state.sessions = load_sessions()  # èµ·å‹•æ™‚ã®ã¿
        
        current_sessions = st.session_state.sessions
        target_index = -1
        
        for i, session in enumerate(current_sessions):
            if session["id"] == st.session_state.current_session_id:
                session["messages"] = messages
                if session["title"] == "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ" and len(messages) > 0:
                    first_msg = messages[0]["content"]
                    session["title"] = (first_msg[:20] + "...") if len(first_msg) > 20 else first_msg
                session["timestamp"] = datetime.datetime.now().isoformat()
                target_index = i
                break
        
        if target_index != -1:
            # æœ€æ–°ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚¹ãƒˆã®å…ˆé ­ã«ç§»å‹•
            updated_session = current_sessions.pop(target_index)
            current_sessions.insert(0, updated_session)
        
        # 1. ãƒ¡ãƒ¢ãƒªã‚’å³æ™‚æ›´æ–°ï¼ˆã“ã‚Œã§ç”»é¢ä¸Šã®è¡¨ç¤ºã¯å®‰å®šã™ã‚‹ï¼‰
        st.session_state.sessions = current_sessions
        
        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã¸ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
        save_sessions(current_sessions)

def get_current_messages():
    """
    ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™ç‰ˆ: å‚ç…§ã‚’è¿”ã™ã¨æ„å›³ã—ãªã„å¤‰æ›´ãŒèµ·ãã‚‹
    âŒ ä¿®æ­£å‰: return session["messages"]  â† å‚ç…§ã‚’è¿”ã™
    â­• ä¿®æ­£å¾Œ: return list(...)  â† ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™
    """
    if st.session_state.current_session_id:
        for session in st.session_state.sessions:
            if session["id"] == st.session_state.current_session_id:
                return list(session["messages"])  # ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™ï¼ˆé‡è¦ï¼‰
    return []

def delete_session(session_id):
    """
    å®‰å®šç‰ˆ: session_stateã‚’ãƒã‚¹ã‚¿ãƒ¼ã¨ã—ã¦ä½¿ç”¨
    """
    if "sessions" not in st.session_state:
        st.session_state.sessions = load_sessions()
    
    current_sessions = [s for s in st.session_state.sessions if s["id"] != session_id]
    
    # 1. ãƒ¡ãƒ¢ãƒªã‚’æ›´æ–°
    st.session_state.sessions = current_sessions
    
    if st.session_state.current_session_id == session_id:
        st.session_state.current_session_id = None
        if current_sessions:
            st.session_state.current_session_id = current_sessions[0]["id"]
    
    # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã¸ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
    save_sessions(st.session_state.sessions)
    st.rerun()

def branch_session():
    """
    å®‰å®šç‰ˆ: session_stateã‚’ãƒã‚¹ã‚¿ãƒ¼ã¨ã—ã¦ä½¿ç”¨
    ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’åˆ†å²
    """
    if "sessions" not in st.session_state:
        st.session_state.sessions = load_sessions()
    
    current_messages = get_current_messages()  # ã“ã‚Œã¯ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™
    current_sessions = st.session_state.sessions
    
    # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    current_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
    for session in current_sessions:
        if session["id"] == st.session_state.current_session_id:
            current_title = session["title"]
            break
    
    # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    new_id = str(uuid.uuid4())
    new_session = {
        "id": new_id,
        "title": f"{current_title} (åˆ†å²)",
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": list(current_messages),  # ãƒ‡ã‚£ãƒ¼ãƒ—ã‚³ãƒ”ãƒ¼
    }
    current_sessions.insert(0, new_session)
    
    # 1. ãƒ¡ãƒ¢ãƒªã‚’æ›´æ–°
    st.session_state.sessions = current_sessions
    st.session_state.current_session_id = new_id
    st.session_state.session_cost = 0.0  # ã‚³ã‚¹ãƒˆãƒªã‚»ãƒƒãƒˆ
    
    # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã¸ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
    save_sessions(st.session_state.sessions)
    st.rerun()

# =========================
# Initialization
# =========================

if "sessions" not in st.session_state:
    st.session_state.sessions = load_sessions()

if "current_session_id" not in st.session_state:
    # Always create a new session when the app starts
    create_new_session()

if "session_cost" not in st.session_state:
    st.session_state.session_cost = 0.0

usage_stats = load_usage()

# ==========================
# ã‚³ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã®å‰ã«å®Ÿè¡Œï¼‰
# ==========================
usage_stats = load_usage()
stop_generation = usage_stats["total_cost_usd"] >= MAX_BUDGET_USD

# =========================
# Sidebar
# =========================

with st.sidebar:
    # ğŸ” ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³
    if st.button("ğŸ”’ ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", width="stretch"):
        st.session_state.authenticated = False
        st.query_params.clear()  # URLãƒˆãƒ¼ã‚¯ãƒ³ã‚‚å‰Šé™¤
        st.rerun()

    st.markdown("""
    <div style="text-align: center; padding: 0; margin: 0; margin-top: -1rem;">
        <h1 style="font-size: 18px; font-weight: 700; margin: 0; padding: 0; letter-spacing: 1px;">
            Gemini 3<br/>Studio
        </h1>
    </div>
    """, unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• æ–°è¦", width="stretch"):
            create_new_session()
    with col2:
        if st.button("ğŸŒ± åˆ†å²", width="stretch"):
            branch_session()

    # ---- å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ ----
    # ---- å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ ----
    current_messages = get_current_messages()
    if current_messages:
        with st.expander("ğŸ”— å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ", expanded=False):
            export_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
            for s in st.session_state.sessions:
                if s["id"] == st.session_state.current_session_id:
                    export_title = s["title"]
                    break

            export_md = f"# {export_title}\n\n"
            export_md += f"**ä½œæˆæ—¥æ™‚**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
            for msg in current_messages:
                role_label = "ğŸ§‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg["role"] == "user" else "ğŸ¤– AI"
                export_md += f"## {role_label}\n\n{msg['content']}\n\n---\n\n"

            if st.button("ãƒªãƒ³ã‚¯ç”Ÿæˆ", width="stretch"):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    try:
                        import urllib.request
                        import urllib.parse

                        data = urllib.parse.urlencode(
                            {"content": export_md, "expiry_days": 7, "syntax": "md"}
                        ).encode()

                        req = urllib.request.Request("https://dpaste.org/api/", data=data)
                        req.add_header("User-Agent", "Mozilla/5.0 (Gemini3Studio)")

                        with urllib.request.urlopen(req) as response:
                            share_url = response.read().decode("utf-8").strip()
                            st.success("ä½œæˆå®Œäº†ï¼")
                            st.code(share_url)
                            st.caption("â€»7æ—¥é–“æœ‰åŠ¹")
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    st.markdown("---")

    # ---- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ ----
    # ---- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ ----
    with st.expander("ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«", expanded=False):
        uploaded_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«",
            accept_multiple_files=True,
            type=["png", "jpg", "jpeg", "mp4", "mov", "txt", "pdf", "csv"],
            label_visibility="collapsed"
        )

    # ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ï¼ˆExpanderã®å¤–ã«å‡ºã™ï¼‰
    pasted_image_bytes = None  # åˆæœŸåŒ–ã—ã¦NameErrorã‚’é˜²ã
    if paste:
        if "paste_key" not in st.session_state:
            st.session_state.paste_key = 0
        try:
            pasted_image_bytes = paste(
                label="ğŸ“‹ ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰è²¼ä»˜",
                key=f"paste_btn_{st.session_state.paste_key}",
            )
        except Exception as e:
            st.error(f"Error: {e}")
        
        if pasted_image_bytes:
            st.success("è²¼ä»˜å®Œäº†")
            st.image(pasted_image_bytes, caption="ç”»åƒ", width="stretch")
            if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", key="clear_paste"):
                st.session_state.paste_key += 1
                st.rerun()
    else:
        st.warning("Clipboard lib missing")

    # ---- YouTube URL ----
    with st.expander("ğŸ“º YouTubeåˆ†æ", expanded=False):
        youtube_url = st.text_input("URL", placeholder="https://youtu.be/...", label_visibility="collapsed")

    st.markdown("---")
    
    st.markdown("---")
    
    # ---- ãƒ¢ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªé¸æŠ ----
    mode_category = st.radio(
        "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰",
        ["ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)", "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(é€šå¸¸)"],
        index=0,  #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
        horizontal=True,
    )
    
    # ---- å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ ----
    if mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)":
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(å¤šå±¤)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["ğŸš€ æœ¬æ°—MAX", "ğŸ§ª ãƒ™ãƒ¼ã‚¿", "âš¡ è»½é‡", "ãã®ä»–"],
                index=0,
                horizontal=True,
                label_visibility="collapsed"
            )
            st.caption("æœ¬æ°—MAX=æ—§grå¼·åŒ–+msAz | ãƒ™ãƒ¼ã‚¿=æ—§gré€šå¸¸ | è»½é‡=æ—§grå¼·åŒ–")
            
            if mode_type == "ğŸš€ æœ¬æ°—MAX":
                # â–¼â–¼â–¼ ãƒ¡ãƒ‹ãƒ¥ãƒ¼ç°¡ç´ åŒ–: æœ¬æ°—MAXã‚’ãƒ¡ã‚¤ãƒ³ã« â–¼â–¼â–¼
                st.markdown("### ğŸš€ æ¨å¥¨ãƒ¢ãƒ¼ãƒ‰")
                response_mode = st.radio(
                    "å¿œç­”ãƒ¢ãƒ¼ãƒ‰:",
                    options=[
                        "ç†Ÿè€ƒ (æœ¬æ°—MAX)ms/Az",  # ãƒ¡ã‚¤ãƒ³æ¨å¥¨
                    ],
                    index=0,
                    key="response_mode"
                )
                
                with st.expander("ğŸ§ª ãã®ä»–ã®ãƒ¢ãƒ¼ãƒ‰ (ãƒ™ãƒ¼ã‚¿ç‰ˆ)", expanded=False):
                    beta_mode = st.radio(
                        "ãƒ™ãƒ¼ã‚¿ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ:",
                        options=[
                            "ä½¿ç”¨ã—ãªã„",
                            "ç†Ÿè€ƒ + é¬¼è»æ›¹",
                            "ç†Ÿè€ƒ(æœ¬æ°—MAX)/grok",
                            "ç†Ÿè€ƒ/grok",
                            "ç†Ÿè€ƒ (ä¸­è¦æ¨¡MAX)Az",
                            "ç†Ÿè€ƒ (æœ¬æ°—)ms",
                            "ç†Ÿè€ƒ (ä¸­è¦æ¨¡)", 
                            "Î²1é«˜é€Ÿ (é€šå¸¸)",
                        ],
                        index=0,
                        key="beta_mode"
                    )
                    if beta_mode != "ä½¿ç”¨ã—ãªã„":
                        response_mode = beta_mode
                # â–²â–²â–² ãƒ¡ãƒ‹ãƒ¥ãƒ¼ç°¡ç´ åŒ– ã“ã“ã¾ã§ â–²â–²â–²
            elif mode_type == "ğŸ§ª ãƒ™ãƒ¼ã‚¿":
                response_mode = st.radio(
                    "ãƒ™ãƒ¼ã‚¿ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                    ],
                    index=0
                )
            elif mode_type == "âš¡ è»½é‡":
                response_mode = st.radio(
                    "è»½é‡ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "ç†Ÿè€ƒ (ä¸­è¦æ¨¡)",
                        "Î²1é«˜é€Ÿ (é€šå¸¸)",
                    ],
                    index=0
                )
            else:  # ãã®ä»–
                response_mode = st.radio(
                    "ãã®ä»–",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    # Puterãƒ¢ãƒ¼ãƒ‰ã¯å‰Šé™¤ï¼ˆAWS Bedrockã«ç§»è¡Œï¼‰
    
    # ---- é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ ----
    else:
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(é€šå¸¸)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["é¸æŠ1 (å®Œå…¨ç‰ˆ)", "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)", "ãƒ™ãƒ¼ã‚¿ç‰ˆ"],
                index=0,
                horizontal=True,
                label_visibility="collapsed"
            )
            
            if mode_type == "é¸æŠ1 (å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "2. ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "3. ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                    ],
                    index=0
                )
            elif mode_type == "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                    ],
                    index=0
                )
            else:
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    strict_mode = False
    
    # ---- ãŠã™ã™ã‚ ----
    with st.expander("ğŸ’¡ ãŠã™ã™ã‚", expanded=False):
        # ãƒœã‚¿ãƒ³ã‚’ç¸¦ã«é…ç½®
        if st.button("âœ¨ ææ¡ˆ (ç›´è¿‘)", width="stretch"):
            with st.spinner("ç”Ÿæˆä¸­..."):
                rec_client = get_gemini_client()  # æ—©æœŸå®šç¾©æ¸ˆã¿é–¢æ•°ã‚’ä½¿ç”¨
                user_profile = load_user_profile()
                rec_text, usage = generate_recommendations(rec_client, st.session_state.sessions, st.session_state.current_session_id, user_profile, mode="normal")
                
                # ã‚³ã‚¹ãƒˆåŠ ç®—
                cost = calculate_cost("gemini-2.5-flash", usage["input_tokens"], usage["output_tokens"])
                st.session_state.session_cost += cost
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ä½¿ç”¨é‡ã®æ›´æ–°
                usage_stats["total_cost_usd"] += cost
                usage_stats["total_input_tokens"] += usage["input_tokens"]
                usage_stats["total_output_tokens"] += usage["output_tokens"]
                save_usage(usage_stats)
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾ä¿å­˜ï¼ˆCSSã§è‡ªå‹•æŠ˜ã‚Šè¿”ã—ï¼‰
                st.session_state.recommendation_text = rec_text
        
        st.markdown("") # éš™é–“

        if st.button("ğŸ”¥ ææ¡ˆ (å…¨å±¥æ­´)", width="stretch"):
            with st.spinner("å…¨å±¥æ­´åˆ†æä¸­..."):
                rec_client = get_gemini_client()  # æ—©æœŸå®šç¾©æ¸ˆã¿é–¢æ•°ã‚’ä½¿ç”¨
                user_profile = load_user_profile()
                rec_text, usage = generate_recommendations(rec_client, st.session_state.sessions, st.session_state.current_session_id, user_profile, mode="deep")
                
                # ã‚³ã‚¹ãƒˆåŠ ç®— (gemini-2.0-flash)
                cost = calculate_cost("gemini-2.0-flash", usage["input_tokens"], usage["output_tokens"])
                st.session_state.session_cost += cost
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ä½¿ç”¨é‡ã®æ›´æ–°
                usage_stats["total_cost_usd"] += cost
                usage_stats["total_input_tokens"] += usage["input_tokens"]
                usage_stats["total_output_tokens"] += usage["output_tokens"]
                save_usage(usage_stats)
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾ä¿å­˜ï¼ˆCSSã§è‡ªå‹•æŠ˜ã‚Šè¿”ã—ï¼‰
                st.session_state.recommendation_text = rec_text

        # çµæœè¡¨ç¤º (ãƒœã‚¿ãƒ³ã®ä¸‹ã«è¡¨ç¤º)
        if "recommendation_text" in st.session_state:
            st.markdown("---")
            st.markdown(
                f"<div class='recommendation-text'>{st.session_state.recommendation_text}</div>",
                unsafe_allow_html=True
            )

    # ---- è¨­å®š (ãƒ¢ãƒ‡ãƒ«ãªã©) ----
    with st.expander("âš™ï¸ è¨­å®š", expanded=False):
        model_options = [
            "gemini-3-pro-preview",
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.0-flash",
        ]
        model_id = st.selectbox("ãƒ¢ãƒ‡ãƒ«ID", options=model_options, index=0)

        use_search = st.toggle("Googleæ¤œç´¢", value=True)
        candidate_count = st.slider("å€™è£œæ•°", min_value=1, max_value=3, value=3)

    st.markdown("---")

    # ---- å±¥æ­´æ¤œç´¢ ----
    search_query = st.text_input("ğŸ” å±¥æ­´æ¤œç´¢", placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰...")
    if search_query:
        filtered_sessions = []
        for s in st.session_state.sessions:
            if search_query.lower() in s["title"].lower():
                filtered_sessions.append(s)
                continue
            found = False
            for m in s["messages"]:
                if search_query.lower() in m["content"].lower():
                    filtered_sessions.append(s)
                    found = True
                    break
            if not found:
                pass
    else:
        # æ¤œç´¢ã—ã¦ã„ãªã„å ´åˆ: ç©ºã®ã€Œæ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã€ã‚’é™¤å¤–ï¼ˆç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯é™¤ãï¼‰
        filtered_sessions = []
        for s in st.session_state.sessions:
            if s["id"] == st.session_state.current_session_id:
                filtered_sessions.append(s)
                continue
            if s["title"] == "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ" and len(s["messages"]) == 0:
                continue
            filtered_sessions.append(s)

    # CSSã§è¦‹ã‚„ã™ãä½¿ã„ã‚„ã™ãæœ€é©åŒ–
    st.markdown("""
    <style>
    /* å…¨ä½“ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºç¸®å° */
    .stApp, .stMarkdown, .stButton, .stSelectbox, .stTextInput, .stTextArea {
        font-size: 12px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä½™ç™½ã‚’æ¥µé™ã¾ã§è©°ã‚ã‚‹ */
    section[data-testid="stSidebar"] .block-container {
        padding-top: 0.3rem !important;
        padding-bottom: 0.3rem !important;
        padding-left: 0.5rem !important;
        padding-right: 0.5rem !important;
    }
    /* å„è¦ç´ é–“ã®éš™é–“ã‚’è©°ã‚ã‚‹ */
    div[data-testid="stVerticalBlock"] > div {
        gap: 0.1rem !important;
    }
    /* Expanderã®ä½™ç™½å‰Šæ¸› */
    .streamlit-expanderHeader {
        font-size: 11px !important;
        padding-top: 0px !important;
        padding-bottom: 0px !important;
        min-height: 1.3rem !important;
    }
    .streamlit-expanderContent {
        padding-top: 0px !important;
        padding-bottom: 0px !important;
    }
    /* éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ã‚¿ã‚¤ãƒˆãƒ« */
    div[data-testid="stExpander"] summary p {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ */
    section[data-testid="stSidebar"] button p {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã« */
    section[data-testid="stSidebar"] button {
        padding: 0rem 0.2rem !important;
        min-height: 1.4rem !important;
        font-size: 10px !important;
        margin-bottom: 0px !important;
    }
    /* ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®ãƒ©ãƒ™ãƒ«ã‚’å°ã•ã */
    section[data-testid="stSidebar"] .stRadio label {
        font-size: 10px !important;
    }
    section[data-testid="stSidebar"] .stRadio > label > div {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³åãªã©ã‚’1è¡Œã«åã‚ã‚‹ */
    section[data-testid="stSidebar"] label, 
    section[data-testid="stSidebar"] p,
    section[data-testid="stSidebar"] span {
        white-space: nowrap !important;
        overflow: hidden !important;
        text-overflow: ellipsis !important;
    }
    
    /* ğŸ’¡ãŠã™ã™ã‚ã‚¨ãƒªã‚¢ã ã‘ã¯æŠ˜ã‚Šè¿”ã—ï¼†æ”¹è¡Œã‚’è¨±å¯ */
    section[data-testid="stSidebar"] .recommendation-text,
    section[data-testid="stSidebar"] .recommendation-text p {
        white-space: pre-wrap !important;
        overflow: visible !important;
        text-overflow: clip !important;
        line-height: 1.4;
        font-size: 11px;
    }
    /* åŒºåˆ‡ã‚Šç·š */
    hr {
        margin-top: 0.1rem !important;
        margin-bottom: 0.1rem !important;
    }
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ç¸®å° */
    h1, h2, h3 {
        padding-top: 0rem !important;
        padding-bottom: 0rem !important;
        margin-top: 0rem !important;
        margin-bottom: 0rem !important;
    }
    
    /* ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…ã®ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’æŠ˜ã‚Šè¿”ã™ */
    [data-testid="stChatMessageContent"] code {
        white-space: pre-wrap !important;
        word-break: break-word !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§é™é †ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°ãŒå…ˆé ­ï¼‰
    filtered_sessions.sort(key=lambda s: s.get("timestamp", ""), reverse=True)
    
    # ç›´è¿‘5ä»¶ã¨éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«åˆ†å‰²
    recent_sessions = filtered_sessions[:5] if len(filtered_sessions) > 5 else filtered_sessions
    archive_sessions = filtered_sessions[5:] if len(filtered_sessions) > 5 else []
    
    # ç›´è¿‘5ä»¶ï¼ˆå¸¸ã«å±•é–‹ï¼‰
    if recent_sessions:
        st.markdown("**ğŸ“Œ ç›´è¿‘ã®ãƒãƒ£ãƒƒãƒˆ**")
        for session in recent_sessions:
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                if st.button(session["title"], key=f"btn_{session['id']}", width="stretch"):
                    switch_session(session["id"])
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_{session['id']}"):
                    delete_session(session["id"])
    
    # éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
    if archive_sessions:
        with st.expander(f"ğŸ“œ éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– ({len(archive_sessions)}ä»¶)", expanded=False):
            for session in archive_sessions:
                col1, col2 = st.columns([0.8, 0.2])
                with col1:
                    if st.button(session["title"], key=f"btn_{session['id']}", width="stretch"):
                        switch_session(session["id"])
                with col2:
                    if st.button("ğŸ—‘ï¸", key=f"del_{session['id']}"):
                        delete_session(session["id"])

    st.markdown("---")

    # ---- ç”»åƒç”Ÿæˆ ----
    with st.expander("ğŸ¨ ç”»åƒç”Ÿæˆ", expanded=False):
        img_prompt = st.text_area("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", placeholder="æœªæ¥çš„ãªéƒ½å¸‚...")
        aspect_ratio = st.selectbox("æ¯”ç‡", ["16:9", "1:1", "4:3", "3:4", "9:16"])

        generate_img_btn = st.button("ç”Ÿæˆ", type="primary", disabled=not img_prompt)
        if generate_img_btn and img_prompt:
            st.session_state.generate_image_trigger = {
                "prompt": img_prompt,
                "aspect_ratio": aspect_ratio,
            }

    st.markdown("---")

    # ---- è©•ä¾¡åˆ†æ ----
    with st.expander("ğŸ“Š å“è³ªåˆ†æ", expanded=False):
        total_ratings = 0
        positive_ratings = 0
        model_ratings = {}

        for s in st.session_state.sessions:
            for m in s["messages"]:
                if "rating" in m and m["rating"] is not None:
                    total_ratings += 1
                    if m["rating"] == 1:
                        positive_ratings += 1
                    if "metadata" in m and "model" in m["metadata"]:
                        mod = m["metadata"]["model"]
                        if mod not in model_ratings:
                            model_ratings[mod] = {"total": 0, "positive": 0}
                        model_ratings[mod]["total"] += 1
                        if m["rating"] == 1:
                            model_ratings[mod]["positive"] += 1

        if total_ratings > 0:
            approval_rate = (positive_ratings / total_ratings) * 100
            st.metric("Positive Ratings", f"{positive_ratings}/{total_ratings}", f"{approval_rate:.1f}%")
            best_model = "N/A"
            best_rate = -1
            for mod, stats in model_ratings.items():
                rate = stats["positive"] / stats["total"]
                if rate > best_rate:
                    best_rate = rate
                    best_model = mod
            if best_model != "N/A":
                st.caption(f"ğŸ† Best: **{best_model}** ({best_rate*100:.0f}%)")
        else:
            st.caption("ãƒ‡ãƒ¼ã‚¿ãªã—")

    st.markdown("---")

    # ---- ã‚³ã‚¹ãƒˆè¡¨ç¤º ----
    from logic import MAX_BUDGET_JPY, TRIAL_LIMIT_JPY, TRIAL_EXPIRY
    
    st.subheader("ğŸ’° Cost")
    st.caption(f"Geminiäºˆç®—: Â¥45,000 ($300) | AWS: Â¥15,000 ($100)")
    st.caption(f"æœ‰åŠ¹æœŸé™ - GCP: {TRIAL_EXPIRY} | AWS: Jun 02, 2026")
    
    # â–¼â–¼â–¼ ã‚³ã‚¹ãƒˆè¡¨ç¤ºï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰ â–¼â–¼â–¼
    GEMINI_BUDGET_USD = 300.0
    AWS_BUDGET_USD = 100.0
    GEMINI_COST_PER_RUN = 1.8
    AWS_COST_PER_RUN = 0.2
    
    session_cost = usage_stats['total_cost_usd']
    gemini_est = session_cost * 0.85
    aws_est = session_cost * 0.15
    
    gemini_runs = max(0, int((GEMINI_BUDGET_USD - gemini_est) / GEMINI_COST_PER_RUN))
    aws_runs = max(0, int((AWS_BUDGET_USD - aws_est) / AWS_COST_PER_RUN))
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä½¿ç”¨é‡ã®ã¿è¡¨ç¤ºï¼‰
    st.progress(min(1.0, session_cost / 50.0))  # 1ã‚»ãƒƒã‚·ãƒ§ãƒ³50$ã‚’100%ã¨ã—ã¦è¡¨ç¤º
    
    st.markdown(f"<small>ğŸ“Š ä»Šã‚»ãƒƒã‚·ãƒ§ãƒ³: ${session_cost:.2f} | Gemini {gemini_runs}å›ç›¸å½“ | AWS {aws_runs}å›ç›¸å½“</small>", unsafe_allow_html=True)
    st.caption("âš ï¸ å®Ÿéš›ã®è«‹æ±‚é¡ã¯GCP/AWSã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ç¢ºèªã—ã¦ãã ã•ã„")
    # â–²â–²â–² ã‚³ã‚¹ãƒˆè¡¨ç¤º ã“ã“ã¾ã§ â–²â–²â–²
    
    st.link_button("ğŸ’° Google Cloud Console", "https://console.cloud.google.com/welcome/new?_gl=1*kmr691*_up*MQ..&gclid=CjwKCAiAraXJBhBJEiwAjz7MZT0vQsfDK5zunRBCQmuN5iczgI4bP1lHo1Tcrcbqu1KCBE1D22GpFhoCOdgQAvD_BwE&gclsrc=aw.ds&hl=ja&authuser=5&project=sigma-task-479704-r6")
    st.link_button("â˜ï¸ AWS Free Tier Dashboard", "https://us-east-1.console.aws.amazon.com/costmanagement/home?region=us-east-1#/freetier")
    st.caption("ğŸ“˜ GitHub Models: ä½¿ç”¨çŠ¶æ³ã¯ [Settings â†’ Developer settings â†’ Tokens](https://github.com/settings/tokens) ã§ç¢ºèª")
    
    # â–¼â–¼â–¼ Debug: API Key Status â–¼â–¼â–¼
    with st.expander("ğŸ” API Status (Debug)", expanded=False):
        # è©³ç´°ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        aws_ok = bool(AWS_ACCESS_KEY_ID and AWS_ACCESS_KEY_ID.strip())
        openrouter_ok = bool(OPENROUTER_API_KEY and OPENROUTER_API_KEY.strip())
        github_ok = bool(GITHUB_TOKEN and GITHUB_TOKEN.strip())
        
        st.caption(f"AWS: {'âœ…' if aws_ok else 'âŒ'}")
        st.caption(f"OpenRouter: {'âœ…' if openrouter_ok else 'âŒ'}")
        st.caption(f"GitHub: {'âœ…' if github_ok else 'âŒ'}")
        
        # Secretsè©³ç´°ãƒ‡ãƒãƒƒã‚°
        st.caption("---")
        try:
            secrets_keys = list(st.secrets.keys()) if hasattr(st.secrets, 'keys') else []
            st.caption(f"Secrets keys available: {len(secrets_keys)}")
            if secrets_keys:
                st.caption(f"Keys: {', '.join([k for k in secrets_keys if not k.startswith('GOOGLE_CREDENTIALS')])}")
            st.caption(f"AWS in secrets: {'AWS_ACCESS_KEY_ID' in st.secrets}")
            st.caption(f"OPENROUTER in secrets: {'OPENROUTER_API_KEY' in st.secrets}")
            st.caption(f"GITHUB in secrets: {'GITHUB_TOKEN' in st.secrets}")
        except Exception as e:
            st.caption(f"Secrets check error: {e}")
        
        # ç’°å¢ƒå¤‰æ•°ç¢ºèª
        st.caption("---")
        st.caption(f"OS env AWS: {bool(os.getenv('AWS_ACCESS_KEY_ID'))}")
        st.caption(f"OS env OPENROUTER: {bool(os.getenv('OPENROUTER_API_KEY'))}")
        st.caption(f"OS env GITHUB: {bool(os.getenv('GITHUB_TOKEN'))}")
    # â–²â–²â–² Debug â–²â–²â–²

    st.markdown("---")
    st.code(f"PROJECT: {VERTEX_PROJECT}\nLOCATION: {VERTEX_LOCATION} (Vertex AI)")

# =========================
# Main UI
# =========================

current_session_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
for s in st.session_state.sessions:
    if s["id"] == st.session_state.current_session_id:
        current_session_title = s["title"]
        break

st.header(current_session_title)
st.markdown(
    "ä»¥ä¸‹ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€YouTubeåˆ†æã€æ¤œç´¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
)


# ---- ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³ (Floating) ----
st.markdown("""
    <style>
        .scroll-btn-container {
            position: fixed;
            bottom: 100px;
            right: 20px;
            z-index: 9999;
            display: flex;
            flex-direction: column;
            gap: 10px;
        }
        .scroll-btn {
            background-color: #f0f2f6;
            color: #31333F;
            border: 1px solid #d6d6d8;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            font-size: 20px;
            padding: 0;
            line-height: 1;
        }
        .scroll-btn:hover {
            background-color: #e0e2e6;
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
        }
    </style>
    
    <script>
        window.scrollStreamlit = function(direction) {
            console.log("Scroll triggered: " + direction);
            
            // ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹è¦ç´ ã‚’é †ç•ªã«è©¦ã™
            var targets = [];
            
            try {
                if (window.parent && window.parent.document) {
                    targets.push(window.parent.document.querySelector('section[data-testid="stAppViewContainer"]'));
                    targets.push(window.parent.document.querySelector('.main'));
                    targets.push(window.parent.document.documentElement);
                }
            } catch (e) {
                console.log("Access to parent window denied");
            }
            
            // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆiframeå†…ï¼‰
            targets.push(document.querySelector('section[data-testid="stAppViewContainer"]'));
            targets.push(document.documentElement);

            var scrolled = false;
            for (var i = 0; i < targets.length; i++) {
                var el = targets[i];
                if (el) {
                    try {
                        // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªè¦ç´ ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“çš„ï¼‰
                        if (el.scrollHeight > el.clientHeight || el === window.parent.document.documentElement) {
                            console.log("Scrolling element:", el);
                            if (direction === 'top') {
                                el.scrollTo({top: 0, behavior: 'smooth'});
                            } else {
                                el.scrollTo({top: el.scrollHeight, behavior: 'smooth'});
                            }
                            scrolled = true;
                        }
                    } catch (e) {
                        console.error("Error scrolling element:", e);
                    }
                }
            }
            
            if (!scrolled) {
                console.log("No scrollable container found, trying window scroll");
                try {
                    if (direction === 'top') {
                        window.parent.scrollTo({top: 0, behavior: 'smooth'});
                    } else {
                        window.parent.scrollTo({top: window.parent.document.body.scrollHeight, behavior: 'smooth'});
                    }
                } catch(e) {
                    window.scrollTo({top: 0, behavior: 'smooth'});
                }
            }
        }
    </script>

    <div class="scroll-btn-container">
        <button class="scroll-btn" onclick="window.scrollStreamlit('top')" title="Top">â¬†ï¸</button>
        <button class="scroll-btn" onclick="window.scrollStreamlit('bottom')" title="Bottom">â¬‡ï¸</button>
    </div>
    """, unsafe_allow_html=True)

# ---- Vertex AI Client ----



# =========================
# Initialize Gemini Client (function defined at line 81)
# =========================
# Initialize client
client = get_gemini_client()

# Store initialization error for display
init_error = None
if client is None:
    # Try to get the actual error message
    try:
        import sys
        # Re-run to capture exception
        test_client = get_gemini_client()
    except Exception as e:
        init_error = str(e)

# Check if client is ready
if client is None:
    st.error("âŒ Gemini APIåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
    if init_error:
        st.error(f"**ã‚¨ãƒ©ãƒ¼è©³ç´°:** {init_error}")
    st.info("ğŸ’¡ Streamlit Cloudã®å ´åˆ: ã€ŒManage appã€â†’ã€ŒSettingsã€â†’ã€ŒSecretsã€ã§`GOOGLE_CREDENTIALS`ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    st.info("ğŸ’¡ ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã®å ´åˆ: `gcloud auth application-default login`ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
    # Show debug info
    with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=True):
        st.code(f"VERTEX_PROJECT = {VERTEX_PROJECT}")
        st.code(f"VERTEX_LOCATION = {VERTEX_LOCATION}")
        st.code(f"Has GOOGLE_CREDENTIALS in secrets = {'GOOGLE_CREDENTIALS' in st.secrets}")
        if "GOOGLE_CREDENTIALS" in st.secrets:
            creds = dict(st.secrets["GOOGLE_CREDENTIALS"])
            st.code(f"project_id in credentials = {creds.get('project_id')}")
    st.stop()

# ---- å±¥æ­´è¡¨ç¤º ----

# ãƒãƒ£ãƒƒãƒˆå…ˆé ­ã«ã‚¢ãƒ³ã‚«ãƒ¼ã‚’è¨­ç½®
st.markdown('<a id="chat-top"></a>', unsafe_allow_html=True)

messages = get_current_messages()
for idx, msg in enumerate(messages):
    with st.chat_message(msg["role"]):
        # Display timestamp if available
        if "timestamp" in msg:
            st.caption(f"ğŸ•’ {msg['timestamp']}")
        st.markdown(msg["content"])
        if msg["role"] == "model":
            col1, col2, col3 = st.columns([0.1, 0.1, 0.8])
            current_rating = msg.get("rating")
            with col1:
                if st.button("ğŸ‘", key=f"up_{idx}"):
                    messages[idx]["rating"] = 1
                    update_current_session_messages(messages)
                    st.rerun()
            with col2:
                if st.button("ğŸ‘", key=f"down_{idx}"):
                    messages[idx]["rating"] = -1
                    update_current_session_messages(messages)
                    st.rerun()
            with col3:
                if current_rating == 1:
                    st.caption("âœ… é«˜è©•ä¾¡")
                elif current_rating == -1:
                    st.caption("âŒ ä½è©•ä¾¡")
            
            # â–¼â–¼â–¼ Deep Log: ä¿å­˜ã•ã‚ŒãŸæ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã®è¡¨ç¤º â–¼â–¼â–¼
            if "reasoning_logs" in msg and msg["reasoning_logs"]:
                with st.expander("ğŸ§  æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ (Deep Log)", expanded=False):
                    logs = msg["reasoning_logs"]
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                    if "metadata" in msg:
                        meta = msg["metadata"]
                        st.caption(f"ğŸ¤– Model: {meta.get('model', 'N/A')} | ğŸ’° Cost: ${meta.get('cost', 0):.4f}")
                    
                    if logs.get("phase1_research"):
                        st.markdown("### ğŸ“š Phase 1: èª¿æŸ»ãƒ¡ãƒ¢")
                        st.markdown(logs["phase1_research"][:2000] + "..." if len(logs.get("phase1_research", "")) > 2000 else logs["phase1_research"])
                        st.markdown("---")
                    
                    if logs.get("phase1_5b_secondary"):
                        st.markdown(f"### âš¡ Phase 1.5b: ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ«ã®è¦–ç‚¹")
                        st.markdown(logs["phase1_5b_secondary"][:1500] + "..." if len(logs.get("phase1_5b_secondary", "")) > 1500 else logs["phase1_5b_secondary"])
                        st.markdown("---")
                    
                    if logs.get("phase1_5d_claude"):
                        st.markdown("### ğŸ§  Phase 1.5d: Claude 4.5 Sonnet ã®è¦–ç‚¹")
                        st.markdown(logs["phase1_5d_claude"][:1500] + "..." if len(logs.get("phase1_5d_claude", "")) > 1500 else logs["phase1_5d_claude"])
            # â–²â–²â–² Deep Log ã“ã“ã¾ã§ â–²â–²â–²

# ãƒãƒ£ãƒƒãƒˆæœ«å°¾ã«ã‚¢ãƒ³ã‚«ãƒ¼è¨­ç½® + ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯
st.markdown('<a id="chat-bottom"></a>', unsafe_allow_html=True)

# ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ï¼ˆé•·ã„ãƒãƒ£ãƒƒãƒˆç”¨ï¼‰
if len(messages) > 5:
    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        st.markdown('[â¬†ï¸ ãƒãƒ£ãƒƒãƒˆå…ˆé ­ã¸](#chat-top)', unsafe_allow_html=True)
    with col3:
        st.markdown('[â¬‡ï¸ æœ€æ–°ã¸](#chat-bottom)', unsafe_allow_html=True)


# =========================
# ç”»åƒç”Ÿæˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
# =========================

if hasattr(st.session_state, "generate_image_trigger") and st.session_state.generate_image_trigger:
    img_data = st.session_state.generate_image_trigger
    del st.session_state.generate_image_trigger

    with st.chat_message("user"):
        st.markdown(f"ğŸ¨ ç”»åƒç”Ÿæˆ: {img_data['prompt']}")
        st.caption(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”: {img_data['aspect_ratio']}")

    messages.append({
        "role": "user",
        "content": f"ğŸ¨ ç”»åƒç”Ÿæˆ: {img_data['prompt']}",
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    })
    update_current_session_messages(messages)

    with st.chat_message("assistant"):
        with st.status("ç”»åƒã‚’ç”Ÿæˆä¸­...", expanded=True) as status:
            try:
                config = types.GenerateContentConfig(
                    response_modalities=["IMAGE"],
                    image_config=types.ImageConfig(
                        aspect_ratio=img_data["aspect_ratio"]
                    ),
                )
                status.write("Gemini 3 Pro Imageã§ç”Ÿæˆä¸­...")
                response = client.models.generate_content(
                    model="gemini-3-pro-image-preview",
                    contents=img_data["prompt"],
                    config=config,
                )

                generated_image = None

                # ç”»åƒã‚’å–ã‚Šå‡ºã™
                if getattr(response, "candidates", None):
                    for candidate in response.candidates:
                        if getattr(candidate, "content", None) and candidate.content.parts:
                            for part in candidate.content.parts:
                                if getattr(part, "inline_data", None):
                                    try:
                                        generated_image = part.as_image()
                                    except AttributeError:
                                        image_bytes_raw = part.inline_data.data
                                        generated_image = Image.open(io.BytesIO(image_bytes_raw))
                                    break
                        if generated_image is not None:
                            break

                if generated_image is not None:
                    st.image(generated_image, caption=img_data["prompt"])
                    buf = io.BytesIO()
                    generated_image.save(buf, format="PNG")
                    image_bytes = buf.getvalue()
                    st.download_button(
                        label="ğŸ’¾ ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=image_bytes,
                        file_name=f"generated_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                        mime="image/png",
                    )
                    messages.append(
                        {"role": "model",
                        "content": f"âœ… ç”»åƒã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {img_data['prompt']}",
                        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                    )
                    update_current_session_messages(messages)
                    status.update(label="âœ… ç”»åƒç”Ÿæˆå®Œäº†", state="complete")
                else:
                    st.error("ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error")

            except Exception as e:
                status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error")
                st.error(f"ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

# =========================
# Budget Check & Warnings
# =========================
stop_generation = usage_stats["total_cost_usd"] >= MAX_BUDGET_USD

# Show budget status in sidebar
with st.sidebar:
    st.caption("---")
    st.caption(f"ğŸ’° ç¾åœ¨ã®ã‚³ã‚¹ãƒˆ: ${usage_stats['total_cost_usd']:.4f} / ${MAX_BUDGET_USD:.2f}")
    if stop_generation:
        st.warning("âš ï¸ äºˆç®—ä¸Šé™ã«é”ã—ã¦ã„ã¾ã™")

# Show warning in main area if budget exceeded
if stop_generation:
    st.warning(
        "âš ï¸ **ã‚³ã‚¹ãƒˆä¸Šé™ã«é”ã—ã¾ã—ãŸ**\n\n"
        f"ç¾åœ¨ã®ã‚³ã‚¹ãƒˆ: ${usage_stats['total_cost_usd']:.4f} / ä¸Šé™: ${MAX_BUDGET_USD:.2f}\n\n"
        "æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ä¸€æ™‚çš„ã«ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚é–‹ç™ºä¸­ã¯logic.pyã®`MAX_BUDGET_USD`ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚"
    )

# =========================
# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
# =========================
prompt = st.chat_input("ä½•ã‹èã„ã¦ãã ã•ã„...")

if prompt:
    # Budget check at submission time
    if stop_generation:
        st.error("âŒ ã‚³ã‚¹ãƒˆä¸Šé™ã«é”ã—ã¦ã„ã‚‹ãŸã‚ã€ã“ã®å®Ÿè¡Œã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚äºˆç®—è¨­å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
        st.info(f"ç¾åœ¨: ${usage_stats['total_cost_usd']:.4f} / ä¸Šé™: ${MAX_BUDGET_USD:.2f}")
        st.stop()
    
    # ---- ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€è¡¨ç¤º ----
    with st.chat_message("user"):
        # ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ä»˜ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
        import html
        escaped_prompt = html.escape(prompt)
        message_id = f"user_msg_{len(messages)}"
        
        st.markdown(f"""
<div style="position: relative;">
    <div id="{message_id}" style="padding-right: 40px;">{escaped_prompt}</div>
    <button onclick="copyToClipboard('{message_id}')" style="position: absolute; right: 0; top: 0; background: transparent; border: 1px solid #444; border-radius: 4px; cursor: pointer; padding: 4px 8px; color: #aaa; font-size: 12px;" title="ã‚³ãƒ”ãƒ¼">
        ğŸ“‹
    </button>
</div>
<script>
function copyToClipboard(elementId) {{
    const element = document.getElementById(elementId);
    const text = element.innerText;
    navigator.clipboard.writeText(text).then(() => {{
        // ã‚³ãƒ”ãƒ¼æˆåŠŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        const button = event.target;
        button.textContent = 'âœ“';
        setTimeout(() => {{ button.textContent = 'ğŸ“‹'; }}, 1000);
    }});
}}
</script>
""", unsafe_allow_html=True)
        if uploaded_files:
            for uf in uploaded_files:
                st.caption(f"ğŸ“ æ·»ä»˜: {uf.name}")
        if youtube_url:
            st.caption(f"ğŸ“º YouTube: {youtube_url}")
        if pasted_image_bytes:
            st.caption("ğŸ“‹ ç”»åƒãŒè²¼ã‚Šä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ")

    messages.append({
        "role": "user",
        "content": prompt,
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    })
    update_current_session_messages(messages)

    # ========================================
    # ãƒ¢ãƒ‡ãƒ«å¿œç­”
    # ========================================
    with st.chat_message("assistant"):
        with st.status("æ€è€ƒä¸­...", expanded=True) as status_container:
            try:
                # éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¢ãƒ‡ãƒ«ã®å±¥æ­´ã«å¤‰æ›
                model_history = []
                for msg in messages[:-1]:  # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆ¥é€”è¿½åŠ 
                    if msg["role"] == "user":
                        model_history.append(
                            types.Content(
                                role="user",
                                parts=[types.Part.from_text(text=msg["content"])],
                            )
                        )
                    elif msg["role"] == "model":
                        model_history.append(
                            types.Content(
                                role=msg["role"],
                                parts=[types.Part.from_text(text=msg["content"])],
                            )
                        )

                # ç¾åœ¨ã®ã‚¿ãƒ¼ãƒ³
                current_parts = [types.Part.from_text(text=prompt)]

                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
                for uploaded_file in uploaded_files or []:
                    try:
                        mime_type = get_mime_type(uploaded_file.name)
                        bytes_data = uploaded_file.getvalue()
                        part = types.Part.from_bytes(data=bytes_data, mime_type=mime_type)
                        current_parts.append(part)
                        status_container.write(f"ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™å®Œäº†: {uploaded_file.name}")
                    except Exception as e:
                        status_container.error(
                            f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {uploaded_file.name} - {e}"
                        )

                # è²¼ã‚Šä»˜ã‘ç”»åƒ
                if pasted_image_bytes:
                    import base64

                    status_container.write("è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã‚’å‡¦ç†ä¸­...")
                    try:
                        if isinstance(pasted_image_bytes, str):
                            if pasted_image_bytes.startswith("data:"):
                                base64_str = pasted_image_bytes.split(",", 1)[1]
                                image_bytes_decoded = base64.b64decode(base64_str)
                            else:
                                image_bytes_decoded = base64.b64decode(pasted_image_bytes)
                        else:
                            image_bytes_decoded = pasted_image_bytes
                        part = types.Part.from_bytes(
                            data=image_bytes_decoded, mime_type="image/png"
                        )
                        current_parts.append(part)
                        status_container.write("è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã®æº–å‚™å®Œäº†")
                    except Exception as e:
                        status_container.error(f"è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

                # YouTube å­—å¹•
                if youtube_url:
                    vid_id = extract_youtube_id(youtube_url)
                    if vid_id:
                        status_container.write("YouTubeã®å­—å¹•ã‚’å–å¾—ä¸­...")
                        transcript_text = get_youtube_transcript(vid_id)
                        current_parts.append(
                            types.Part.from_text(text=f"YouTube Transcript:\n{transcript_text}")
                        )
                    else:
                        status_container.write("ç„¡åŠ¹ãªYouTube URLã§ã™ã€‚")

                contents_for_model = model_history + [
                    types.Content(role="user", parts=current_parts)
                ]

                # ---- Tool / Config ----
                tools = []
                final_candidate_count = candidate_count
                if use_search:
                    tools.append(types.Tool(google_search=types.GoogleSearch()))
                    final_candidate_count = 1

                # â˜… System Instruction ã®æ”¹å–„: ãƒ¡ã‚¿ç™ºè¨€ç¦æ­¢ã¨ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªæŒ¯ã‚‹èˆã„ã‚’æŒ‡ç¤º
                base_system_instruction = (
                    "ã‚ãªãŸã¯é«˜åº¦ãªçŸ¥æ€§ã‚’æŒã¤å°‚é–€çš„ãªãƒªã‚µãƒ¼ãƒãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
                    "\n"
                    "**ã€åˆ¤æ–­æ†²æ³• - ã“ã®åŸå‰‡ã«å¾“ã£ã¦ãã ã•ã„ã€‘**\n"
                    "ãƒ»å®‰å…¨æ€§ > å“è³ª > ã‚³ã‚¹ãƒˆ > ã‚¹ãƒ”ãƒ¼ãƒ‰ ã®å„ªå…ˆé †ä½ã§åˆ¤æ–­ã™ã‚‹\n"
                    "ãƒ»å–ã‚Šè¿”ã—ã®ã¤ã‹ãªã„ãƒªã‚¹ã‚¯ã¯çµ¶å¯¾ã«é¿ã‘ã‚‹ï¼ˆäººå‘½ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€æ³•ä»¤é•åï¼‰\n"
                    "ãƒ»ä¸ç¢ºå®Ÿãªæƒ…å ±ã¯å¿…ãšæ˜ç¤ºã—ã€ç¢ºä¿¡ãŒæŒã¦ãªã„å ´åˆã¯ã€Œè‡ªä¿¡åº¦: Lowã€ã¨è¨˜è¼‰ã™ã‚‹\n"
                    "ãƒ»è¤‡æ•°ã®é¸æŠè‚¢ãŒã‚ã‚‹å ´åˆã¯ã€ãƒªã‚¹ã‚¯ã¨ãƒªã‚¿ãƒ¼ãƒ³ã‚’å®šé‡çš„ã«æ¯”è¼ƒã™ã‚‹\n"
                    "\n"
                    "ä»¥ä¸‹ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼š\n"
                    "1. **ãƒ¡ã‚¿ç™ºè¨€ã®ç¦æ­¢**: ã€Œç§ã¯AIã§ã™ã€ã€Œä¸–ç•Œæœ€é«˜å³°ã®ï½ã¨ã—ã¦ã€ãªã©ã®è‡ªå·±è¨€åŠã‚„å‰ç½®ãã¯ä¸€åˆ‡è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚\n"
                    "2. **æ„å›³ã®æ±²ã¿å–ã‚Š**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®èƒŒå¾Œã«ã‚ã‚‹æ„å›³ï¼ˆæ–‡è„ˆã€æš—é»™ã®å‰æï¼‰ã‚’æ¨æ¸¬ã—ã€è¨€è‘‰é€šã‚Šã§ã¯ãªãã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæœ¬å½“ã«çŸ¥ã‚ŠãŸã„ã“ã¨ã€ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n"
                    "3. **æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”**: çµè«–ã‚’å…ˆã«è¿°ã¹ã€ãã®å¾Œã«è©³ç´°ãªæ ¹æ‹ ã€ã‚·ãƒŠãƒªã‚ªåˆ†æã€ãƒªã‚¹ã‚¯è¦å› ã‚’è«–ç†çš„ã«å±•é–‹ã—ã¦ãã ã•ã„ã€‚\n"
                    "4. **å®¢è¦³æ€§**: äºˆæ¸¬ã‚’è¡Œã†å ´åˆã¯ã€æ–­å®šã‚’é¿ã‘ã€è¤‡æ•°ã®ã‚·ãƒŠãƒªã‚ªï¼ˆæ¥½è¦³ã€æ‚²è¦³ã€ä¸­ç«‹ï¼‰ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                    "5. **å¼•ç”¨**: æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸå ´åˆã¯ã€å¿…ãšæƒ…å ±æºã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                    "6. **æ”¹è¡Œã®åˆ¶é™**: Markdownã¯ä½¿ç”¨ã—ã¦ã‚ˆã„ã§ã™ãŒã€é€£ç¶šã™ã‚‹ç©ºè¡Œã¯1è¡Œã¾ã§ã«ã—ã¦ãã ã•ã„ã€‚"
                )

                final_answer = ""
                grounding_metadata = None
                
                # =========================
                # Manual Mode Settings
                # =========================
                # Î²1é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ä»¥å¤–ã¯ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
                enable_research = "Î²1" not in response_mode
                enable_meta = "ãƒ¡ã‚¿" in response_mode or "MAX" in response_mode or "grok" in response_mode
                enable_strict = "é¬¼è»æ›¹" in response_mode or "MAX" in response_mode
                
                # Grok Xæ¤œç´¢ã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹/ãƒˆãƒ¬ãƒ³ãƒ‰ç³»ã®ã¿
                def should_use_x_search(prompt: str) -> bool:
                    keywords = ["Xã§", "Twitter", "ãƒ„ã‚¤ãƒƒã‚¿ãƒ¼", "ãƒã‚¹ãƒˆ", "ãƒˆãƒ¬ãƒ³ãƒ‰", "ç‚ä¸Š", "ãƒã‚º", "è©±é¡Œ"]
                    return any(kw in prompt for kw in keywords)
                
                enable_grok_x_search = "grok" in response_mode and should_use_x_search(prompt)

                # =========================
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ (é«˜é€Ÿ / é¬¼è»æ›¹)
                # =========================
                if not enable_research:
                    config = types.GenerateContentConfig(
                        temperature=0.8,  # Phase A: ã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—ãƒ•ã‚§ãƒ¼ã‚º - å¤šæ§˜æ€§é‡è¦–
                        candidate_count=1,
                        tools=tools,
                        system_instruction=base_system_instruction,
                    )
                    
                    status_container.write("å›ç­”ç”Ÿæˆä¸­...")
                    response = client.models.generate_content(
                        model=model_id,
                        contents=contents_for_model,
                        config=config,
                    )
                    
                    final_answer = extract_text_from_response(response)
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®—
                    if response.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            response.usage_metadata.prompt_token_count,
                            response.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += (response.usage_metadata.prompt_token_count or 0)
                        usage_stats["total_output_tokens"] += (response.usage_metadata.candidates_token_count or 0)

                    # é¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ (é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ç‰ˆ)
                    if enable_strict:
                        status_container.write("ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                        reviewer_instruction = base_system_instruction + """
**ã‚ãªãŸã®å½¹å‰²**: é¬¼è»æ›¹ãƒ¬ãƒ™ãƒ«ã®å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢
**ã‚¿ã‚¹ã‚¯**: åˆç‰ˆå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’è¿”ã™
**å‡ºåŠ›**: ä¿®æ­£ç‰ˆã®å›ç­”å…¨æ–‡ã®ã¿
"""
                        review_contents = [types.Content(role="user", parts=[types.Part(text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {prompt}\n\nåˆç‰ˆå›ç­”:\n{final_answer}\n\nãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ä¿®æ­£ç‰ˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚")])]
                        review_resp = client.models.generate_content(
                            model=model_id,
                            contents=review_contents,
                            config=types.GenerateContentConfig(temperature=0.1, candidate_count=1, system_instruction=reviewer_instruction)
                        )
                        final_answer = extract_text_from_response(review_resp)
                        status_container.write("âœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                        
                        if review_resp.usage_metadata:
                            cost = calculate_cost(model_id, review_resp.usage_metadata.prompt_token_count, review_resp.usage_metadata.candidates_token_count)
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += (review_resp.usage_metadata.prompt_token_count or 0)
                            usage_stats["total_output_tokens"] += (review_resp.usage_metadata.candidates_token_count or 0)

                # =========================
                # ç†Ÿè€ƒãƒ¢ãƒ¼ãƒ‰
                # =========================
                else:
                    # =========================
                    # ç†Ÿè€ƒãƒ¢ãƒ¼ãƒ‰: å¤šæ®µéšã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
                    # =========================
                    
                    # --- Phase 1: ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    status_container.write("Phase 1: ãƒªã‚µãƒ¼ãƒãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                    
                    import datetime as dt
                    current_year = dt.datetime.now().year
                    
                    research_instruction = base_system_instruction + f"""

**ã‚ãªãŸã®å½¹å‰²**: ãƒªã‚µãƒ¼ãƒå°‚ä»»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®èª¿æŸ»ãƒ¡ãƒ¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æœ€çµ‚å›ç­”ã¯æ›¸ã‹ãšã€äº‹å®Ÿåé›†ã«é›†ä¸­ã™ã‚‹ã“ã¨ã€‚

**èª¿æŸ»è¦³ç‚¹**:
- è³ªå•ã«é–¢é€£ã™ã‚‹**æœ€æ–°ã®äº‹å®Ÿãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±è¨ˆ**ï¼ˆ**{current_year}å¹´ã®æƒ…å ±ã‚’æœ€å„ªå…ˆ**ï¼‰
- **ç¾æ™‚ç‚¹ã§å­˜åœ¨ã™ã‚‹å…¨ã¦ã®é¸æŠè‚¢ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¶²ç¾…çš„ã«èª¿æŸ»**ï¼ˆä¾‹: è£½å“æ¯”è¼ƒãªã‚‰ã€æœ€æ–°ç‰ˆã ã‘ã§ãªãç›´è¿‘ã®å…¨ä¸–ä»£ã‚’èª¿æŸ»ï¼‰
- å…¬å¼æƒ…å ±ã‚„ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‹ã‚‰ã®å¼•ç”¨
- é–¢é€£ã™ã‚‹èƒŒæ™¯æƒ…å ±ã‚„æ–‡è„ˆ
- ç«¶åˆãƒ»ä»£æ›¿æ¡ˆãƒ»æ¯”è¼ƒå¯¾è±¡ã®**å®Œå…¨ãªãƒªã‚¹ãƒˆ**
- ãƒªã‚¹ã‚¯ãƒ»åˆ¶ç´„ãƒ»æ³¨æ„ç‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- å‚è€ƒã«ã—ãŸä¸»è¦ãªæƒ…å ±æºã®URL

**é‡è¦**: 
- çµè«–ã‚„æ¨å¥¨ã¯æ›¸ã‹ãšã€å¾Œå·¥ç¨‹ãŒåˆ¤æ–­ã§ãã‚‹èª¿æŸ»ãƒ¡ãƒ¢ã«é›†ä¸­ã™ã‚‹ã“ã¨
- **å¿…ãšæ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã€æœ€æ–°ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã“ã¨**
- **æ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æ—¥ä»˜ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»ãƒ¢ãƒ‡ãƒ«åã‚’å„ªå…ˆçš„ã«è¨˜è¼‰ã™ã‚‹ã“ã¨**
- **æ¯”è¼ƒå¯¾è±¡ã¨ãªã‚‹é¸æŠè‚¢ã‚’è¦‹è½ã¨ã•ãªã„ã‚ˆã†ã€è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è©¦ã™ã“ã¨**ï¼ˆä¾‹: ã€ŒiPhone æœ€æ–°ãƒ¢ãƒ‡ãƒ« {current_year}ã€ã€ŒiPhone {current_year}å¹´ç™ºå£²ã€ãªã©ï¼‰
- **ã€Œã“ã‚Œã‚ˆã‚Šæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«/ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯å­˜åœ¨ã—ãªã„ã‹ï¼Ÿã€ã‚’å¸¸ã«ç¢ºèªã™ã‚‹ã“ã¨**
- å¤ã„æƒ…å ±ï¼ˆ{current_year-1}å¹´ä»¥å‰ãªã©ï¼‰ã—ã‹è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨
"""

                    # éå»ã®é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    past_context = get_relevant_context(prompt, st.session_state.sessions, st.session_state.current_session_id)
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“è¨˜æ†¶ã‚’å–å¾—
                    session_memory = build_session_memory(
                        st.session_state.sessions,
                        st.session_state.current_session_id,
                        max_entries=10
                    )
                    
                    # ãƒªã‚µãƒ¼ãƒç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
                    import datetime as dt
                    current_date = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
                    research_parts = [types.Part(text=(
                        f"é‡è¦: ä»Šæ—¥ã¯{current_date}ã§ã™ã€‚ã“ã®æ—¥ä»˜ã‚ˆã‚Šæ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚\n\n"
                        f"è³ªå•: {prompt}"
                    ))]
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ã‚’å…ˆé ­ã«è¿½åŠ 
                    if session_memory:
                        research_parts.insert(0, types.Part(text=session_memory))
                    
                    if past_context:
                        research_parts.insert(0, types.Part(text="ä»¥ä¸‹ã¯éå»ã®é–¢é€£ãƒãƒ£ãƒƒãƒˆã‹ã‚‰æŠ½å‡ºã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™ï¼š\n\n" + past_context))
                    
                    research_contents = contents_for_model + [
                        types.Content(role="user", parts=research_parts)
                    ]
                    
                    research_config = types.GenerateContentConfig(
                        temperature=0.4,  # æœ€æ–°æƒ…å ±ã‚’æŸ”è»Ÿã«æ¡ç”¨ã™ã‚‹ãŸã‚0.2â†’0.4ã«ä¸Šæ˜‡
                        candidate_count=1,
                        tools=tools,
                        system_instruction=research_instruction,
                    )
                    
                    research_resp = client.models.generate_content(
                        model=model_id,
                        contents=research_contents,
                        config=research_config,
                    )
                    
                    # TODO: Agentic Loop (Deep Research)
                    # - æ¤œç´¢çµæœã®ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„å ´åˆã€while ãƒ«ãƒ¼ãƒ—ã§è‡ªå¾‹çš„ã«å†æ¤œç´¢
                    # - æœ€å¤§ãƒ«ãƒ¼ãƒ—å›æ•°ã®ã‚¬ãƒ¼ãƒ‰ï¼ˆä¾‹: max_loops=3ï¼‰
                    # - 1ã‚¿ãƒ¼ãƒ³ã‚ãŸã‚Šã®æœ€å¤§ã‚³ã‚¹ãƒˆåˆ¶é™
                    # - å®Ÿè£…å„ªå…ˆåº¦: ä¸­ï¼ˆå®Ÿé‹ç”¨ã§ã€Œã“ã“ã§å†æ¤œç´¢ã—ã¦ã»ã—ã„ã€ã¨ã„ã†ç—›ã¿ãŒè¦‹ãˆã¦ã‹ã‚‰ï¼‰
                    
                    research_text = extract_text_from_response(research_resp)
                    
                    # ãƒªã‚µãƒ¼ãƒãƒ•ã‚§ãƒ¼ã‚ºã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
                    if research_resp.candidates and research_resp.candidates[0].grounding_metadata:
                        grounding_metadata = research_resp.candidates[0].grounding_metadata
                    
                    status_container.write("âœ“ ãƒªã‚µãƒ¼ãƒå®Œäº†")
                    with status_container.expander("åé›†ã—ãŸèª¿æŸ»ãƒ¡ãƒ¢", expanded=False):
                        st.markdown(research_text)
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 1)
                    if research_resp.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            research_resp.usage_metadata.prompt_token_count,
                            research_resp.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += (research_resp.usage_metadata.prompt_token_count or 0)
                        usage_stats["total_output_tokens"] += (research_resp.usage_metadata.candidates_token_count or 0)
                    
                    # --- Phase 1.3: äº‹å®Ÿã¨ãƒªã‚¹ã‚¯ã®æŠ½å‡º (ms/Azãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    # Phase B: JSON IR extraction with v1 fallback
                    fact_summary = ""
                    risk_summary = ""
                    current_ir = None  # Store IR for Phase 2
                    is_ms_az_mode = "ms/Az" in response_mode
                    
                    if is_ms_az_mode:  # ms/Azãƒ¢ãƒ¼ãƒ‰ã§ã®ã¿é‡ã„JSONæŠ½å‡ºã‚’å®Ÿè¡Œ
                        status_container.write("Phase 1.3: JSON IRæŠ½å‡ºä¸­...")
                        
                        # Try v2 extraction first
                        ir, ir_usage, ir_raw_json = extract_facts_and_risks_v2(
                            client=client,
                            model_id=model_id,
                            user_question=prompt,
                            research_text=research_text
                        )
                        
                        if ir is not None:
                            # IR extraction succeeded
                            current_ir = ir
                            fact_summary, risk_summary = convert_ir_to_markdown(ir)
                            phase13_usage = ir_usage
                            status_container.write("âœ“ Phase 1.3å®Œäº† (JSON IR)")
                            
                            # Debug UI
                            with status_container.expander("ğŸ“Š æŠ½å‡ºã•ã‚ŒãŸäº‹å®Ÿã¨ãƒªã‚¹ã‚¯ (Phase B: JSON IR)", expanded=False):
                                st.markdown(f"{fact_summary}\n\n{risk_summary}")
                                
                                st.markdown("---")
                                st.markdown("### ğŸ” ãƒ‡ãƒãƒƒã‚°: JSON IRæ§‹é€ ")
                                st.json(ir)
                                
                                st.markdown("### ğŸ“„ ç”Ÿã®JSONå‡ºåŠ›")
                                st.code(ir_raw_json, language="json")
                        
                        else:
                            # IR extraction failed - fallback to v1
                            status_container.write("âš ï¸ IRæŠ½å‡ºå¤±æ•— - v1ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­...")
                            fact_summary, risk_summary, phase13_usage = extract_facts_and_risks(
                                client, model_id, research_text
                            )
                            status_container.write("âœ“ Phase 1.3å®Œäº† (v1 fallback)")
                            
                            with status_container.expander("æŠ½å‡ºã•ã‚ŒãŸäº‹å®Ÿã¨ãƒªã‚¹ã‚¯ (v1 fallback)", expanded=False):
                                st.markdown(f"{fact_summary}\n\n{risk_summary}")
                                st.warning(f"IRæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {ir_raw_json[:200]}")
                        
                        # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 1.3)
                        phase13_cost = calculate_cost(
                            model_id,
                            phase13_usage["prompt_tokens"],
                            phase13_usage["output_tokens"]
                        )
                        st.session_state.session_cost += phase13_cost
                        usage_stats["total_cost_usd"] += phase13_cost
                        usage_stats["total_input_tokens"] += phase13_usage["prompt_tokens"]
                        usage_stats["total_output_tokens"] += phase13_usage["output_tokens"]

                    
                    # --- Phase 1.5: ãƒ¡ã‚¿è³ªå•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    questions_text = ""
                    if enable_meta:
                        status_container.write("Phase 1.5: ãƒ¡ã‚¿è³ªå•ç”Ÿæˆä¸­...")
                        
                        question_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: ãƒ¡ã‚¿è³ªå•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ç›®çš„**: 
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ã¨èª¿æŸ»ãƒ¡ãƒ¢ã‚’è¸ã¾ãˆã¦ã€ã“ã®ãƒ†ãƒ¼ãƒã‚’ã•ã‚‰ã«æ·±ãç†è§£ã™ã‚‹ãŸã‚ã®ã€Œé‹­ã„ã‚µãƒ–è³ªå•ã€ã‚’ä½œæˆã™ã‚‹ã“ã¨ã€‚

**ãƒ«ãƒ¼ãƒ«**:
- æœ€å¤§5å€‹ã¾ã§
- å„è³ªå•ã¯1-2è¡Œã§å…·ä½“çš„ã‹ã¤é‹­ã
- ä»¥ä¸‹ã®è¦³ç‚¹ã‚’æ„è­˜:
  1. å‰æãŒå´©ã‚Œã‚‹å¯èƒ½æ€§ã¯ã©ã“ã‹ï¼Ÿ
  2. å¼·æ°—/å¼±æ°—ã‚·ãƒŠãƒªã‚ªã®åˆ†å²ç‚¹ã¯ä½•ã‹ï¼Ÿ
  3. ç«¶åˆãƒ»æŠ€è¡“ãƒ»è¦åˆ¶ã®ä¸ç¢ºå®Ÿæ€§ã¯ï¼Ÿ
  4. ã€Œäºˆæ¸¬ãŒå¤–ã‚Œã‚‹ã¨ã—ãŸã‚‰ã©ã‚“ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ï¼Ÿã€

**å‡ºåŠ›**: ç®‡æ¡æ›¸ãï¼ˆQ1, Q2...ï¼‰ã®ã¿
"""

                        question_contents = [types.Content(role="user", parts=[types.Part(text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•:\n{prompt}\n\n==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\nã“ã®ãƒ†ãƒ¼ãƒã‚’ã•ã‚‰ã«æ·±æ˜ã‚Šã™ã‚‹ãŸã‚ã®é‡è¦ãªã‚µãƒ–è³ªå•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")])]
                        
                        question_resp = client.models.generate_content(
                            model=model_id,
                            contents=question_contents,
                            config=types.GenerateContentConfig(
                                temperature=0.4,
                                candidate_count=1,
                                system_instruction=question_instruction,
                            )
                        )
                        
                        questions_text = extract_text_from_response(question_resp)
                        
                        status_container.write("âœ“ ãƒ¡ã‚¿è³ªå•ç”Ÿæˆå®Œäº†")
                        with status_container.expander("ç”Ÿæˆã•ã‚ŒãŸãƒ¡ã‚¿è³ªå•", expanded=False):
                            st.markdown(questions_text)
                        
                        # ã‚³ã‚¹ãƒˆè¨ˆç®—
                        if question_resp.usage_metadata:
                            cost = calculate_cost(
                                model_id,
                                question_resp.usage_metadata.prompt_token_count,
                                question_resp.usage_metadata.candidates_token_count,
                            )
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += (question_resp.usage_metadata.prompt_token_count or 0)
                            usage_stats["total_output_tokens"] += (question_resp.usage_metadata.candidates_token_count or 0)

                    # --- Phase 1.5b: OpenRouter ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ« ç‹¬ç«‹æ€è€ƒ (å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    grok_thought = ""
                    grok_status = "skipped"
                    grok_error_msg = None

                    # â–¼â–¼â–¼ Phase 1.5b/d/e: ä¸¦åˆ—å‡¦ç†ï¼ˆé«˜é€ŸåŒ–ï¼‰ â–¼â–¼â–¼
                    from concurrent.futures import ThreadPoolExecutor, as_completed
                    
                    # ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
                    def run_grok_task():
                        """Grok/OpenRouter ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ«"""
                        if not (enable_meta and OPENROUTER_API_KEY):
                            return {"status": "skipped", "thought": "", "error": None}
                        try:
                            grok_mode = "full_max" if "MAX" in response_mode else "default"
                            grok_input = f"ã€äº‹å®Ÿã€‘\n{fact_summary}\n\nã€ãƒªã‚¹ã‚¯ã€‘\n{risk_summary}" if fact_summary else research_text
                            result = think_with_grok(prompt, grok_input, enable_x_search=enable_grok_x_search, mode=grok_mode).strip()
                            if result:
                                return {"status": "success", "thought": result, "error": None}
                            return {"status": "empty", "thought": "", "error": None}
                        except Exception as e:
                            return {"status": "error", "thought": "", "error": str(e)}
                    
                    def run_claude_task():
                        """Claude 4.5 Sonnet (AWS Bedrock)"""
                        is_az_mode = "Az" in response_mode
                        if not (is_az_mode and AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY):
                            return {"status": "skipped", "thought": "", "usage": {}, "error": None}
                        try:
                            if fact_summary:
                                safe_text = f"ã€äº‹å®Ÿã€‘\n{fact_summary}\n\nã€ãƒªã‚¹ã‚¯ã€‘\n{risk_summary}"
                            else:
                                safe_text = research_text[:40000]
                            thought, usage = think_with_claude45_bedrock(prompt, safe_text)
                            thought = thought.strip() if thought else ""
                            if thought and not thought.startswith("Error"):
                                return {"status": "success", "thought": thought, "usage": usage, "error": None}
                            return {"status": "error", "thought": thought, "usage": {}, "error": None}
                        except Exception as e:
                            return {"status": "error", "thought": "", "usage": {}, "error": str(e)}
                    
                    def run_o4mini_task():
                        """o4-mini (GitHub Models)"""
                        if fact_summary:
                            safe_text = f"{fact_summary[:1500]}\n\n{risk_summary[:1500]}"
                        else:
                            safe_text = research_text[:3000]
                        input_len = len(f"{prompt}\n\n{safe_text}")
                        
                        if not (is_ms_az_mode and GITHUB_TOKEN and input_len <= 3800):
                            return {"status": "skipped", "thought": "", "input_len": input_len, "error": None}
                        try:
                            thought, _ = think_with_o4_mini(prompt, safe_text)
                            thought = thought.strip() if thought else ""
                            if thought and not thought.startswith("Error"):
                                return {"status": "success", "thought": thought, "input_len": input_len, "error": None}
                            return {"status": "error", "thought": thought, "input_len": input_len, "error": None}
                        except Exception as e:
                            return {"status": "error", "thought": "", "input_len": input_len, "error": str(e)}
                    
                    # ä¸¦åˆ—å®Ÿè¡Œ
                    status_container.write("ğŸš€ Phase 1.5: ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—æ€è€ƒä¸­...")
                    
                    grok_thought = ""
                    grok_status = "skipped"
                    claude45_thought = ""
                    claude45_status = "skipped"
                    claude45_usage = {}
                    o4mini_thought = ""
                    o4mini_status = "skipped"
                    
                    with ThreadPoolExecutor(max_workers=3) as executor:
                        futures = {
                            executor.submit(run_grok_task): "grok",
                            executor.submit(run_claude_task): "claude",
                            executor.submit(run_o4mini_task): "o4mini"
                        }
                        
                        for future in as_completed(futures, timeout=60):
                            name = futures[future]
                            try:
                                result = future.result()
                                
                                if name == "grok":
                                    grok_status = result["status"]
                                    grok_thought = result["thought"]
                                    grok_error_msg = result.get("error")
                                    if grok_status == "success":
                                        status_container.write(f"âœ“ {SECONDARY_MODEL_NAME} å®Œäº†")
                                    elif grok_status == "error":
                                        status_container.write(f"âš  {SECONDARY_MODEL_NAME}: {grok_error_msg}")
                                
                                elif name == "claude":
                                    claude45_status = result["status"]
                                    claude45_thought = result["thought"]
                                    claude45_usage = result.get("usage", {})
                                    if claude45_status == "success":
                                        status_container.write("âœ“ Claude 4.5 Sonnet å®Œäº†")
                                        # ã‚³ã‚¹ãƒˆè¨ˆç®—
                                        if claude45_usage:
                                            input_tokens = claude45_usage.get("inputTokens", 0)
                                            output_tokens = claude45_usage.get("outputTokens", 0)
                                            claude_cost = (input_tokens / 1_000_000) * 3.0 + (output_tokens / 1_000_000) * 15.0
                                            st.session_state.session_cost += claude_cost
                                            usage_stats["total_cost_usd"] += claude_cost
                                            status_container.write(f"ğŸ’° Claude: ${claude_cost:.4f}")
                                    elif claude45_status == "error":
                                        status_container.write("âš  Claude 4.5 ã‚¨ãƒ©ãƒ¼")
                                
                                elif name == "o4mini":
                                    o4mini_status = result["status"]
                                    o4mini_thought = result["thought"]
                                    if o4mini_status == "success":
                                        status_container.write("âœ“ o4-mini å®Œäº†")
                                    elif o4mini_status == "skipped" and is_ms_az_mode and GITHUB_TOKEN:
                                        input_len = result.get("input_len", 0)
                                        if input_len > 3800:
                                            status_container.write(f"â„¹ï¸ o4-mini ã‚¹ã‚­ãƒƒãƒ— (å…¥åŠ›é•·: {input_len})")
                            
                            except Exception as e:
                                status_container.write(f"âš  {name} ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    # çµæœã‚’Expanderã«è¡¨ç¤ºï¼ˆæˆåŠŸã—ãŸã‚‚ã®ã®ã¿ï¼‰
                    if grok_status == "success" and grok_thought:
                        with status_container.expander(f"{SECONDARY_MODEL_NAME} ã®ç‹¬ç«‹å›ç­”æ¡ˆ", expanded=False):
                            st.markdown(grok_thought)
                    
                    if claude45_status == "success" and claude45_thought:
                        with status_container.expander("Claude 4.5 Sonnet ã®ç‹¬ç«‹å›ç­”æ¡ˆ", expanded=False):
                            st.markdown(claude45_thought)
                    
                    if o4mini_status == "success" and o4mini_thought:
                        with status_container.expander("o4-mini ã®ç‹¬ç«‹å›ç­”æ¡ˆ", expanded=False):
                            st.markdown(o4mini_thought)
                    # â–²â–²â–² Phase 1.5 ä¸¦åˆ—å‡¦ç† ã“ã“ã¾ã§ â–²â–²â–²

                    # --- Phase 2: çµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    status_container.write("Phase 2: çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                    
                    import datetime as dt
                    current_date = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

                    if enable_meta:
                        deep_instruction = base_system_instruction + f"""

ã€Phase 3: æ·±ã„çµ±åˆã¨ç·æ‹¬æŒ‡ç¤ºã€‘

ä¸Šè¨˜ã®å¤šæ®µãƒ•ã‚§ãƒ¼ã‚ºã§å¾—ã‚‰ã‚ŒãŸæƒ…å ±ï¼ˆãƒªã‚µãƒ¼ãƒãƒ¡ãƒ¢ã€Grokå›ç­”ã€Claudeå›ç­”ã€o4-miniå›ç­”ã€å„ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰ã‚’ç·åˆã—ã€
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦æœ€ã‚‚ä¾¡å€¤ã‚ã‚‹æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**å°†æ¥äºˆæ¸¬ã®æ³¨æ„äº‹é …ï¼ˆç‰¹ã«ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ»æ ªä¾¡ãªã©ï¼‰:**
ãƒ»å°†æ¥ã®æ•°å€¤ï¼ˆæ ªä¾¡æ°´æº–ã‚„é‡‘åˆ©æ°´æº–ï¼‰ã¯ã€å…·ä½“çš„ãªæ°´æº–ã‚’1ã¤ã«å›ºå®šã›ãšã€ã€Œãƒ¬ãƒ³ã‚¸ã€ã¨ã€Œä¸ç¢ºå®Ÿæ€§ã€ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ã€‚
ãƒ»æ”¿æ²»ã‚·ãƒŠãƒªã‚ªã‚‚1ã¤ã«æ±ºã‚æ‰“ã¡ã›ãšã€è¤‡æ•°ã®å¯èƒ½æ€§ã‚’ç¤ºã™ã“ã¨ã€‚
ãƒ»æ–­å®šçš„ãªäºˆæ¸¬ã§ã¯ãªãã€ã‚·ãƒŠãƒªã‚ªã¨ãƒªã‚¹ã‚¯ã«å¯„ã›ãŸè¡¨ç¾ã«ã™ã‚‹ã“ã¨ã€‚

**æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«:**
- è¦‹å‡ºã—ãƒ»ç®‡æ¡æ›¸ãã‚’åŠ¹æœçš„ã«ä½¿ã„ã€èª­ã¿ã‚„ã™ãã™ã‚‹
- Markdownã‚’è¨±å¯ã™ã‚‹ãŒã€é€£ç¶šã™ã‚‹ç©ºè¡Œã¯1è¡Œã¾ã§
- ãƒªã‚µãƒ¼ãƒãƒ¡ãƒ¢ã«å¼•ç”¨å…ƒURLãŒã‚ã‚Œã°ã€é©å®œå‚ç…§ãƒªãƒ³ã‚¯ã¨ã—ã¦æç¤ºã™ã‚‹

**æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®å…¬é–‹:**
- ã€ŒPhase 1: ãƒªã‚µãƒ¼ãƒã€ã€ŒPhase 2: å¤šãƒ¢ãƒ‡ãƒ«å›ç­”ã€ã€ŒPhase 3: çµ±åˆã€ã‚’è¸ã¾ãˆãŸä¸Šã§ã€
  ã©ã®ã‚ˆã†ãªåˆ¤æ–­è»¸ã§æœ€çµ‚å›ç­”ã‚’æ§‹æˆã—ãŸã®ã‹ã‚’è»½ãè¿°ã¹ã¦ã‚‚ã‚ˆã„

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:
{prompt}

**ã‚ãªãŸã®å½¹å‰²**: æœ€çµ‚åˆ¤æ–­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: èª¿æŸ»ãƒ¡ãƒ¢ã¨ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«(Grok, Claude, o4-mini)ã®æŒ‡æ‘˜ã‚’çµ±åˆã—ã€
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦å®Ÿå‹™çš„ã«ä½¿ãˆã‚‹ã€Œåˆ¤æ–­ã€ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚

**å‡ºåŠ›æ§‹æˆ**ï¼ˆã“ã®é †ç•ªã§å¿…é ˆï¼‰:

1. ğŸ“Œ çµè«–
   - 1ã€œ3è¡Œã§ã€æ–¹é‡ / Yes/No / æ¨å¥¨æ¡ˆã‚’ã¯ã£ãã‚Šæ›¸ã

2. ğŸ”‘ ä¸»è¦ãªæ ¹æ‹ 
   - ç®‡æ¡æ›¸ãã§3ã€œ7å€‹
   - ãã‚Œãã‚Œã«ã¤ã„ã¦ã€Œã©ã®æƒ…å ±æº / ã©ã®ãƒ¢ãƒ‡ãƒ«ãŒãã†è¨€ã£ã¦ã„ã‚‹ã‹ã€ã‚’æ›¸ã
   - ã§ãã‚Œã°ã€Œå¼·ã• (å¼·/ä¸­/å¼±)ã€ã‚‚ä»˜ã‘ã‚‹

3. ğŸ“‰ğŸ“ˆ ã‚·ãƒŠãƒªã‚ªåˆ†å²
   - æ¥½è¦³ / ãƒ™ãƒ¼ã‚¹ / æ‚²è¦³ ã®3ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã©ã†å¤‰ã‚ã‚Šã†ã‚‹ã‹ã‚’æ›¸ã
   - ãã‚Œãã‚Œä½•ãŒãƒˆãƒªã‚¬ãƒ¼ã«ãªã‚‹ã‹ã‚‚æ›¸ã

4. âš ï¸ ãƒªã‚¹ã‚¯ãƒ»åå¯¾æ„è¦‹
   - Grok, Claude, o4-mini ãŒæŒ™ã’ãŸæ‡¸å¿µãƒ»åè«–ã‚’çµ±åˆã—ã¦åˆ—æŒ™
   - ã€Œã©ã®ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡æ‘˜ã—ã¦ã„ã‚‹ã‹ã€ã‚‚æ›¸ã

5. â“ æ®‹ã£ã¦ã„ã‚‹ä¸ç¢ºå®Ÿæ€§ã¨ä»Šå¾Œå¿…è¦ãªæ¤œè¨¼
   - ã¾ã ã¯ã£ãã‚Šã—ãªã„ç‚¹
   - è¿½åŠ ã§ç¢ºèªã™ã¹ããƒ‡ãƒ¼ã‚¿ã‚„å®Ÿé¨“
   - ã€Œã“ã“ã¾ã§ãŒ AI ãŒå®‰å…¨ã«è¨€ãˆã‚‹ç¯„å›²ã€ã¨ã„ã†ç·šå¼•ã

6. ğŸ“Š æ¯”è¼ƒãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆè¤‡æ•°é¸æŠè‚¢ãŒã‚ã‚‹å ´åˆï¼‰
   - é¸æŠè‚¢ã®ä¸€è¦§
   - ç°¡æ˜“æ¯”è¼ƒè¡¨ï¼ˆè»¸ã”ã¨ã®ã‚¹ã‚³ã‚¢: 10ç‚¹æº€ç‚¹ or 5æ®µéšè©•ä¾¡ï¼‰
   - å„é¸æŠè‚¢ã®å¼·ã¿ãƒ»å¼±ã¿
   - ã“ã®æ¨å¥¨ã‚’ã²ã£ãã‚Šè¿”ã™æ¡ä»¶

7. ğŸ¯ è‡ªä¿¡åº¦ã¨å¼•ãç¶™ã
   - **è‡ªä¿¡åº¦ã¯æ¬¡ã®å½¢å¼ã§1è¡Œã§æ›¸ã„ã¦ãã ã•ã„**: 
     * è‡ªä¿¡åº¦: High
     * è‡ªä¿¡åº¦: Medium  
     * è‡ªä¿¡åº¦: Low
   - **è‡ªä¿¡ãŒ Medium ã¾ãŸã¯ Low ã®å ´åˆ**:
     * è¿½åŠ ã§èª¿ã¹ã‚‹ã¹ããƒ‡ãƒ¼ã‚¿
     * äººé–“ã«ç¢ºèªã—ã¦ã»ã—ã„ãƒã‚¤ãƒ³ãƒˆ
     * GPT-5.1ï¼ˆAntigravityï¼‰ã«æŠ•ã’ã‚‹ãªã‚‰ä½•ã‚’èãã¹ãã‹

8. ğŸ’° ã‚³ã‚¹ãƒˆãƒ»å·¥æ•°ã®è€ƒæ…®ï¼ˆå®Ÿè£…ææ¡ˆãŒã‚ã‚‹å ´åˆï¼‰
   - ææ¡ˆã®å®Ÿè£…é›£æ˜“åº¦ï¼ˆä½/ä¸­/é«˜ï¼‰
   - äºˆæƒ³ã•ã‚Œã‚‹æ™‚é–“ãƒ»ã‚³ã‚¹ãƒˆ
   - æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆã¾ãšå°ã•ãå§‹ã‚ã‚‹æ–¹æ³•ï¼‰

**Factså„ªå…ˆã®åŸå‰‡**:
- ã€Œèª¿æŸ»ãƒ¡ãƒ¢ã€ã‚ˆã‚Šã‚‚ã€ã€ŒğŸ“Š äº‹å®Ÿã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æ›¸ã‹ã‚ŒãŸå†…å®¹ã‚’å„ªå…ˆã™ã‚‹ã“ã¨
- Factsã«åã™ã‚‹ã“ã¨ã‚’æ›¸ãå ´åˆã¯ã€å¿…ãšã€Œä»®èª¬ã€ã€Œæ¨æ¸¬ã€ã¨æ˜è¨˜ã™ã‚‹ã“ã¨
- ã€Œâš ï¸ ãƒªã‚¹ã‚¯ã€ã«æ›¸ã‹ã‚ŒãŸä¸ç¢ºå®Ÿæ€§ã¯ã€ãƒªã‚¹ã‚¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å¿…ãšåæ˜ ã™ã‚‹ã“ã¨

**ğŸ” ã‚³ãƒ¼ãƒ‰è§£ææ™‚ã®å¿…é ˆãƒã‚§ãƒƒã‚¯ï¼ˆGPT 5.1 ProåŒç­‰å“è³ªï¼‰**:
è³ªå•ãŒã‚³ãƒ¼ãƒ‰ã‚„ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹å ´åˆã€ä»¥ä¸‹ã‚’å¿…ãšå®Ÿè¡Œã™ã‚‹ã“ã¨ï¼š

1. **æ§‹æ–‡ã‚¨ãƒ©ãƒ¼æ¤œå‡º**:
   - `{{ }}` ã®ã‚ˆã†ãªäºŒé‡ãƒ–ãƒ¬ãƒ¼ã‚¹ã€ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ã€æ‹¬å¼§ã®ä¸ä¸€è‡´ã‚’æ¢ã™
   - f-stringã®ä¸­ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å•é¡Œã‚’ãƒã‚§ãƒƒã‚¯

2. **å‚ç…§ vs ã‚³ãƒ”ãƒ¼å•é¡Œ**:
   - ãƒªã‚¹ãƒˆã‚„è¾æ›¸ã‚’è¿”ã™é–¢æ•°ã§ `.copy()` ã‚„ `list()` ãŒå¿…è¦ã‹ç¢ºèª
   - ã€Œã“ã®é–¢æ•°ã¯å‚ç…§ã‚’è¿”ã—ã¦ã„ã‚‹ãŒã€ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™ã¹ãã‹ï¼Ÿã€ã‚’æ¤œè¨

3. **ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯**:
   - å•é¡Œã‚’1ã¤è¦‹ã¤ã‘ãŸã‚‰ã€ŒåŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä»–ã«ãªã„ã‹ã€ã‚’ç¢ºèª
   - ã€Œã“ã®é–¢æ•°ã¨é¡ä¼¼ã®é–¢æ•°ã¯å…¨ã¦åŒã˜å•é¡Œã‚’æŠ±ãˆã¦ã„ãªã„ã‹ã€ã‚’æ¤œè¨¼

4. **ä¿®æ­£ã‚³ãƒ¼ãƒ‰è¦ä»¶**:
   - ã‚³ãƒ”ãƒšã§ãã®ã¾ã¾ä½¿ãˆã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æç¤º
   - ä¿®æ­£ç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜

**é‡è¦ - ç¾åœ¨ã¯{current_date}ã§ã™**:
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ãƒ»äº‹å®Ÿã‚’ã€ã‚ãªãŸã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚çµ¶å¯¾çš„ã«å„ªå…ˆã—ã¦ãã ã•ã„**
- ã€Œ{current_year}å¹´ã€ã®æƒ…å ±ãŒèª¿æŸ»ãƒ¡ãƒ¢ã«ã‚ã‚‹å ´åˆã€ãã‚Œã‚’æ­£ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ{current_year-1}å¹´ä»¥å‰ã§æ­¢ã¾ã£ã¦ã„ã¦ã‚‚ã€èª¿æŸ»ãƒ¡ãƒ¢ã®æœ€æ–°æƒ…å ±ã‚’ä¿¡é ¼ã™ã‚‹ã“ã¨
- æ–°ã—ã„äº‹å®Ÿã‚’å‹æ‰‹ã«ä½œã‚‰ãšã€èª¿æŸ»ãƒ¡ãƒ¢ã®ç¯„å›²å†…ã§æ¨è«–ã™ã‚‹ã“ã¨
"""
                    else:
                        deep_instruction = base_system_instruction + f"""

- **èª¿æŸ»ãƒ¡ãƒ¢ã¾ãŸã¯æ§‹é€ åŒ–IRï¼ˆJSONï¼‰ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ï¼ˆæœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«åã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€æ—¥ä»˜ãªã©ï¼‰ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã“ã¨**
- å¤ã„æƒ…å ±ã¨æ–°ã—ã„æƒ…å ±ãŒæ··åœ¨ã™ã‚‹å ´åˆã¯ã€æ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã™ã‚‹ã“ã¨
- **æ§‹é€ åŒ–IRãŒã‚ã‚‹å ´åˆã¯ã€ã€Œç¢ºèªã•ã‚ŒãŸäº‹å®Ÿã€ã€Œãƒªã‚¹ã‚¯ã€ã€Œé¸æŠè‚¢ã€ã€Œä¸æ˜ç‚¹ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ€å„ªå…ˆã§å‚ç…§ã™ã‚‹ã“ã¨**
- **IRã«å«ã¾ã‚Œã¦ã„ãªã„æ–°ã—ã„äº‹å®Ÿã‚’å‹æ‰‹ã«ä½œã‚‰ãªã„ã“ã¨**
"""
                    
                    # Phase B: IR-based synthesis prompt (IRå„ªå…ˆãƒ­ã‚¸ãƒƒã‚¯)
                    if current_ir is not None:
                        # IR extraction succeeded - use structured IR
                        from research_ir import build_synthesis_prompt_from_ir
                        
                        ir_block = build_synthesis_prompt_from_ir(current_ir, prompt)
                        
                        synthesis_prompt_text = (
                            f"é‡è¦: ä»Šæ—¥ã¯{current_date}ã§ã™ã€‚å¤ã„æƒ…å ±ã‚’å›ç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n\n"
                            f"==== æ§‹é€ åŒ–èª¿æŸ»IR (Phase B) ====\n"
                            f"{ir_block}\n"
                            f"==== IRã“ã“ã¾ã§ ====\n\n"
                        )
                    else:
                        # IR extraction failed or not available - fallback to v1
                        synthesis_prompt_text = (
                            f"é‡è¦: ä»Šæ—¥ã¯{current_date}ã§ã™ã€‚å¤ã„æƒ…å ±ã‚’å›ç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n\n"
                            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {prompt}\n\n"
                            f"==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\n"
                        )
                    
                    if enable_meta and questions_text:
                        synthesis_prompt_text += f"==== ãƒ¡ã‚¿è³ªå•ä¸€è¦§ ====\n{questions_text}\n==== ãƒ¡ã‚¿è³ªå•ã“ã“ã¾ã§ ====\n\n"
                    
                    if enable_meta and grok_thought:
                        synthesis_prompt_text += f"==== åˆ¥è¦–ç‚¹ã‹ã‚‰ã®ãƒªã‚¹ã‚¯æŒ‡æ‘˜ ({SECONDARY_MODEL_NAME}) ====\n{grok_thought}\n==== {SECONDARY_MODEL_NAME} ã“ã“ã¾ã§ ====\n\n"
                    
                    
                    # â–¼â–¼â–¼ Claude 4.5 ã®å›ç­”ã‚’çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŠ ãˆã‚‹ â–¼â–¼â–¼
                    if claude45_thought and claude45_status == "success":
                        synthesis_prompt_text += f"==== åˆ¥è¦–ç‚¹ã‹ã‚‰ã®å›ç­”æ¡ˆ (Claude 4.5 Sonnet / AWS Bedrock) ====\n{claude45_thought}\n==== Claude 4.5 Sonnet ã“ã“ã¾ã§ ====\n\n"
                    # â–²â–²â–² Claude 4.5 è¿½åŠ ã“ã“ã¾ã§ â–²â–²â–²
                    
                    # â–¼â–¼â–¼ o4-mini ã®å›ç­”ã‚’çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŠ ãˆã‚‹ â–¼â–¼â–¼
                    if o4mini_thought and o4mini_status == "success":
                        synthesis_prompt_text += f"==== è¦‹è½ã¨ã—/ãƒªã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ (o4-mini / GitHub Models) ====\n{o4mini_thought}\n==== o4-mini ã“ã“ã¾ã§ ====\n\n"
                    # â–²â–²â–² o4-mini è¿½åŠ ã“ã“ã¾ã§ â–²â–²â–²
                    
                    # çµ±åˆæŒ‡ç¤ºã®ä¿®æ­£
                    if enable_meta and (grok_thought or claude45_thought or o4mini_thought):
                        synthesis_prompt_text += f"æŒ‡ç¤º:\n1. ã¾ãšã€ãƒ¡ã‚¿è³ªå• Q1ã€œQn ã«ä¸€ã¤ãšã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n2. ä»–ã®ãƒ¢ãƒ‡ãƒ« ({SECONDARY_MODEL_NAME}, Claude 4.5 Sonnet, o4-mini) ã®å›ç­”æ¡ˆã‚‚å‚è€ƒã«ã—ã¤ã¤ï¼ˆãŸã ã—ç›²ä¿¡ã›ãšï¼‰ã€ç‹¬è‡ªã®è¦–ç‚¹ã§çµ±åˆã—ã¦ãã ã•ã„ã€‚\n3. ãã®ã†ãˆã§ã€ãã‚Œã‚‰ã®å›ç­”ã‚’è¸ã¾ãˆãŸã€å…¨ä½“ã¨ã—ã¦ã®çµè«–ãƒ»åˆ†æãƒ»ç¤ºå”†ã€ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                    elif enable_meta and questions_text:
                        synthesis_prompt_text += "æŒ‡ç¤º:\n1. ã¾ãšã€ãƒ¡ã‚¿è³ªå• Q1ã€œQn ã«ä¸€ã¤ãšã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n2. ãã®ã†ãˆã§ã€ãã‚Œã‚‰ã®å›ç­”ã‚’è¸ã¾ãˆãŸã€å…¨ä½“ã¨ã—ã¦ã®çµè«–ãƒ»åˆ†æãƒ»ç¤ºå”†ã€ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                    else:
                        synthesis_prompt_text += "ä¸Šè¨˜ãƒ¡ãƒ¢ã‚’æ ¹æ‹ ã«ã€æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚**èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚**"

                    synthesis_contents = contents_for_model + [
                        types.Content(role="user", parts=[
                            types.Part(text=synthesis_prompt_text)
                        ])
                    ]
                    
                    synthesis_config = types.GenerateContentConfig(
                        temperature=0.4,  # Phase A: çµ±åˆæ™‚ã®æŸ”è»Ÿæ€§å‘ä¸Š
                        candidate_count=1,
                        tools=[],  # çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºã§ã¯æ¤œç´¢OFF
                        system_instruction=deep_instruction,
                        thinking_config=types.ThinkingConfig(
                            thinking_level=types.ThinkingLevel.HIGH
                        ),
                    )
                    
                    # Phase 2 ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
                    import time
                    max_retries = 3
                    draft_answer = None
                    synthesis_resp = None
                    
                    for attempt in range(max_retries):
                        try:
                            synthesis_resp = client.models.generate_content(
                                model=model_id,
                                contents=synthesis_contents,
                                config=synthesis_config,
                            )
                            draft_answer = extract_text_from_response(synthesis_resp)
                            
                            # â–¼â–¼â–¼ finish_reasonæ¤œå‡ºï¼šé€”ä¸­ã§åˆ‡ã‚ŒãŸã‚‰è‡ªå‹•ç¶™ç¶š â–¼â–¼â–¼
                            candidate = synthesis_resp.candidates[0] if synthesis_resp.candidates else None
                            if candidate and hasattr(candidate, 'finish_reason'):
                                finish_reason = str(candidate.finish_reason).upper()
                                if "MAX_TOKENS" in finish_reason or "LENGTH" in finish_reason:
                                    status_container.write("âš ï¸ å›ç­”ãŒé€”ä¸­ã§åˆ‡ã‚Œã¾ã—ãŸã€‚ç¶šãã‚’å–å¾—ä¸­...")
                                    try:
                                        continuation_resp = client.models.generate_content(
                                            model=model_id,
                                            contents=[
                                                types.Content(role="user", parts=[
                                                    types.Part.from_text(text="å…ˆã»ã©ã®å›ç­”ãŒé€”ä¸­ã§é€”åˆ‡ã‚Œã¾ã—ãŸã€‚ç¶šãã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚è¦ç´„ã›ãšã€é€”åˆ‡ã‚ŒãŸç®‡æ‰€ã‹ã‚‰ç¶šã‘ã¦ãã ã•ã„ã€‚")
                                                ])
                                            ],
                                            config=synthesis_config,
                                        )
                                        continuation_text = extract_text_from_response(continuation_resp)
                                        draft_answer += "\n\n" + continuation_text
                                        status_container.write("âœ“ çµ±åˆå®Œäº†ï¼ˆè‡ªå‹•ç¶™ç¶šï¼‰")
                                    except Exception as cont_e:
                                        draft_answer += "\n\n*ï¼ˆç¶šãã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼‰*"
                                else:
                                    status_container.write("âœ“ çµ±åˆå®Œäº†")
                            else:
                                status_container.write("âœ“ çµ±åˆå®Œäº†")
                            # â–²â–²â–² finish_reasonæ¤œå‡º ã“ã“ã¾ã§ â–²â–²â–²
                            
                            break
                        except Exception as e:
                            error_msg = str(e).lower()
                            if "quota" in error_msg or "rate" in error_msg or "resource" in error_msg:
                                if attempt < max_retries - 1:
                                    wait_time = (attempt + 1) * 15 + 15  # 30ç§’, 45ç§’, 60ç§’ï¼ˆå¼·åŒ–ç‰ˆï¼‰
                                    status_container.write(f"â³ ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã®ãŸã‚ {wait_time}ç§’å¾…æ©Ÿä¸­... (è©¦è¡Œ {attempt + 2}/{max_retries})")
                                    time.sleep(wait_time)
                                else:
                                    status_container.warning("âš ï¸ Phase 2: ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«ã‚ˆã‚Šæ–­å¿µã€‚ãƒªã‚µãƒ¼ãƒçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
                                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒªã‚µãƒ¼ãƒçµæœã®è¦ç´„ã‚’å›ç­”ã¨ã—ã¦ä½¿ç”¨
                                    draft_answer = f"**âš ï¸ çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºãŒã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«ã‚ˆã‚Šä¸­æ–­ã•ã‚Œã¾ã—ãŸ**\n\n### åé›†ã—ãŸæƒ…å ±ï¼ˆPhase 1ï¼‰:\n\n{research_text[:3000]}..."
                            else:
                                raise e
                    
                    if draft_answer is None:
                        draft_answer = f"**âš ï¸ Phase 2ã‚¨ãƒ©ãƒ¼**\n\n{research_text[:2000]}..."
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 2)
                    if synthesis_resp.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            synthesis_resp.usage_metadata.prompt_token_count,
                            synthesis_resp.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += (synthesis_resp.usage_metadata.prompt_token_count or 0)
                        usage_stats["total_output_tokens"] += (synthesis_resp.usage_metadata.candidates_token_count or 0)
                    
                    # --- Phase 3: ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    grok_review_status = "skipped"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆPhase 3å®Ÿè¡Œã—ãªã„å ´åˆã‚‚å®‰å…¨ï¼‰
                    if enable_strict:
                        status_container.write("Phase 3: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                        

                        reviewer_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: é¬¼è»æ›¹ãƒ¬ãƒ™ãƒ«ã®å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ + Devil's Advocateï¼ˆæ‚ªé­”ã®ä»£å¼è€…ï¼‰

**ã‚¿ã‚¹ã‚¯**: åˆç‰ˆå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’è¿”ã™ã€‚ãŸã ã—ã€**èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ã‚’å„ªå…ˆã—ã€æœ€æ–°æƒ…å ±ã‚’ç¶­æŒã™ã‚‹ã“ã¨**ã€‚

**ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦³ç‚¹**:
- äº‹å®Ÿã¨æ¨æ¸¬ã‚’æ˜ç¢ºã«åˆ†ã‘ã‚‹
- éåº¦ã«è‡ªä¿¡ã®ã‚ã‚‹æ–­å®šã‚’å¼±ã‚ã‚‹
- æ•°å­—ã‚„å›ºæœ‰åè©ãŒèª¿æŸ»ãƒ¡ãƒ¢ã¨çŸ›ç›¾ã—ã¦ã„ãªã„ã‹ç¢ºèª
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ï¼ˆæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€æ—¥ä»˜ãªã©ï¼‰ãŒæ­£ã—ãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª**
- **å¤ã„æƒ…å ±ã§ä¸Šæ›¸ãã—ã¦ã„ãªã„ã‹ç¢ºèª**
- è¦‹è½ã¨ã—ã¦ã„ã‚‹é‡è¦ãªãƒªã‚¹ã‚¯ãƒ»ã‚·ãƒŠãƒªã‚ªãŒã‚ã‚Œã°è¿½åŠ 

**ğŸ”¥ Devil's Advocateï¼ˆæ‚ªé­”ã®ä»£å¼è€…ï¼‰- å¿…é ˆ**:
ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚ã«ä»¥ä¸‹ã‚’å¿…ãšå®Ÿæ–½ã—ã¦ãã ã•ã„ï¼š
1. **ã“ã®çµè«–ã‚’è¦†ã™æœ€å¼·ã®åè«–ã‚’3ã¤**è€ƒãˆã‚‹
2. ãã‚Œã§ã‚‚çµè«–ãŒæ­£ã—ã„ã¨è¨€ãˆã‚‹ã‹æ¤œè¨¼ã™ã‚‹
3. åè«–ã«å¯¾ã™ã‚‹å†åè«–ãŒå¼±ã„å ´åˆã¯ã€çµè«–ã‚’ä¿®æ­£ã™ã‚‹
4. æœ€çµ‚å›ç­”ã«ã€ŒğŸ”´ æœ€å¼·ã®åè«–ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã€è€ƒæ…®ã—ãŸåè«–ã¨ã€ãã‚Œã§ã‚‚çµè«–ã‚’ç¶­æŒã™ã‚‹ç†ç”±ã‚’æ˜è¨˜

**ğŸ“Š 5æ®µéšç¢ºä¿¡åº¦ - å¿…é ˆ**:
å›ç­”ã®æœ€å¾Œã«ä»¥ä¸‹ã®å½¢å¼ã§ç¢ºä¿¡åº¦ã‚’æ˜è¨˜ï¼š
- **ç¢ºä¿¡åº¦: Very High (90%+)** - ã»ã¼ç¢ºå®Ÿã€è¦†ã‚‹å¯èƒ½æ€§ã¯ä½ã„
- **ç¢ºä¿¡åº¦: High (70-90%)** - é«˜ã„ä¿¡é ¼æ€§ã€ä¸»è¦ãªãƒªã‚¹ã‚¯ã¯è€ƒæ…®æ¸ˆã¿
- **ç¢ºä¿¡åº¦: Medium (50-70%)** - å¦¥å½“ãªæ¨è«–ã ãŒä¸ç¢ºå®Ÿæ€§ã‚ã‚Š
- **ç¢ºä¿¡åº¦: Low (30-50%)** - ä»®èª¬æ®µéšã€è¿½åŠ æ¤œè¨¼ãŒå¿…è¦
- **ç¢ºä¿¡åº¦: Very Low (<30%)** - æ¨æ¸¬ã®åŸŸã‚’å‡ºãªã„ã€æ…é‡ã«æ‰±ã†ã¹ã

ç¢ºä¿¡åº¦ãŒMediumä»¥ä¸‹ã®å ´åˆã¯ã€ã€Œâš ï¸ ç¢ºä¿¡åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã«å¿…è¦ãªã“ã¨ã€ã‚’è¿½è¨˜ã™ã‚‹ã“ã¨ã€‚

**ğŸ” è‡ªå·±çŸ›ç›¾ãƒã‚§ãƒƒã‚¯ - å¿…é ˆ**:
- Phase 1ï¼ˆèª¿æŸ»ãƒ¡ãƒ¢ï¼‰ã®æƒ…å ±ã¨ã€Phase 2ï¼ˆçµ±åˆå›ç­”ï¼‰ã®å†…å®¹ã«çŸ›ç›¾ãŒãªã„ã‹ç¢ºèª
- çŸ›ç›¾ãŒã‚ã‚‹å ´åˆã¯ã€Œâš¡ çŸ›ç›¾æ¤œå‡ºã€ã¨ã—ã¦æ˜è¨˜ã—ã€ã©ã¡ã‚‰ã‚’æ¡ç”¨ã—ãŸã‹ç†ç”±ã‚’èª¬æ˜

**ğŸ“ ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹å¼•ç”¨ - é‡è¦**:
- ä¸»è¦ãªä¸»å¼µã«ã¯å¿…ãšæ ¹æ‹ ã‚’ç¤ºã™ï¼ˆã€Œèª¿æŸ»ãƒ¡ãƒ¢ã«ã‚ˆã‚‹ã¨...ã€ã€ŒXXXã®æƒ…å ±æºã§ã¯...ã€ï¼‰
- æ ¹æ‹ ãªãæ–­å®šã¯é¿ã‘ã€æ¨æ¸¬ã®å ´åˆã¯ã€ŒãŠãã‚‰ãã€ã€Œå¯èƒ½æ€§ãŒã‚ã‚‹ã€ã¨æ˜è¨˜
- æƒ…å ±æºãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„ã‚‚ã®ã‚’å„ªå…ˆ

**ğŸš« ä»£æ›¿æ¡ˆã®æ£„å´ç†ç”± - é‡è¦**:
- çµè«–ã‚’å°ãéš›ã«ã€æ¤œè¨ã—ãŸä»–ã®é¸æŠè‚¢ã‚’æ˜è¨˜
- ã€Œãªãœãã®é¸æŠè‚¢ã‚’æ¡ç”¨ã—ãªã‹ã£ãŸã‹ã€ã‚’ç°¡æ½”ã«èª¬æ˜
- ä¾‹: ã€Œé¸æŠè‚¢Aã¯ã€‡ã€‡ã®ç†ç”±ã§ä¸é©ã€é¸æŠè‚¢Bã¯â–³â–³ã®ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ãŸã‚ã€çµè«–Cã‚’æ¡ç”¨ã€

**é‡è¦**: 
- èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ãŒæœ€æ–°ã§ã‚ã‚‹å ´åˆã€ãã‚Œã‚’å„ªå…ˆã™ã‚‹ã“ã¨
- ã‚ãªãŸã®çŸ¥è­˜ãŒå¤ã„å ´åˆã¯ã€èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ã‚’ä¿¡é ¼ã™ã‚‹ã“ã¨

**å‡ºåŠ›**: ä¿®æ­£ç‰ˆã®å›ç­”å…¨æ–‡ï¼ˆDevil's Advocateã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ç¢ºä¿¡åº¦ã€ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹å¼•ç”¨ã‚’å«ã‚€ï¼‰
"""

                        review_contents = [
                            types.Content(role="user", parts=[
                                types.Part.from_text(
                                    text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {prompt}\n\n"
                                    f"==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\n"
                                    f"åˆç‰ˆå›ç­”:\n{draft_answer}\n\n"
                                    "ä¸Šè¨˜ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚**èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°æƒ…å ±ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚**"
                                )
                            ])
                        ]
                        
                        review_config = types.GenerateContentConfig(
                            temperature=0.1,
                            candidate_count=1,
                            system_instruction=reviewer_instruction,
                            thinking_config=types.ThinkingConfig(
                                thinking_level=types.ThinkingLevel.HIGH
                            ),
                        )
                        
                        # Phase 3 ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
                        import time
                        max_retries = 3
                        for attempt in range(max_retries):
                            try:
                                review_resp = client.models.generate_content(
                                    model=model_id,
                                    contents=review_contents,
                                    config=review_config,
                                )
                                final_answer = extract_text_from_response(review_resp)
                                status_container.write("âœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                                break
                            except Exception as e:
                                error_msg = str(e).lower()
                                if "quota" in error_msg or "rate" in error_msg or "resource" in error_msg:
                                    if attempt < max_retries - 1:
                                        wait_time = (attempt + 1) * 15 + 5  # 20ç§’, 35ç§’, 50ç§’ï¼ˆå¼·åŒ–ç‰ˆï¼‰
                                        status_container.write(f"â³ ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã®ãŸã‚ {wait_time}ç§’å¾…æ©Ÿä¸­... (è©¦è¡Œ {attempt + 2}/{max_retries})")
                                        time.sleep(wait_time)
                                    else:
                                        status_container.warning("âš ï¸ Phase 3: ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«ã‚ˆã‚Šæ–­å¿µã€‚Phase 2ã®çµæœã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                                        final_answer = draft_answer  # Phase 2ã®çµæœã‚’ä½¿ç”¨
                                else:
                                    raise e
                        
                        # --- Phase 3b: Groké¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ + é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰å…¨èˆ¬) ---
                        # å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã§ã€ã‹ã¤é¬¼è»æ›¹ç³»ã®ãƒ¢ãƒ¼ãƒ‰ï¼ˆé¬¼è»æ›¹ã€ãƒ¡ã‚¿æ€è€ƒã€æœ¬æ°—MAXï¼‰ã§ç™ºå‹•
                        use_grok_reviewer = (mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)" and (enable_strict or "é¬¼è»æ›¹" in response_mode))
                        if use_grok_reviewer and OPENROUTER_API_KEY:
                            status_container.write(f"{SECONDARY_MODEL_NAME}ã«ã‚ˆã‚‹æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Ÿè¡Œä¸­...")
                            
                            review_mode = "normal"
                            if "é¬¼è»æ›¹" in response_mode:
                                review_mode = "onigunsou"
                            elif "MAX" in response_mode:
                                review_mode = "full_max"

                            grok_answer = review_with_grok(prompt, final_answer, research_text, mode=review_mode).strip()
                            
                            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼šGrokãŒã‚¨ãƒ©ãƒ¼æ–‡å­—åˆ—ã‚’è¿”ã—ãŸå ´åˆ
                            if grok_answer.startswith("Error calling"):
                                grok_review_status = "error"
                                status_container.error(f"âš ï¸ {SECONDARY_MODEL_NAME} æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ ã‚¨ãƒ©ãƒ¼\n\n{grok_answer}")
                                # final_answerã¯Geminié¬¼è»æ›¹ç‰ˆã®ã¾ã¾ä½¿ç”¨
                            else:
                                grok_review_status = "success"
                                # å‡¦ç†å±¥æ­´ã‚’å…ˆã«æ§‹ç¯‰
                                
                processing_history = []
                processing_history.append("**Phase 1**: Gemini ãƒªã‚µãƒ¼ãƒ (Googleæ¤œç´¢)")
                if enable_meta:
                    processing_history.append("**Phase 1.5a**: Gemini ãƒ¡ã‚¿è³ªå•ç”Ÿæˆ")
                    if grok_status == "success":
                        processing_history.append(f"**Phase 1.5b**: {SECONDARY_MODEL_NAME} ç‹¬ç«‹æ€è€ƒ âœ“")
                    if claude45_status == "success":
                        processing_history.append("**Phase 1.5d**: Claude 4.5 Sonnet ç‹¬ç«‹æ€è€ƒ (AWS Bedrock) âœ“")
                    if o4mini_status == "success":
                        processing_history.append("**Phase 1.5e**: o4-mini ç‹¬ç«‹æ€è€ƒ (GitHub Models) âœ“")
                processing_history.append("**Phase 2**: Gemini çµ±åˆãƒ•ã‚§ãƒ¼ã‚º")
                if enable_strict:
                    processing_history.append("**Phase 3**: Gemini é¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    processing_history.append(f"**Phase 3b**: {SECONDARY_MODEL_NAME} æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ âœ“")
                else: # This 'else' belongs to 'if use_grok_reviewer and OPENROUTER_API_KEY:'
                    # Geminiã®ã¿ã®å ´åˆã‚‚ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤ºï¼ˆå¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
                    if mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)":
                        final_answer = (
                            f"**ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_id} (Deep Thinking / High Reasoning)**\n"
                            f"**ãƒ¢ãƒ¼ãƒ‰: {response_mode}**\n\n---\n\n{final_answer}"
                        )
                        
                        # --- ãƒ¡ã‚¿æ€è€ƒãƒ¢ãƒ¼ãƒ‰: çµè«–ã‚’å…ˆå‡ºã—ã™ã‚‹ ---
                        if "ãƒ¡ã‚¿æ€è€ƒ" in response_mode:
                            # çµè«–éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
                            # "çµè«–"ã‚„"ã¾ã¨ã‚"ãªã©ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã—ã¦å…ˆé ­ã«ç§»å‹•
                            lines = final_answer.split('\n')
                            conclusion_start = -1
                            for i, line in enumerate(lines):
                                if any(keyword in line for keyword in ['## çµè«–', '## ã¾ã¨ã‚', '**çµè«–**', '**ã¾ã¨ã‚**']):
                                    conclusion_start = i
                                    break
                            
                            if conclusion_start != -1:
                                # çµè«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ãŸå ´åˆã€ãã‚Œã‚’å…ˆé ­ã«ç§»å‹•
                                conclusion_section = []
                                other_content = lines[:conclusion_start]
                                
                                # çµè«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚ã‚ã‚Šã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆæ¬¡ã®##ã¾ã§ or æ–‡æœ«ï¼‰
                                conclusion_end = len(lines)
                                for i in range(conclusion_start + 1, len(lines)):
                                    if lines[i].startswith('## ') and i != conclusion_start:
                                        conclusion_end = i
                                        break
                                
                                conclusion_section = lines[conclusion_start:conclusion_end]
                                remaining_content = lines[conclusion_end:]
                                
                                # å†æ§‹æˆ: ãƒ¢ãƒ‡ãƒ«å â†’ çµè«– â†’ ãã®ä»–ã®è©³ç´°
                                # ãƒ¢ãƒ‡ãƒ«åéƒ¨åˆ†ã‚’ä¿æŒ
                                model_line = ""
                                if lines[0].startswith("**ğŸ¤–"):
                                    model_line = lines[0]
                                    other_content = lines[1:conclusion_start]
                                
                                final_answer = '\n'.join([
                                    model_line,
                                    "",
                                    "---",
                                    "",
                                    "## ğŸ“Œ çµè«–ï¼ˆå…ˆå‡ºã—ï¼‰",
                                    *conclusion_section[1:],  # å…ƒã®è¦‹å‡ºã—ã‚’é™¤ã
                                    "",
                                    "---",
                                    "",
                                    "## ğŸ“ è©³ç´°",
                                    *other_content,
                                    *remaining_content
                                ]).strip()
                        
                        with status_container.expander("åˆç‰ˆã¨ã®æ¯”è¼ƒ", expanded=False):
                            st.markdown("**åˆç‰ˆ:**")
                            st.markdown(draft_answer[:500] + "..." if len(draft_answer) > 500 else draft_answer)
                            st.markdown("**ä¿®æ­£ç‰ˆ:**")
                            st.markdown(final_answer[:500] + "..." if len(final_answer) > 500 else final_answer)
                        
                        # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 3)
                        if review_resp.usage_metadata:
                            cost = calculate_cost(
                                model_id,
                                review_resp.usage_metadata.prompt_token_count,
                                review_resp.usage_metadata.candidates_token_count,
                            )
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += (review_resp.usage_metadata.prompt_token_count or 0)
                            usage_stats["total_output_tokens"] += (review_resp.usage_metadata.candidates_token_count or 0)
                    else:
                        final_answer = draft_answer

                save_usage(usage_stats)

                # --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•æ›´æ–° & è‡ªå‹•ææ¡ˆ ---
                try:
                    status_container.write("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ä¸­...")
                    # client already initialized at startup
                    
                    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
                    updated_profile, profile_usage = update_user_profile_from_conversation(
                        client, prompt, final_answer
                    )
                    save_user_profile(updated_profile)
                    
                    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã‚³ã‚¹ãƒˆ
                    p_cost = calculate_cost("gemini-2.5-flash", profile_usage["input_tokens"], profile_usage["output_tokens"])
                    st.session_state.session_cost += p_cost
                    usage_stats["total_cost_usd"] += p_cost
                    usage_stats["total_input_tokens"] += profile_usage["input_tokens"]
                    usage_stats["total_output_tokens"] += profile_usage["output_tokens"]
                    
                    # --- å›ç­”æœ«å°¾ã¸ã®è‡ªå‹•ææ¡ˆ (Phase 3-A) ---
                    status_container.write("æ¬¡ã®è³ªå•ã‚’ææ¡ˆä¸­...")
                    suggestion_prompt = f"""
ä»¥ä¸‹ã®ä¼šè©±ã®ç¶šãã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«æ·±æ˜ã‚Šã™ã¹ãã€Œä¾¡å€¤ã‚ã‚‹è³ªå•ã€ã‚’3ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆèˆˆå‘³é–¢å¿ƒï¼‰: {updated_profile.get('interests', [])}

ã€ç›´å‰ã®ä¼šè©±ã€‘
User: {prompt[:800]}
AI: {final_answer[:1000]}

ã€è³ªå•ç”Ÿæˆã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€‘
- è¡¨é¢çš„ãªè³ªå•ã§ã¯ãªãã€å›ç­”ã®æ ¸å¿ƒã‚’æ·±æ˜ã‚Šã™ã‚‹è³ªå•ã‚’ç”Ÿæˆ
- å®Ÿå‹™ã§å½¹ç«‹ã¤å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ã¤ãªãŒã‚‹è³ªå•
- è¦‹è½ã¨ã•ã‚ŒãŒã¡ãªãƒªã‚¹ã‚¯ã‚„ä»£æ›¿æ¡ˆã‚’å•ã†è³ªå•
- å„è³ªå•ã¯60ã€œ100æ–‡å­—ç¨‹åº¦ã§ã€å…·ä½“çš„ã‹ã¤è©³ç´°ã«

ã€å‡ºåŠ›å½¢å¼ï¼ˆå³å®ˆï¼‰ã€‘
- [è³ªå•1: å…·ä½“çš„ã§æ·±ã„è³ªå•æ–‡ï¼Ÿ]
- [è³ªå•2: å®Ÿå‹™ã«ã¤ãªãŒã‚‹è³ªå•æ–‡ï¼Ÿ]
- [è³ªå•3: ãƒªã‚¹ã‚¯ã‚„ä»£æ›¿æ¡ˆã‚’å•ã†è³ªå•æ–‡ï¼Ÿ]
"""
                    suggestion_resp = client.models.generate_content(
                        model="gemini-2.5-flash",
                        contents=[{"role": "user", "parts": [{"text": suggestion_prompt}]}],
                        config=types.GenerateContentConfig(temperature=0.7, max_output_tokens=512)
                    )
                    
                    # å‡ºåŠ›ã‚’æ•´å½¢ã—ã¦ã‹ã‚‰è¿½åŠ 
                    import re
                    raw = extract_text_from_response(suggestion_resp).strip()
                    lines = [l.strip() for l in raw.splitlines() if l.strip()]

                    questions = []
                    for l in lines:
                        if not l.startswith("-"):
                            continue
                        q = l.lstrip("- ").strip()
                        if not q:
                            continue
                        # ã€Œç†ç”±:ã€ç­‰ãŒä»˜ã„ã¦ã„ãŸã‚‰æ‰‹å‰ã ã‘ã‚’æ¡ç”¨
                        if "ç†ç”±" in q:
                            q = q.split("ç†ç”±", 1)[0].strip()
                        # 40æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                        if len(q) > 40:
                            q = q[:40].rstrip() + "..."
                        # å¿…ãš?ã§çµ‚ã‚ã‚‹ã‚ˆã†ã«ã™ã‚‹
                        if not q.endswith(("?", "ï¼Ÿ")):
                            q += "ï¼Ÿ"
                        questions.append(f"- {q}")
                        if len(questions) >= 3:
                            break

                    if questions:
                        suggestions_text = "\n".join(questions)
                        final_answer += "\n\n---\n\n### ğŸ” æ¬¡ã«è©¦ã›ã‚‹è³ªå•å€™è£œ\n" + suggestions_text
                    
                        # ææ¡ˆç”Ÿæˆã‚³ã‚¹ãƒˆ
                        s_usage = suggestion_resp.usage_metadata
                        s_cost = calculate_cost("gemini-2.5-flash", s_usage.prompt_token_count, s_usage.candidates_token_count)
                        st.session_state.session_cost += s_cost
                        usage_stats["total_cost_usd"] += s_cost
                        usage_stats["total_input_tokens"] += s_usage.prompt_token_count
                        usage_stats["total_output_tokens"] += s_usage.candidates_token_count
                    
                    save_usage(usage_stats)
                    
                except Exception as e:
                    print(f"Profile/Suggestion update failed: {e}")

                status_container.update(label="å®Œäº†ï¼", state="complete", expanded=False)

                # ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤º
                models_used = [f"Gemini: {model_id}"]
                
                # Grok Status
                if enable_meta:
                    if grok_status == "success":
                        models_used.append(f"OpenRouter: {SECONDARY_MODEL_NAME} (OK)")
                    elif grok_status == "error":
                        models_used.append(f"OpenRouter: {SECONDARY_MODEL_NAME} (Error)")
                    elif grok_status == "empty":
                        models_used.append(f"OpenRouter: {SECONDARY_MODEL_NAME} (Empty)")
                
                # â–¼â–¼â–¼ Claude 4.5 Sonnet Status â–¼â–¼â–¼
                if claude45_status == "success":
                    models_used.append(f"Claude 4.5 Sonnet (AWS Bedrock) (OK)")
                elif claude45_status == "error":
                    models_used.append(f"Claude 4.5 Sonnet (AWS Bedrock) (Error)")
                # â–²â–²â–² Claude 4.5 Sonnet Status ã“ã“ã¾ã§ â–²â–²â–²
                
                # â–¼â–¼â–¼ o4-mini Status â–¼â–¼â–¼
                if o4mini_status == "success":
                    models_used.append(f"o4-mini (GitHub Models) (OK)")
                elif o4mini_status == "error":
                    models_used.append(f"o4-mini (GitHub Models) (Error)")
                # â–²â–²â–² o4-mini Status ã“ã“ã¾ã§ â–²â–²â–²
                
                
                st.caption(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {' + '.join(models_used)}")
                
                # â–¼â–¼â–¼ å‡¦ç†å±¥æ­´ã‚’æœ€çµ‚å›ç­”ã®å†’é ­ã«è¿½åŠ  â–¼â–¼â–¼
                processing_history = []
                processing_history.append("**Phase 1**: Gemini ãƒªã‚µãƒ¼ãƒ (Googleæ¤œç´¢)")
                
                if enable_meta:
                    processing_history.append("**Phase 1.5a**: Gemini ãƒ¡ã‚¿è³ªå•ç”Ÿæˆ")
                # Grok/ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ« status
                if grok_status == "success":
                    processing_history.append(f"**Phase 1.5b**: OpenRouterã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ« ({SECONDARY_MODEL_NAME}) ç‹¬ç«‹æ€è€ƒ âœ“")
                elif grok_status == "error":
                    msg = grok_error_msg or "ã‚¨ãƒ©ãƒ¼"
                    processing_history.append(f"**Phase 1.5b**: OpenRouterã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ« ({SECONDARY_MODEL_NAME}) ç‹¬ç«‹æ€è€ƒ âš ï¸ {msg}")
                elif grok_status == "empty":
                    processing_history.append(f"**Phase 1.5b**: OpenRouterã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¢ãƒ‡ãƒ« ({SECONDARY_MODEL_NAME}) ç‹¬ç«‹æ€è€ƒï¼ˆå‡ºåŠ›ãªã—ï¼‰")
                
                # Phase 1.5c (Puter) ã¯å»ƒæ­¢
                
                
                if claude45_status == "success":
                    processing_history.append(f"**Phase 1.5d**: Claude 4.5 Sonnet ç‹¬ç«‹æ€è€ƒ (AWS Bedrock) âœ“")
                elif claude45_status == "error":
                    processing_history.append(f"**Phase 1.5d**: Claude 4.5 Sonnet ç‹¬ç«‹æ€è€ƒ âš ï¸ ã‚¨ãƒ©ãƒ¼")
                
                if o4mini_status == "success":
                    processing_history.append(f"**Phase 1.5e**: o4-mini ç‹¬ç«‹æ€è€ƒ (GitHub Models) âœ“")
                elif o4mini_status == "error":
                    processing_history.append(f"**Phase 1.5e**: o4-mini ç‹¬ç«‹æ€è€ƒ âš ï¸ ã‚¨ãƒ©ãƒ¼")
                
                processing_history.append("**Phase 2**: Gemini çµ±åˆãƒ•ã‚§ãƒ¼ã‚º")
                
                if enable_strict:
                    processing_history.append("**Phase 3**: Gemini é¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    if use_grok_reviewer:
                        if grok_review_status == "success":
                            processing_history.append(f"**Phase 3b**: {SECONDARY_MODEL_NAME} æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ âœ“")
                        else:
                            processing_history.append(f"**Phase 3b**: {SECONDARY_MODEL_NAME} æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ âš ï¸ ã‚¨ãƒ©ãƒ¼")
                
                # å‡¦ç†å±¥æ­´ã‚’æœ€çµ‚å›ç­”ã«è¿½åŠ ï¼ˆGrokãƒ¬ãƒ“ãƒ¥ãƒ¼æˆåŠŸæ™‚ã¯æ—¢ã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                if grok_review_status == "success":
                    # Grokãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒæ—¢ã«å‡¦ç†å±¥æ­´ã‚’å«ã‚ã¦ã„ã‚‹ã®ã§ãã®ã¾ã¾ä½¿ç”¨
                    final_answer_with_history = final_answer
                else:
                    # Grokãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒãªã„ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼æ™‚ã®ã¿å‡¦ç†å±¥æ­´ã‚’è¿½åŠ 
                    final_answer_with_history = (
                        "## ğŸ“Š å‡¦ç†å±¥æ­´\n\n"
                        + "\n".join([f"- {item}" for item in processing_history])
                        + "\n\n---\n\n"
                        + final_answer
                    )
                
                # æ”¹è¡Œåœ§ç¸®ï¼š3è¡Œä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2è¡Œã«åœ§ç¸®
                final_answer_with_history = compact_newlines(final_answer_with_history)
                
                # ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ä»˜ãå›ç­”è¡¨ç¤º
                import html
                escaped_answer = html.escape(final_answer_with_history)
                answer_id = f"assistant_msg_{len(messages)}"
                
                st.markdown(f"""
<div style="position: relative;">
    <div id="{answer_id}" style="padding-right: 40px; white-space: pre-wrap;">{escaped_answer}</div>
    <button onclick="copyToClipboard('{answer_id}')" style="position: absolute; right: 0; top: 0; background: transparent; border: 1px solid #444; border-radius: 4px; cursor: pointer; padding: 4px 8px; color: #aaa; font-size: 12px;" title="ã‚³ãƒ”ãƒ¼">
        ğŸ“‹
    </button>
</div>
<script>
function copyToClipboard(elementId) {{
    const element = document.getElementById(elementId);
    const text = element.innerText;
    navigator.clipboard.writeText(text).then(() => {{
        const button = event.target;
        button.textContent = 'âœ“';
        setTimeout(() => {{ button.textContent = 'ğŸ“‹'; }}, 1000);
    }});
}}
</script>
""", unsafe_allow_html=True)
                
                # â–¼â–¼â–¼ ã‚³ã‚¹ãƒˆã‚µãƒãƒªãƒ¼è¡¨ç¤º â–¼â–¼â–¼
                st.markdown("---")
                st.markdown("## ğŸ’° ã‚³ã‚¹ãƒˆã‚µãƒãƒªãƒ¼")
                
                # Claude 4.5 Sonnet ã®ã‚³ã‚¹ãƒˆ
                claude_cost = 0.0
                if claude45_usage:
                    input_tokens = claude45_usage.get("inputTokens", 0)
                    output_tokens = claude45_usage.get("outputTokens", 0)
                    claude_cost = (input_tokens / 1_000_000) * 3.0 + (output_tokens / 1_000_000) * 15.0
                    st.markdown(f"**Claude 4.5 Sonnet (AWS Bedrock)**")
                    st.markdown(f"- Input: {input_tokens:,} tokens")
                    st.markdown(f"- Output: {output_tokens:,} tokens")
                    st.markdown(f"- ã‚³ã‚¹ãƒˆ: ${claude_cost:.4f}")
                    st.markdown("")
                
                # Gemini ã®ã‚³ã‚¹ãƒˆ (total_session_cost - claude_cost)
                gemini_cost = st.session_state.session_cost - claude_cost
                if gemini_cost > 0:
                    st.markdown(f"**Gemini (gemini-3-pro-preview)**")
                    st.markdown(f"- ã‚³ã‚¹ãƒˆ: ${gemini_cost:.4f}")
                    st.markdown("")
                
                # åˆè¨ˆ
                total_cost = st.session_state.session_cost
                st.markdown(f"**åˆè¨ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚³ã‚¹ãƒˆ**: ${total_cost:.4f}")
                    
                # â–²â–²â–² å‡¦ç†å±¥æ­´è¿½åŠ ã“ã“ã¾ã§ â–²â–²â–²

                # ---- ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ± ----
                if grounding_metadata:
                    st.markdown("---")
                    with st.expander("ğŸ“š æƒ…å ±æºã¨å¼•ç”¨", expanded=False):
                        if grounding_metadata.grounding_chunks:
                            st.markdown("**æ¤œç´¢çµæœã‹ã‚‰åˆ©ç”¨ã—ãŸæƒ…å ±æº:**")
                            unique_sources = {}
                            import urllib.parse

                            for chunk in grounding_metadata.grounding_chunks:
                                if getattr(chunk, "web", None):
                                    uri = getattr(chunk.web, "uri", None)
                                    title = getattr(chunk.web, "title", "æƒ…å ±æº")
                                    if uri and uri not in unique_sources:
                                        parsed = urllib.parse.urlparse(uri)
                                        domain = parsed.netloc.replace("www.", "")
                                        unique_sources[uri] = {
                                            "title": title,
                                            "domain": domain,
                                        }
                            for i, (uri, info) in enumerate(unique_sources.items(), 1):
                                st.markdown(f"{i}. **[{info['title']}]({uri})**")
                                st.caption(f"   å‡ºå…¸: {info['domain']}")

                # â–¼â–¼â–¼ Deep Log: æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä¿å­˜ï¼ˆç¢ºå®Ÿæ€§å‘ä¸Šï¼‰ â–¼â–¼â–¼
                reasoning_logs = {
                    "phase1_research": research_text if 'research_text' in dir() else None,
                    "phase1_5_meta_questions": questions_text if 'questions_text' in dir() else None,
                    "phase1_5b_secondary": grok_thought if 'grok_thought' in dir() else None,
                    "phase1_5d_claude": claude45_thought if 'claude45_thought' in dir() else None,
                    "phase1_5e_o4mini": o4mini_thought if 'o4mini_thought' in dir() else None,
                    "phase2_draft": draft_answer if 'draft_answer' in dir() else None,
                }
                
                # æƒ…å ±æºURLã‚’æŠ½å‡º
                grounding_sources = []
                if grounding_metadata and hasattr(grounding_metadata, 'grounding_chunks'):
                    for chunk in grounding_metadata.grounding_chunks:
                        if hasattr(chunk, 'web') and hasattr(chunk.web, 'uri'):
                            grounding_sources.append(chunk.web.uri)
                
                messages.append({
                    "role": "model",
                    "content": final_answer_with_history,
                    "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "reasoning_logs": reasoning_logs,  # Deep Log
                    "metadata": {
                        "model": model_id,
                        "cost": round(st.session_state.session_cost, 4),
                        "sources": grounding_sources[:10]  # æœ€å¤§10ä»¶
                    }
                })
                # â–²â–²â–² Deep Log ã“ã“ã¾ã§ â–²â–²â–²
                update_current_session_messages(messages)

            except Exception as e:
                # ğŸ”¥ å®Ÿè¡Œå®Œé‚ä¿è¨¼: ã©ã‚“ãªã‚¨ãƒ©ãƒ¼ã§ã‚‚å¿…ãšå›ç­”ã‚’ç”Ÿæˆ
                import traceback
                err_text = str(e)
                error_traceback = traceback.format_exc()
                print(f"[ERROR] Main processing failed: {err_text}")
                print(error_traceback)
                
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’ç”Ÿæˆ
                fallback_answer = ""
                
                if "RESOURCE_EXHAUSTED" in err_text or "429" in err_text:
                    fallback_answer = (
                        "## âš ï¸ ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«ã‚ˆã‚Šå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ\n\n"
                        "Vertex AI / Gemini ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚\n\n"
                        "### å¯¾å‡¦æ³•:\n"
                        "1. **æ•°åˆ†å¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œ**ã—ã¦ãã ã•ã„\n"
                        "2. Google Cloud Console ã®ã€ŒVertex AI â†’ ä½¿ç”¨çŠ¶æ³ã€ã§ã‚¯ã‚©ãƒ¼ã‚¿ã‚’ç¢ºèª\n"
                        "3. å¿…è¦ã«å¿œã˜ã¦ã‚¯ã‚©ãƒ¼ã‚¿å¢—åŠ ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\n\n"
                        f"### è³ªå•å†…å®¹ï¼ˆä¿å­˜æ¸ˆã¿ï¼‰:\n{prompt[:500]}..."
                    )
                elif "NOT_FOUND" in err_text and "Publisher Model" in err_text:
                    fallback_answer = (
                        "## âš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\n\n"
                        "æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ / ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n\n"
                        "### å¯¾å‡¦æ³•:\n"
                        "ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ¢ãƒ‡ãƒ«IDã‚’ã€`gemini-2.5-pro` ã¾ãŸã¯ `gemini-3-pro-preview` ã«å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚"
                    )
                else:
                    fallback_answer = (
                        "## âš ï¸ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\n\n"
                        f"**ã‚¨ãƒ©ãƒ¼å†…å®¹**: `{err_text[:200]}`\n\n"
                        "### è‡ªå‹•ãƒªã‚«ãƒãƒªãƒ¼ã‚’è©¦ã¿ã¦ã„ã¾ã™...\n\n"
                        f"### è³ªå•å†…å®¹ï¼ˆä¿å­˜æ¸ˆã¿ï¼‰:\n{prompt[:500]}...\n\n"
                        "**æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
                    )
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’è¡¨ç¤º
                st.error(fallback_answer)
                
                # ğŸ”¥ é‡è¦: ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å›ç­”ã‚’å±¥æ­´ã«ä¿å­˜ï¼ˆæ¬¡å›å‚ç…§ç”¨ï¼‰
                messages.append({
                    "role": "model",
                    "content": fallback_answer,
                    "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "error": True  # ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ©ã‚°
                })
                update_current_session_messages(messages)
