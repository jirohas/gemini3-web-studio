import os
import uuid
import datetime
import streamlit as st
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
import io
from logic import (
    USAGE_FILE, SESSIONS_FILE, MAX_BUDGET_USD, PRICING,
    VERTEX_PROJECT, VERTEX_LOCATION,
    load_usage, save_usage, calculate_cost, get_mime_type,
    extract_youtube_id, get_youtube_transcript, get_relevant_context,
    extract_text_from_response, load_sessions, save_sessions, get_client
)

try:
    from st_img_pastebutton import paste
    import_error_msg = None
except ImportError as e:
    paste = None
    import_error_msg = str(e)

# =========================
# ç’°å¢ƒå¤‰æ•° & å®šæ•°
# =========================

load_dotenv()



st.set_page_config(page_title="Gemini 3 Web Studio", layout="wide")

# ğŸ” ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ­ãƒƒã‚¯ (***REMOVED***) + URLãƒˆãƒ¼ã‚¯ãƒ³æ°¸ç¶šåŒ–
SECRET_TOKEN = "access_granted_***REMOVED***"

# 1. URLãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
query_params = st.query_params
url_token = query_params.get("auth", None)

if url_token == SECRET_TOKEN:
    st.session_state.authenticated = True
elif "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# 2. æœªèªè¨¼ãªã‚‰ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ç”»é¢
if not st.session_state.authenticated:
    st.title("Gemini 3 Studio")
    st.write("ã“ã®ã‚¢ãƒ—ãƒªã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™ã€‚")

    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")

    if st.button("ãƒ­ã‚°ã‚¤ãƒ³"):
        if password == "***REMOVED***":
            st.session_state.authenticated = True
            # URLã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä»˜ä¸ã—ã¦ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã§ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯å¯èƒ½ã«ãªã‚‹ï¼‰
            st.query_params["auth"] = SECRET_TOKEN
            st.rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™ã€‚")

    st.stop()

# =========================
# Helper Functions
# =========================

# =========================
# Helper Functions
# =========================
# Moved to logic.py

# =========================
# Grok Review Function (OpenRouter API)
# =========================

import requests
from curl_cffi import requests as crequests  # Puterç”¨

# OpenRouter API Keyã®å–å¾— (st.secretså„ªå…ˆã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°)
try:
    if "OPENROUTER_API_KEY" in st.secrets:
        OPENROUTER_API_KEY = st.secrets["OPENROUTER_API_KEY"]
    else:
        OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")
except:
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")

# Puter èªè¨¼æƒ…å ±
PUTER_USERNAME = None
PUTER_PASSWORD = None
try:
    if "PUTER_USERNAME" in st.secrets:
        PUTER_USERNAME = st.secrets["PUTER_USERNAME"]
        PUTER_PASSWORD = st.secrets["PUTER_PASSWORD"]
    else:
        PUTER_USERNAME = os.getenv("PUTER_USERNAME")
        PUTER_PASSWORD = os.getenv("PUTER_PASSWORD")
except Exception:
    PUTER_USERNAME = os.getenv("PUTER_USERNAME")
    PUTER_PASSWORD = os.getenv("PUTER_PASSWORD")

def review_with_grok(user_question: str, gemini_answer: str, research_text: str = None) -> str:
    """
    Grok 4.1 Fast Free ã‚’ä½¿ã£ã¦ã€Geminiã®å›ç­”ã‚’æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹
    research_text: GeminiãŒåé›†ã—ãŸèª¿æŸ»ãƒ¡ãƒ¢ï¼ˆæœ€æ–°æƒ…å ±ã‚’å«ã‚€ï¼‰
    """
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
    user_content = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
    
    if research_text:
        user_content += f"Gemini ã®èª¿æŸ»ãƒ¡ãƒ¢ï¼ˆæœ€æ–°æƒ…å ±ï¼‰:\n{research_text}\n\n"
    
    user_content += (
        f"Gemini ã®æœ€çµ‚å›ç­”:\n{gemini_answer}\n\n"
        "**é‡è¦**: èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹äº‹å®Ÿã®æ–¹ã‚’ã€ã‚ãªãŸè‡ªèº«ã®çŸ¥è­˜ã‚ˆã‚Šã‚‚å„ªå…ˆã—ã¦ãã ã•ã„ã€‚\n"
        "æœ€æ–°ã®æƒ…å ±ãŒèª¿æŸ»ãƒ¡ãƒ¢ã«ã‚ã‚‹å ´åˆã€ãã‚Œã‚’ä¿¡é ¼ã—ã¦ãã ã•ã„ã€‚\n\n"
        "1. **é‡å¤§ãªäº‹å®Ÿèª¤èªãƒ»è«–ç†é£›èºã®æŒ‡æ‘˜**ï¼ˆãªã‘ã‚Œã°ã€ç‰¹ã«ãªã—ã€ï¼‰\n"
        "2. **å…·ä½“çš„ãªä¿®æ­£ææ¡ˆã®ãƒªã‚¹ãƒˆ**ï¼ˆã€ä¿®æ­£å‰ã€â†’ã€ä¿®æ­£å¾Œã€ã®å½¢å¼ã§ï¼‰\n"
        "3. **ç‰¹ã«ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„ãƒã‚¤ãƒ³ãƒˆï¼ˆå‰æä¸è¶³ã€ãƒ‡ãƒ¼ã‚¿ä¸è¶³ãªã©ï¼‰ã® bullet list**\n"
    )
    
    data = {
        "model": "x-ai/grok-4.1-fast:free",
        "reasoning": {"enabled": True},  # Enable multi-step reasoning for deeper review
        "messages": [
            {
                "role": "system",
                "content": (
                    "ã‚ãªãŸã¯å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ã§ã™ã€‚"
                    "äº‹å®Ÿèª¤èªãƒ»è«–ç†ã®é£›èºãƒ»æŠœã‘æ¼ã‚Œã‚’å®¹èµ¦ãªãæŒ‡æ‘˜ã—ã€"
                    "å¿…è¦ãªã‚‰å›ç­”ã‚’å…¨æ–‡æ›¸ãç›´ã—ã¦ãã ã•ã„ã€‚"
                ),
            },
            {
                "role": "user",
                "content": user_content.strip() + (
                    "ã ã‘ã‚’æ—¥æœ¬èªã§å‡ºã—ã¦ãã ã•ã„ã€‚"
                ),
            },
        ],
    }
    try:
        resp = requests.post(url, headers=headers, json=data, timeout=60)
        resp.raise_for_status()
        j = resp.json()
        return j["choices"][0]["message"]["content"]
    except Exception as e:
        return f"[Grokãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}]\n\n{gemini_answer}"

# =========================
# Session Management
# =========================



def think_with_grok(user_question: str, research_text: str, enable_x_search: bool = False) -> str:
    """
    Grok 4.1 Fast Free ã‚’ä½¿ã£ã¦ã€ãƒªã‚µãƒ¼ãƒãƒ¡ãƒ¢ã‚’å…ƒã«ç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã™ã‚‹
    enable_x_search=True ã®å ´åˆã€X/Twitteræƒ…å ±ã®æ´»ç”¨ã‚’ä¿ƒã™
    """
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    
    # Xæ¤œç´¢å¼·åŒ–ç‰ˆã®å ´åˆã€ç‰¹åˆ¥ãªæŒ‡ç¤ºã‚’è¿½åŠ 
    x_search_instruction = ""
    if enable_x_search:
        x_search_instruction = (
            "\n\n**é‡è¦**: ã‚ãªãŸã¯Grokã¨ã—ã¦Xï¼ˆTwitterï¼‰ã®æƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚\n"
            "ä¸Šè¨˜ã®èª¿æŸ»ãƒ¡ãƒ¢ã«åŠ ãˆã¦ã€Xä¸Šã®æœ€æ–°ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»è­°è«–ãƒ»åå¿œã‚’è€ƒæ…®ã—ã€\n"
            "ãã‚Œã‚‰ã‚’å«ã‚ãŸç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n"
            "æ³¨æ„: Xä¸Šã®æƒ…å ±ãŒç¢ºèªã§ããªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ­£ç›´ã«è¿°ã¹ã¦ãã ã•ã„ã€‚\n"
            "æ¶ç©ºã®æŠ•ç¨¿ã‚„å­˜åœ¨ã—ãªã„åå¿œã‚’ä½œæˆã—ãªã„ã“ã¨ã€‚"
        )
    
    user_content = (
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
        f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
        "æŒ‡ç¤º:\n"
        "ã‚ãªãŸã¯Geminiã¨ã¯åˆ¥ã®ç‹¬ç«‹ã—ãŸAIã§ã™ã€‚\n"
        "ä¸Šè¨˜ã®èª¿æŸ»ãƒ¡ãƒ¢ã‚’å‚è€ƒã«ã—ã¤ã¤ã‚‚ã€ã‚ãªãŸè‡ªèº«ã®è¦–ç‚¹ã§ç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
        "Geminiã®æ„è¦‹ã«åˆã‚ã›ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        f"{x_search_instruction}"
    )
    
    data = {
        "model": "x-ai/grok-2-1212",
        "messages": [
            {"role": "user", "content": user_content}
        ],
    }
    
    try:
        resp = requests.post(url, headers=headers, json=data, timeout=60)
        resp.raise_for_status()
        j = resp.json()
        return j["choices"][0]["message"]["content"]
    except Exception as e:
        return f"[Grokæ€è€ƒã‚¨ãƒ©ãƒ¼: {e}]"

def _extract_puter_text(message_content):
    """
    puter.ai.chat ã®è¿”ã‚Šå€¤ã‹ã‚‰ ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã‚’å–ã‚Šå‡ºã™
    """
    if isinstance(message_content, str):
        return message_content

    if isinstance(message_content, list):
        chunks = []
        for part in message_content:
            if isinstance(part, dict) and part.get("type") == "text":
                chunks.append(part.get("text", ""))
        return "".join(chunks)

    return str(message_content)


def call_claude_opus_via_puter(
    user_question: str,
    research_text: str | None = None,
    system_prompt: str | None = None,
) -> str:
    """
    Claude Opus 4.5ï¼ˆputer.comï¼‰ã§æ¨è«–ã•ã›ã‚‹åŒæœŸé–¢æ•°
    curl_cffiã§Chrome 124ã‚’å½è£…ã—ã¦ç›´æ¥APIã‚’å©ã
    """
    if not PUTER_USERNAME or not PUTER_PASSWORD:
        return "[Claude (puter) èªè¨¼æƒ…å ±æœªè¨­å®š]"

    # 1. Prepare Headers (Chrome 124 Masquerading)
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Accept": "*/*",
        "Accept-Language": "en-US,en;q=0.9,ja;q=0.8",
        "Origin": "https://puter.com",
        "Referer": "https://puter.com/",
        "Sec-Ch-Ua": '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        "Sec-Ch-Ua-Mobile": "?0",
        "Sec-Ch-Ua-Platform": '"macOS"',
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-site",
        "Content-Type": "application/json"
    }

    token = st.session_state.get("puter_token")

    # ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„ã€ã¾ãŸã¯å†è©¦è¡ŒãŒå¿…è¦ãªå ´åˆã®ãƒ­ã‚°ã‚¤ãƒ³é–¢æ•°
    def perform_login():
        try:
            resp = crequests.post(
                "https://api.puter.com/login",
                json={"username": PUTER_USERNAME, "password": PUTER_PASSWORD},
                headers=headers,
                timeout=30,
                impersonate="chrome124"
            )
            if resp.status_code != 200:
                return None, f"[Claude Login Error] Status: {resp.status_code}"
            
            new_token = resp.json().get("token")
            if not new_token:
                return None, "[Claude Login Error] Token not found"
            
            return new_token, None
        except Exception as e:
            return None, f"[Claude Login Error] {str(e)}"

    # ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„å ´åˆã¯ãƒ­ã‚°ã‚¤ãƒ³
    if not token:
        token, error = perform_login()
        if error:
            return error
        st.session_state.puter_token = token

    # 2. Chat
    chat_url = "https://api.puter.com/drivers/call"
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    user_content = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
    if research_text:
        user_content += f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
        user_content += "ã“ã®èª¿æŸ»ãƒ¡ãƒ¢ã®äº‹å®Ÿã‚’å„ªå…ˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"

    messages.append({"role": "user", "content": user_content})
    
    payload = {
        "interface": "puter-chat-completion",
        "driver": "claude",
        "method": "complete",
        "args": {
            "messages": messages,
            "model": "claude-opus-4-5",
            "stream": False
        }
    }

    try:
        # åˆå›ãƒˆãƒ©ã‚¤
        auth_headers = headers.copy()
        auth_headers["Authorization"] = f"Bearer {token}"
        chat_resp = crequests.post(chat_url, json=payload, headers=auth_headers, timeout=120, impersonate="chrome124")

        # 401/403ã®å ´åˆã¯å†ãƒ­ã‚°ã‚¤ãƒ³
        if chat_resp.status_code in [401, 403]:
            token, error = perform_login()
            if error:
                return error
            st.session_state.puter_token = token
            
            # ãƒªãƒˆãƒ©ã‚¤
            auth_headers["Authorization"] = f"Bearer {token}"
            chat_resp = crequests.post(chat_url, json=payload, headers=auth_headers, timeout=120, impersonate="chrome124")

        if chat_resp.status_code != 200:
            return f"[Claude API Error] Status: {chat_resp.status_code}"
            
        result = chat_resp.json()
        if not result.get("success", False):
             return f"[Claude API Error] {result.get('error', 'Unknown error')}"

        message = result["result"]["message"]
        return _extract_puter_text(message.get("content"))

    except Exception as e:
        return f"[Claude Error] {str(e)}"

def create_new_session():
    current_sessions = load_sessions()
    
    # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒç©ºãªã‚‰ã€æ–°ã—ãä½œã‚‰ãšã«ãã‚Œã‚’å†åˆ©ç”¨ã™ã‚‹ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
    if st.session_state.get("current_session_id"):
        for s in current_sessions:
            if s["id"] == st.session_state.current_session_id:
                if len(s["messages"]) == 0:
                    st.toast("ã™ã§ã«æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã§ã™")
                    return

    new_id = str(uuid.uuid4())
    new_session = {
        "id": new_id,
        "title": "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ",
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": [],
    }
    current_sessions.insert(0, new_session)
    save_sessions(current_sessions)
    st.session_state.sessions = current_sessions
    st.session_state.current_session_id = new_id
    st.rerun()

def switch_session(session_id):
    st.session_state.current_session_id = session_id
    st.rerun()

def update_current_session_messages(messages):
    if st.session_state.current_session_id:
        current_sessions = load_sessions()
        target_index = -1
        for i, session in enumerate(current_sessions):
            if session["id"] == st.session_state.current_session_id:
                session["messages"] = messages
                if session["title"] == "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ" and len(messages) > 0:
                    first_msg = messages[0]["content"]
                    session["title"] = (first_msg[:20] + "...") if len(first_msg) > 20 else first_msg
                session["timestamp"] = datetime.datetime.now().isoformat()
                target_index = i
                break
        
        if target_index != -1:
            # æœ€æ–°ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚¹ãƒˆã®å…ˆé ­ã«ç§»å‹•
            updated_session = current_sessions.pop(target_index)
            current_sessions.insert(0, updated_session)
            
        save_sessions(current_sessions)
        st.session_state.sessions = current_sessions

def get_current_messages():
    if st.session_state.current_session_id:
        for session in st.session_state.sessions:
            if session["id"] == st.session_state.current_session_id:
                return session["messages"]
    return []

def delete_session(session_id):
    current_sessions = load_sessions()
    current_sessions = [s for s in current_sessions if s["id"] != session_id]
    save_sessions(current_sessions)
    st.session_state.sessions = current_sessions
    if st.session_state.current_session_id == session_id:
        st.session_state.current_session_id = None
        if st.session_state.sessions:
            st.session_state.current_session_id = st.session_state.sessions[0]["id"]
    st.rerun()

def branch_session():
    """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’åˆ†å²"""
    current_messages = get_current_messages()
    current_sessions = load_sessions()
    
    # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    current_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
    for session in current_sessions:
        if session["id"] == st.session_state.current_session_id:
            current_title = session["title"]
            break
    
    # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    new_id = str(uuid.uuid4())
    new_session = {
        "id": new_id,
        "title": f"{current_title} (åˆ†å²)",
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": current_messages.copy(),  # ç¾åœ¨ã®å±¥æ­´ã‚’ã‚³ãƒ”ãƒ¼
    }
    current_sessions.insert(0, new_session)
    save_sessions(current_sessions)
    
    st.session_state.sessions = current_sessions
    st.session_state.current_session_id = new_id
    st.session_state.session_cost = 0.0  # ã‚³ã‚¹ãƒˆãƒªã‚»ãƒƒãƒˆ
    st.rerun()

# =========================
# Initialization
# =========================

if "sessions" not in st.session_state:
    st.session_state.sessions = load_sessions()

if "current_session_id" not in st.session_state:
    # Always create a new session when the app starts
    create_new_session()

if "session_cost" not in st.session_state:
    st.session_state.session_cost = 0.0

usage_stats = load_usage()

# ==========================
# ã‚³ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã®å‰ã«å®Ÿè¡Œï¼‰
# ==========================
usage_stats = load_usage()
stop_generation = usage_stats["total_cost_usd"] >= MAX_BUDGET_USD

# =========================
# Sidebar
# =========================

with st.sidebar:
    # ğŸ” ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³
    if st.button("ğŸ”’ ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", use_container_width=True):
        st.session_state.authenticated = False
        st.query_params.clear()  # URLãƒˆãƒ¼ã‚¯ãƒ³ã‚‚å‰Šé™¤
        st.rerun()

    st.markdown("""
    <div style="text-align: center; padding: 0; margin: 0; margin-top: -1rem;">
        <h1 style="font-size: 18px; font-weight: 700; margin: 0; padding: 0; letter-spacing: 1px;">
            Gemini 3<br/>Studio
        </h1>
    </div>
    """, unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• æ–°è¦", use_container_width=True):
            create_new_session()
    with col2:
        if st.button("ğŸŒ± åˆ†å²", use_container_width=True):
            branch_session()

    # ---- å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ ----
    # ---- å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ ----
    current_messages = get_current_messages()
    if current_messages:
        with st.expander("ğŸ”— å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ", expanded=False):
            export_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
            for s in st.session_state.sessions:
                if s["id"] == st.session_state.current_session_id:
                    export_title = s["title"]
                    break

            export_md = f"# {export_title}\n\n"
            export_md += f"**ä½œæˆæ—¥æ™‚**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
            for msg in current_messages:
                role_label = "ğŸ§‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg["role"] == "user" else "ğŸ¤– AI"
                export_md += f"## {role_label}\n\n{msg['content']}\n\n---\n\n"

            if st.button("ãƒªãƒ³ã‚¯ç”Ÿæˆ", use_container_width=True):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    try:
                        import urllib.request
                        import urllib.parse

                        data = urllib.parse.urlencode(
                            {"content": export_md, "expiry_days": 7, "syntax": "md"}
                        ).encode()

                        req = urllib.request.Request("https://dpaste.org/api/", data=data)
                        req.add_header("User-Agent", "Mozilla/5.0 (Gemini3Studio)")

                        with urllib.request.urlopen(req) as response:
                            share_url = response.read().decode("utf-8").strip()
                            st.success("ä½œæˆå®Œäº†ï¼")
                            st.code(share_url)
                            st.caption("â€»7æ—¥é–“æœ‰åŠ¹")
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    st.markdown("---")

    # ---- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ ----
    # ---- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ ----
    with st.expander("ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«", expanded=False):
        uploaded_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«",
            accept_multiple_files=True,
            type=["png", "jpg", "jpeg", "mp4", "mov", "txt", "pdf", "csv"],
            label_visibility="collapsed"
        )

    # ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ï¼ˆExpanderã®å¤–ã«å‡ºã™ï¼‰
    pasted_image_bytes = None  # åˆæœŸåŒ–ã—ã¦NameErrorã‚’é˜²ã
    if paste:
        if "paste_key" not in st.session_state:
            st.session_state.paste_key = 0
        try:
            pasted_image_bytes = paste(
                label="ğŸ“‹ ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰è²¼ä»˜",
                key=f"paste_btn_{st.session_state.paste_key}",
            )
        except Exception as e:
            st.error(f"Error: {e}")
        
        if pasted_image_bytes:
            st.success("è²¼ä»˜å®Œäº†")
            st.image(pasted_image_bytes, caption="ç”»åƒ", use_container_width=True)
            if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", key="clear_paste"):
                st.session_state.paste_key += 1
                st.rerun()
    else:
        st.warning("Clipboard lib missing")

    # ---- YouTube URL ----
    with st.expander("ğŸ“º YouTubeåˆ†æ", expanded=False):
        youtube_url = st.text_input("URL", placeholder="https://youtu.be/...", label_visibility="collapsed")

    st.markdown("---")
    
    st.markdown("---")
    
    # ---- ãƒ¢ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªé¸æŠ ----
    mode_category = st.radio(
        "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰",
        ["ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)", "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(é€šå¸¸)", "Î²ï¼šğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤+puter)"],
        index=2,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’Puterãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
        horizontal=True,
    )
    
    # ---- å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ ----
    if mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)":
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(å¤šå±¤)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["é¸æŠ1 (å®Œå…¨ç‰ˆ)", "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)", "ãƒ™ãƒ¼ã‚¿ç‰ˆ"],
                index=0,
                horizontal=True,
                label_visibility="collapsed"
            )
            
            if mode_type == "é¸æŠ1 (å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. (è©¦é¨“ä¸­)ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "2. ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "3. (è©¦é¨“ä¸­)ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                        "4. ç†Ÿè€ƒ(ãƒ¡ã‚¿æ€è€ƒ)+grokæ¤œç´¢å¼·åŒ–ç‰ˆ",
                    ],
                    index=1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ãƒ¡ã‚¿æ€è€ƒã«å¤‰æ›´
                )
            elif mode_type == "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                    ],
                    index=0
                )
            else:
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    # ---- å¤šå±¤+puterãƒ¢ãƒ¼ãƒ‰ ----
    elif mode_category == "Î²ï¼šğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤+puter)":
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(å¤šå±¤+puter)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["é¸æŠ1 (å®Œå…¨ç‰ˆ)", "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)", "ãƒ™ãƒ¼ã‚¿ç‰ˆ"],
                index=0,
                horizontal=True,
                label_visibility="collapsed"
            )
            
            if mode_type == "é¸æŠ1 (å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ + é¬¼è»æ›¹(localã®ã¿)",
                        "2. ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "3. ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                    ],
                    index=0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’é¬¼è»æ›¹ã«å¤‰æ›´
                )
            elif mode_type == "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                    ],
                    index=0
                )
            else:
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    # ---- é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ ----
    else:
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(é€šå¸¸)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["é¸æŠ1 (å®Œå…¨ç‰ˆ)", "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)", "ãƒ™ãƒ¼ã‚¿ç‰ˆ"],
                index=0,
                horizontal=True,
                label_visibility="collapsed"
            )
            
            if mode_type == "é¸æŠ1 (å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "2. ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "3. ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                    ],
                    index=0
                )
            elif mode_type == "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                    ],
                    index=0
                )
            else:
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    strict_mode = False
    
    # ---- è¨­å®š (ãƒ¢ãƒ‡ãƒ«ãªã©) ----
    with st.expander("âš™ï¸ è¨­å®š", expanded=False):
        model_options = [
            "gemini-3-pro-preview",
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.0-flash",
        ]
        model_id = st.selectbox("ãƒ¢ãƒ‡ãƒ«ID", options=model_options, index=0)

        use_search = st.toggle("Googleæ¤œç´¢", value=True)
        candidate_count = st.slider("å€™è£œæ•°", min_value=1, max_value=3, value=3)

    st.markdown("---")

    # ---- å±¥æ­´æ¤œç´¢ ----
    search_query = st.text_input("ğŸ” å±¥æ­´æ¤œç´¢", placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰...")
    if search_query:
        filtered_sessions = []
        for s in st.session_state.sessions:
            if search_query.lower() in s["title"].lower():
                filtered_sessions.append(s)
                continue
            found = False
            for m in s["messages"]:
                if search_query.lower() in m["content"].lower():
                    filtered_sessions.append(s)
                    found = True
                    break
            if not found:
                pass
    else:
        # æ¤œç´¢ã—ã¦ã„ãªã„å ´åˆ: ç©ºã®ã€Œæ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã€ã‚’é™¤å¤–ï¼ˆç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯é™¤ãï¼‰
        filtered_sessions = []
        for s in st.session_state.sessions:
            if s["id"] == st.session_state.current_session_id:
                filtered_sessions.append(s)
                continue
            if s["title"] == "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ" and len(s["messages"]) == 0:
                continue
            filtered_sessions.append(s)

    # CSSã§è¦‹ã‚„ã™ãä½¿ã„ã‚„ã™ãæœ€é©åŒ–
    st.markdown("""
    <style>
    /* å…¨ä½“ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºç¸®å° */
    .stApp, .stMarkdown, .stButton, .stSelectbox, .stTextInput, .stTextArea {
        font-size: 12px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä½™ç™½ã‚’æ¥µé™ã¾ã§è©°ã‚ã‚‹ */
    section[data-testid="stSidebar"] .block-container {
        padding-top: 0.3rem !important;
        padding-bottom: 0.3rem !important;
        padding-left: 0.5rem !important;
        padding-right: 0.5rem !important;
    }
    /* å„è¦ç´ é–“ã®éš™é–“ã‚’è©°ã‚ã‚‹ */
    div[data-testid="stVerticalBlock"] > div {
        gap: 0.1rem !important;
    }
    /* Expanderã®ä½™ç™½å‰Šæ¸› */
    .streamlit-expanderHeader {
        font-size: 11px !important;
        padding-top: 0px !important;
        padding-bottom: 0px !important;
        min-height: 1.3rem !important;
    }
    .streamlit-expanderContent {
        padding-top: 0px !important;
        padding-bottom: 0px !important;
    }
    /* éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ã‚¿ã‚¤ãƒˆãƒ« */
    div[data-testid="stExpander"] summary p {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ */
    section[data-testid="stSidebar"] button p {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã« */
    section[data-testid="stSidebar"] button {
        padding: 0rem 0.2rem !important;
        min-height: 1.4rem !important;
        font-size: 10px !important;
        margin-bottom: 0px !important;
    }
    /* ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®ãƒ©ãƒ™ãƒ«ã‚’å°ã•ã */
    section[data-testid="stSidebar"] .stRadio label {
        font-size: 10px !important;
    }
    section[data-testid="stSidebar"] .stRadio > label > div {
        font-size: 10px !important;
    }
    /* ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ã‚‹ï¼ˆæ”¹è¡Œã•ã›ãªã„ï¼‰ */
    section[data-testid="stSidebar"] label, 
    section[data-testid="stSidebar"] p,
    section[data-testid="stSidebar"] span {
        white-space: nowrap !important;
        overflow: hidden !important;
        text-overflow: ellipsis !important;
    }
    /* åŒºåˆ‡ã‚Šç·š */
    hr {
        margin-top: 0.1rem !important;
        margin-bottom: 0.1rem !important;
    }
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ç¸®å° */
    h1, h2, h3 {
        padding-top: 0rem !important;
        padding-bottom: 0rem !important;
        margin-top: 0rem !important;
        margin-bottom: 0rem !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§é™é †ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°ãŒå…ˆé ­ï¼‰
    filtered_sessions.sort(key=lambda s: s.get("timestamp", ""), reverse=True)
    
    # ç›´è¿‘5ä»¶ã¨éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«åˆ†å‰²
    recent_sessions = filtered_sessions[:5] if len(filtered_sessions) > 5 else filtered_sessions
    archive_sessions = filtered_sessions[5:] if len(filtered_sessions) > 5 else []
    
    # ç›´è¿‘5ä»¶ï¼ˆå¸¸ã«å±•é–‹ï¼‰
    if recent_sessions:
        st.markdown("**ğŸ“Œ ç›´è¿‘ã®ãƒãƒ£ãƒƒãƒˆ**")
        for session in recent_sessions:
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                if st.button(session["title"], key=f"btn_{session['id']}", use_container_width=True):
                    switch_session(session["id"])
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_{session['id']}"):
                    delete_session(session["id"])
    
    # éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
    if archive_sessions:
        with st.expander(f"ğŸ“œ éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– ({len(archive_sessions)}ä»¶)", expanded=False):
            for session in archive_sessions:
                col1, col2 = st.columns([0.8, 0.2])
                with col1:
                    if st.button(session["title"], key=f"btn_{session['id']}", use_container_width=True):
                        switch_session(session["id"])
                with col2:
                    if st.button("ğŸ—‘ï¸", key=f"del_{session['id']}"):
                        delete_session(session["id"])

    st.markdown("---")

    # ---- ç”»åƒç”Ÿæˆ ----
    with st.expander("ğŸ¨ ç”»åƒç”Ÿæˆ", expanded=False):
        img_prompt = st.text_area("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", placeholder="æœªæ¥çš„ãªéƒ½å¸‚...")
        aspect_ratio = st.selectbox("æ¯”ç‡", ["16:9", "1:1", "4:3", "3:4", "9:16"])

        generate_img_btn = st.button("ç”Ÿæˆ", type="primary", disabled=not img_prompt)
        if generate_img_btn and img_prompt:
            st.session_state.generate_image_trigger = {
                "prompt": img_prompt,
                "aspect_ratio": aspect_ratio,
            }

    st.markdown("---")

    # ---- è©•ä¾¡åˆ†æ ----
    with st.expander("ğŸ“Š å“è³ªåˆ†æ", expanded=False):
        total_ratings = 0
        positive_ratings = 0
        model_ratings = {}

        for s in st.session_state.sessions:
            for m in s["messages"]:
                if "rating" in m and m["rating"] is not None:
                    total_ratings += 1
                    if m["rating"] == 1:
                        positive_ratings += 1
                    if "metadata" in m and "model" in m["metadata"]:
                        mod = m["metadata"]["model"]
                        if mod not in model_ratings:
                            model_ratings[mod] = {"total": 0, "positive": 0}
                        model_ratings[mod]["total"] += 1
                        if m["rating"] == 1:
                            model_ratings[mod]["positive"] += 1

        if total_ratings > 0:
            approval_rate = (positive_ratings / total_ratings) * 100
            st.metric("Positive Ratings", f"{positive_ratings}/{total_ratings}", f"{approval_rate:.1f}%")
            best_model = "N/A"
            best_rate = -1
            for mod, stats in model_ratings.items():
                rate = stats["positive"] / stats["total"]
                if rate > best_rate:
                    best_rate = rate
                    best_model = mod
            if best_model != "N/A":
                st.caption(f"ğŸ† Best: **{best_model}** ({best_rate*100:.0f}%)")
        else:
            st.caption("ãƒ‡ãƒ¼ã‚¿ãªã—")

    st.markdown("---")

    # ---- ã‚³ã‚¹ãƒˆè¡¨ç¤º ----
    from logic import load_manual_cost, save_manual_cost, MAX_BUDGET_JPY, TRIAL_LIMIT_JPY, TRIAL_EXPIRY
    
    st.subheader("ğŸ’° Cost")
    st.caption(f"äºˆç®—: Â¥{MAX_BUDGET_JPY:,.0f}")
    st.caption(f"ä¸Šé™: Â¥{TRIAL_LIMIT_JPY:,.0f}")
    st.caption(f"æœ‰åŠ¹æœŸé™: {TRIAL_EXPIRY}")
    
    # æ‰‹å‹•ã‚³ã‚¹ãƒˆå…¥åŠ›ï¼ˆæ°¸ç¶šåŒ–ï¼‰
    current_manual_cost = load_manual_cost()
    manual_cost = st.number_input(
        "æ‰‹å‹•å…¥åŠ› (Â¥)",
        min_value=0.0,
        value=current_manual_cost,
        step=10.0,
        format="%.0f",
        key="manual_cost_persistent",
        help="Google Cloud Consoleã§ç¢ºèªã—ãŸå®Ÿéš›ã®ã‚³ã‚¹ãƒˆï¼ˆå††ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ã“ã®å€¤ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¦ã‚‚ä¿æŒã•ã‚Œã¾ã™ã€‚"
    )
    
    # å€¤ãŒå¤‰æ›´ã•ã‚ŒãŸã‚‰ä¿å­˜
    if manual_cost != current_manual_cost:
        save_manual_cost(manual_cost)
    
    st.link_button("ğŸ’° Google Cloud Console", "https://console.cloud.google.com/welcome/new?_gl=1*kmr691*_up*MQ..&gclid=CjwKCAiAraXJBhBJEiwAjz7MZT0vQsfDK5zunRBCQmuN5iczgI4bP1lHo1Tcrcbqu1KCBE1D22GpFhoCOdgQAvD_BwE&gclsrc=aw.ds&hl=ja&authuser=5&project=sigma-task-479704-r6")

    st.markdown("---")
    st.code(f"PROJECT: {VERTEX_PROJECT}\nLOCATION: {VERTEX_LOCATION} (Vertex AI)")

# =========================
# Main UI
# =========================

current_session_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
for s in st.session_state.sessions:
    if s["id"] == st.session_state.current_session_id:
        current_session_title = s["title"]
        break

st.header(current_session_title)
st.markdown(
    "ä»¥ä¸‹ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€YouTubeåˆ†æã€æ¤œç´¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
)


# ---- ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³ (Floating) ----
st.markdown("""
    <style>
        .scroll-btn-container {
            position: fixed;
            bottom: 100px;
            right: 20px;
            z-index: 9999;
            display: flex;
            flex-direction: column;
            gap: 10px;
        }
        .scroll-btn {
            background-color: #f0f2f6;
            color: #31333F;
            border: 1px solid #d6d6d8;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            font-size: 20px;
            padding: 0;
            line-height: 1;
        }
        .scroll-btn:hover {
            background-color: #e0e2e6;
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
        }
    </style>
    
    <script>
        window.scrollStreamlit = function(direction) {
            console.log("Scroll triggered: " + direction);
            
            // ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹è¦ç´ ã‚’é †ç•ªã«è©¦ã™
            var targets = [];
            
            try {
                if (window.parent && window.parent.document) {
                    targets.push(window.parent.document.querySelector('section[data-testid="stAppViewContainer"]'));
                    targets.push(window.parent.document.querySelector('.main'));
                    targets.push(window.parent.document.documentElement);
                }
            } catch (e) {
                console.log("Access to parent window denied");
            }
            
            // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆiframeå†…ï¼‰
            targets.push(document.querySelector('section[data-testid="stAppViewContainer"]'));
            targets.push(document.documentElement);

            var scrolled = false;
            for (var i = 0; i < targets.length; i++) {
                var el = targets[i];
                if (el) {
                    try {
                        // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªè¦ç´ ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“çš„ï¼‰
                        if (el.scrollHeight > el.clientHeight || el === window.parent.document.documentElement) {
                            console.log("Scrolling element:", el);
                            if (direction === 'top') {
                                el.scrollTo({top: 0, behavior: 'smooth'});
                            } else {
                                el.scrollTo({top: el.scrollHeight, behavior: 'smooth'});
                            }
                            scrolled = true;
                        }
                    } catch (e) {
                        console.error("Error scrolling element:", e);
                    }
                }
            }
            
            if (!scrolled) {
                console.log("No scrollable container found, trying window scroll");
                try {
                    if (direction === 'top') {
                        window.parent.scrollTo({top: 0, behavior: 'smooth'});
                    } else {
                        window.parent.scrollTo({top: window.parent.document.body.scrollHeight, behavior: 'smooth'});
                    }
                } catch(e) {
                    window.scrollTo({top: 0, behavior: 'smooth'});
                }
            }
        }
    </script>

    <div class="scroll-btn-container">
        <button class="scroll-btn" onclick="window.scrollStreamlit('top')" title="Top">â¬†ï¸</button>
        <button class="scroll-btn" onclick="window.scrollStreamlit('bottom')" title="Bottom">â¬‡ï¸</button>
    </div>
    """, unsafe_allow_html=True)

# ---- Vertex AI Client ----



client = get_client()

# ---- å±¥æ­´è¡¨ç¤º ----

messages = get_current_messages()
for idx, msg in enumerate(messages):
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg["role"] == "model":
            col1, col2, col3 = st.columns([0.1, 0.1, 0.8])
            current_rating = msg.get("rating")
            with col1:
                if st.button("ğŸ‘", key=f"up_{idx}"):
                    messages[idx]["rating"] = 1
                    update_current_session_messages(messages)
                    st.rerun()
            with col2:
                if st.button("ğŸ‘", key=f"down_{idx}"):
                    messages[idx]["rating"] = -1
                    update_current_session_messages(messages)
                    st.rerun()
            with col3:
                if current_rating == 1:
                    st.caption("âœ… é«˜è©•ä¾¡")
                elif current_rating == -1:
                    st.caption("âŒ ä½è©•ä¾¡")

# ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³ï¼ˆé•·ã„ãƒãƒ£ãƒƒãƒˆç”¨ï¼‰
if len(messages) > 5:
    st.markdown("""
    <div style="position: fixed; right: 20px; bottom: 100px; z-index: 999;">
        <button onclick="window.scrollTo({top: 0, behavior: 'smooth'})" 
                style="display: block; margin: 5px; padding: 10px 15px; font-size: 24px; cursor: pointer; 
                       border: 2px solid #ccc; border-radius: 50%; background: white; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">
            â¬†ï¸
        </button>
        <button onclick="window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'})" 
                style="display: block; margin: 5px; padding: 10px 15px; font-size: 24px; cursor: pointer; 
                       border: 2px solid #ccc; border-radius: 50%; background: white; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">
            â¬‡ï¸
        </button>
    </div>
    """, unsafe_allow_html=True)


# =========================
# ç”»åƒç”Ÿæˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
# =========================

if hasattr(st.session_state, "generate_image_trigger") and st.session_state.generate_image_trigger:
    img_data = st.session_state.generate_image_trigger
    del st.session_state.generate_image_trigger

    with st.chat_message("user"):
        st.markdown(f"ğŸ¨ ç”»åƒç”Ÿæˆ: {img_data['prompt']}")
        st.caption(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”: {img_data['aspect_ratio']}")

    messages.append({"role": "user", "content": f"ğŸ¨ ç”»åƒç”Ÿæˆ: {img_data['prompt']}"})
    update_current_session_messages(messages)

    with st.chat_message("assistant"):
        with st.status("ç”»åƒã‚’ç”Ÿæˆä¸­...", expanded=True) as status:
            try:
                config = types.GenerateContentConfig(
                    response_modalities=["IMAGE"],
                    image_config=types.ImageConfig(
                        aspect_ratio=img_data["aspect_ratio"]
                    ),
                )
                status.write("Gemini 3 Pro Imageã§ç”Ÿæˆä¸­...")
                response = client.models.generate_content(
                    model="gemini-3-pro-image-preview",
                    contents=img_data["prompt"],
                    config=config,
                )

                generated_image = None

                # ç”»åƒã‚’å–ã‚Šå‡ºã™
                if getattr(response, "candidates", None):
                    for candidate in response.candidates:
                        if getattr(candidate, "content", None) and candidate.content.parts:
                            for part in candidate.content.parts:
                                if getattr(part, "inline_data", None):
                                    try:
                                        generated_image = part.as_image()
                                    except AttributeError:
                                        image_bytes_raw = part.inline_data.data
                                        generated_image = Image.open(io.BytesIO(image_bytes_raw))
                                    break
                        if generated_image is not None:
                            break

                if generated_image is not None:
                    st.image(generated_image, caption=img_data["prompt"])
                    buf = io.BytesIO()
                    generated_image.save(buf, format="PNG")
                    image_bytes = buf.getvalue()
                    st.download_button(
                        label="ğŸ’¾ ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=image_bytes,
                        file_name=f"generated_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                        mime="image/png",
                    )
                    messages.append(
                        {"role": "model", "content": f"âœ… ç”»åƒã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {img_data['prompt']}"}
                    )
                    update_current_session_messages(messages)
                    status.update(label="âœ… ç”»åƒç”Ÿæˆå®Œäº†", state="complete")
                else:
                    st.error("ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error")

            except Exception as e:
                status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error")
                st.error(f"ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

# =========================
# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
# =========================

prompt = st.chat_input("ä½•ã‹èã„ã¦ãã ã•ã„...", disabled=stop_generation)

if prompt:
    if stop_generation:
        st.error("äºˆç®—ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚")
    else:
        # ---- ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€è¡¨ç¤º ----
        with st.chat_message("user"):
            st.markdown(prompt)
            if uploaded_files:
                for uf in uploaded_files:
                    st.caption(f"ğŸ“ æ·»ä»˜: {uf.name}")
            if youtube_url:
                st.caption(f"ğŸ“º YouTube: {youtube_url}")
            if pasted_image_bytes:
                st.caption("ğŸ“‹ ç”»åƒãŒè²¼ã‚Šä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ")

        messages.append({"role": "user", "content": prompt})
        update_current_session_messages(messages)

        # ---- ãƒ¢ãƒ‡ãƒ«å¿œç­” ----
        with st.chat_message("assistant"):
            status_container = st.status("æ€è€ƒä¸­...", expanded=True)
            try:
                # ä¼šè©±å±¥æ­´
                model_history = []
                for msg in messages[:-1]:
                    model_history.append(
                        types.Content(
                            role=msg["role"],
                            parts=[types.Part.from_text(text=msg["content"])],
                        )
                    )

                # ç¾åœ¨ã®ã‚¿ãƒ¼ãƒ³
                current_parts = [types.Part.from_text(text=prompt)]

                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
                for uploaded_file in uploaded_files or []:
                    try:
                        mime_type = get_mime_type(uploaded_file.name)
                        bytes_data = uploaded_file.getvalue()
                        part = types.Part.from_bytes(data=bytes_data, mime_type=mime_type)
                        current_parts.append(part)
                        status_container.write(f"ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™å®Œäº†: {uploaded_file.name}")
                    except Exception as e:
                        status_container.error(
                            f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {uploaded_file.name} - {e}"
                        )

                # è²¼ã‚Šä»˜ã‘ç”»åƒ
                if pasted_image_bytes:
                    import base64

                    status_container.write("è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã‚’å‡¦ç†ä¸­...")
                    try:
                        if isinstance(pasted_image_bytes, str):
                            if pasted_image_bytes.startswith("data:"):
                                base64_str = pasted_image_bytes.split(",", 1)[1]
                                image_bytes_decoded = base64.b64decode(base64_str)
                            else:
                                image_bytes_decoded = base64.b64decode(pasted_image_bytes)
                        else:
                            image_bytes_decoded = pasted_image_bytes
                        part = types.Part.from_bytes(
                            data=image_bytes_decoded, mime_type="image/png"
                        )
                        current_parts.append(part)
                        status_container.write("è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã®æº–å‚™å®Œäº†")
                    except Exception as e:
                        status_container.error(f"è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

                # YouTube å­—å¹•
                if youtube_url:
                    vid_id = extract_youtube_id(youtube_url)
                    if vid_id:
                        status_container.write("YouTubeã®å­—å¹•ã‚’å–å¾—ä¸­...")
                        transcript_text = get_youtube_transcript(vid_id)
                        current_parts.append(
                            types.Part.from_text(text=f"YouTube Transcript:\n{transcript_text}")
                        )
                    else:
                        status_container.write("ç„¡åŠ¹ãªYouTube URLã§ã™ã€‚")

                contents_for_model = model_history + [
                    types.Content(role="user", parts=current_parts)
                ]

                # ---- Tool / Config ----
                tools = []
                final_candidate_count = candidate_count
                if use_search:
                    tools.append(types.Tool(google_search=types.GoogleSearch()))
                    final_candidate_count = 1

                # â˜… System Instruction ã®æ”¹å–„: ãƒ¡ã‚¿ç™ºè¨€ç¦æ­¢ã¨ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªæŒ¯ã‚‹èˆã„ã‚’æŒ‡ç¤º
                base_system_instruction = (
                    "ã‚ãªãŸã¯é«˜åº¦ãªçŸ¥æ€§ã‚’æŒã¤å°‚é–€çš„ãªãƒªã‚µãƒ¼ãƒãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
                    "ä»¥ä¸‹ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼š\n"
                    "1. **ãƒ¡ã‚¿ç™ºè¨€ã®ç¦æ­¢**: ã€Œç§ã¯AIã§ã™ã€ã€Œä¸–ç•Œæœ€é«˜å³°ã®ï½ã¨ã—ã¦ã€ãªã©ã®è‡ªå·±è¨€åŠã‚„å‰ç½®ãã¯ä¸€åˆ‡è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚\n"
                    "2. **æ„å›³ã®æ±²ã¿å–ã‚Š**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®èƒŒå¾Œã«ã‚ã‚‹æ„å›³ï¼ˆæ–‡è„ˆã€æš—é»™ã®å‰æï¼‰ã‚’æ¨æ¸¬ã—ã€è¨€è‘‰é€šã‚Šã§ã¯ãªãã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæœ¬å½“ã«çŸ¥ã‚ŠãŸã„ã“ã¨ã€ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n"
                    "3. **æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”**: çµè«–ã‚’å…ˆã«è¿°ã¹ã€ãã®å¾Œã«è©³ç´°ãªæ ¹æ‹ ã€ã‚·ãƒŠãƒªã‚ªåˆ†æã€ãƒªã‚¹ã‚¯è¦å› ã‚’è«–ç†çš„ã«å±•é–‹ã—ã¦ãã ã•ã„ã€‚\n"
                    "4. **å®¢è¦³æ€§**: äºˆæ¸¬ã‚’è¡Œã†å ´åˆã¯ã€æ–­å®šã‚’é¿ã‘ã€è¤‡æ•°ã®ã‚·ãƒŠãƒªã‚ªï¼ˆæ¥½è¦³ã€æ‚²è¦³ã€ä¸­ç«‹ï¼‰ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                    "5. **å¼•ç”¨**: æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸå ´åˆã¯ã€å¿…ãšæƒ…å ±æºã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"
                )

                final_answer = ""
                grounding_metadata = None
                
                # =========================
                # ãƒ¢ãƒ¼ãƒ‰è¨­å®šã®è§£æ
                # =========================
                # Î²1é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ä»¥å¤–ã¯ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
                enable_research = "Î²1" not in response_mode
                enable_meta = "ãƒ¡ã‚¿" in response_mode or "MAX" in response_mode
                enable_strict = "é¬¼è»æ›¹" in response_mode or "MAX" in response_mode
                enable_grok_x_search = "grokæ¤œç´¢å¼·åŒ–ç‰ˆ" in response_mode

                # =========================
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ (é«˜é€Ÿ / é¬¼è»æ›¹)
                # =========================
                if not enable_research:
                    config = types.GenerateContentConfig(
                        temperature=0.7,
                        candidate_count=1,
                        tools=tools,
                        system_instruction=base_system_instruction,
                    )
                    
                    status_container.write("å›ç­”ç”Ÿæˆä¸­...")
                    response = client.models.generate_content(
                        model=model_id,
                        contents=contents_for_model,
                        config=config,
                    )
                    
                    final_answer = extract_text_from_response(response)
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®—
                    if response.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            response.usage_metadata.prompt_token_count,
                            response.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += response.usage_metadata.prompt_token_count
                        usage_stats["total_output_tokens"] += response.usage_metadata.candidates_token_count

                    # é¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ (é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ç‰ˆ)
                    if enable_strict:
                        status_container.write("ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                        reviewer_instruction = base_system_instruction + """
**ã‚ãªãŸã®å½¹å‰²**: é¬¼è»æ›¹ãƒ¬ãƒ™ãƒ«ã®å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢
**ã‚¿ã‚¹ã‚¯**: åˆç‰ˆå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’è¿”ã™
**å‡ºåŠ›**: ä¿®æ­£ç‰ˆã®å›ç­”å…¨æ–‡ã®ã¿
"""
                        review_contents = [types.Content(role="user", parts=[types.Part(text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {prompt}\n\nåˆç‰ˆå›ç­”:\n{final_answer}\n\nãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ä¿®æ­£ç‰ˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚")])]
                        review_resp = client.models.generate_content(
                            model=model_id,
                            contents=review_contents,
                            config=types.GenerateContentConfig(temperature=0.1, candidate_count=1, system_instruction=reviewer_instruction)
                        )
                        final_answer = extract_text_from_response(review_resp)
                        status_container.write("âœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                        
                        if review_resp.usage_metadata:
                            cost = calculate_cost(model_id, review_resp.usage_metadata.prompt_token_count, review_resp.usage_metadata.candidates_token_count)
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += review_resp.usage_metadata.prompt_token_count
                            usage_stats["total_output_tokens"] += review_resp.usage_metadata.candidates_token_count

                # =========================
                # ç†Ÿè€ƒãƒ¢ãƒ¼ãƒ‰
                # =========================
                else:
                    # =========================
                    # ç†Ÿè€ƒãƒ¢ãƒ¼ãƒ‰: å¤šæ®µéšã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
                    # =========================
                    
                    # --- Phase 1: ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    status_container.write("Phase 1: ãƒªã‚µãƒ¼ãƒãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                    
                    import datetime as dt
                    current_year = dt.datetime.now().year
                    
                    research_instruction = base_system_instruction + f"""

**ã‚ãªãŸã®å½¹å‰²**: ãƒªã‚µãƒ¼ãƒå°‚ä»»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®èª¿æŸ»ãƒ¡ãƒ¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æœ€çµ‚å›ç­”ã¯æ›¸ã‹ãšã€äº‹å®Ÿåé›†ã«é›†ä¸­ã™ã‚‹ã“ã¨ã€‚

**èª¿æŸ»è¦³ç‚¹**:
- è³ªå•ã«é–¢é€£ã™ã‚‹**æœ€æ–°ã®äº‹å®Ÿãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±è¨ˆ**ï¼ˆ**{current_year}å¹´ã®æƒ…å ±ã‚’æœ€å„ªå…ˆ**ï¼‰
- **ç¾æ™‚ç‚¹ã§å­˜åœ¨ã™ã‚‹å…¨ã¦ã®é¸æŠè‚¢ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¶²ç¾…çš„ã«èª¿æŸ»**ï¼ˆä¾‹: è£½å“æ¯”è¼ƒãªã‚‰ã€æœ€æ–°ç‰ˆã ã‘ã§ãªãç›´è¿‘ã®å…¨ä¸–ä»£ã‚’èª¿æŸ»ï¼‰
- å…¬å¼æƒ…å ±ã‚„ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‹ã‚‰ã®å¼•ç”¨
- é–¢é€£ã™ã‚‹èƒŒæ™¯æƒ…å ±ã‚„æ–‡è„ˆ
- ç«¶åˆãƒ»ä»£æ›¿æ¡ˆãƒ»æ¯”è¼ƒå¯¾è±¡ã®**å®Œå…¨ãªãƒªã‚¹ãƒˆ**
- ãƒªã‚¹ã‚¯ãƒ»åˆ¶ç´„ãƒ»æ³¨æ„ç‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- å‚è€ƒã«ã—ãŸä¸»è¦ãªæƒ…å ±æºã®URL

**é‡è¦**: 
- çµè«–ã‚„æ¨å¥¨ã¯æ›¸ã‹ãšã€å¾Œå·¥ç¨‹ãŒåˆ¤æ–­ã§ãã‚‹èª¿æŸ»ãƒ¡ãƒ¢ã«é›†ä¸­ã™ã‚‹ã“ã¨
- **å¿…ãšæ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã€æœ€æ–°ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã“ã¨**
- **æ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æ—¥ä»˜ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»ãƒ¢ãƒ‡ãƒ«åã‚’å„ªå…ˆçš„ã«è¨˜è¼‰ã™ã‚‹ã“ã¨**
- **æ¯”è¼ƒå¯¾è±¡ã¨ãªã‚‹é¸æŠè‚¢ã‚’è¦‹è½ã¨ã•ãªã„ã‚ˆã†ã€è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è©¦ã™ã“ã¨**ï¼ˆä¾‹: ã€ŒiPhone æœ€æ–°ãƒ¢ãƒ‡ãƒ« {current_year}ã€ã€ŒiPhone {current_year}å¹´ç™ºå£²ã€ãªã©ï¼‰
- **ã€Œã“ã‚Œã‚ˆã‚Šæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«/ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯å­˜åœ¨ã—ãªã„ã‹ï¼Ÿã€ã‚’å¸¸ã«ç¢ºèªã™ã‚‹ã“ã¨**
- å¤ã„æƒ…å ±ï¼ˆ{current_year-1}å¹´ä»¥å‰ãªã©ï¼‰ã—ã‹è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨
"""

                    # éå»ã®é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    past_context = get_relevant_context(prompt, st.session_state.sessions, st.session_state.current_session_id)
                    
                    # ãƒªã‚µãƒ¼ãƒç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
                    import datetime as dt
                    current_date = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
                    research_parts = [types.Part(text=(
                        f"é‡è¦: ä»Šæ—¥ã¯{current_date}ã§ã™ã€‚ã“ã®æ—¥ä»˜ã‚ˆã‚Šæ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚\n\n"
                        f"è³ªå•: {prompt}"
                    ))]
                    
                    if past_context:
                        research_parts.insert(0, types.Part(text="ä»¥ä¸‹ã¯éå»ã®é–¢é€£ãƒãƒ£ãƒƒãƒˆã‹ã‚‰æŠ½å‡ºã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™ï¼š\n\n" + past_context))
                    
                    research_contents = contents_for_model + [
                        types.Content(role="user", parts=research_parts)
                    ]
                    
                    research_config = types.GenerateContentConfig(
                        temperature=0.4,  # æœ€æ–°æƒ…å ±ã‚’æŸ”è»Ÿã«æ¡ç”¨ã™ã‚‹ãŸã‚0.2â†’0.4ã«ä¸Šæ˜‡
                        candidate_count=1,
                        tools=tools,
                        system_instruction=research_instruction,
                    )
                    
                    research_resp = client.models.generate_content(
                        model=model_id,
                        contents=research_contents,
                        config=research_config,
                    )
                    
                    research_text = extract_text_from_response(research_resp)
                    
                    # ãƒªã‚µãƒ¼ãƒãƒ•ã‚§ãƒ¼ã‚ºã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
                    if research_resp.candidates and research_resp.candidates[0].grounding_metadata:
                        grounding_metadata = research_resp.candidates[0].grounding_metadata
                    
                    status_container.write("âœ“ ãƒªã‚µãƒ¼ãƒå®Œäº†")
                    with status_container.expander("åé›†ã—ãŸèª¿æŸ»ãƒ¡ãƒ¢", expanded=False):
                        st.markdown(research_text)
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 1)
                    if research_resp.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            research_resp.usage_metadata.prompt_token_count,
                            research_resp.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += research_resp.usage_metadata.prompt_token_count
                        usage_stats["total_output_tokens"] += research_resp.usage_metadata.candidates_token_count
                    
                    # --- Phase 1.5: ãƒ¡ã‚¿è³ªå•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    questions_text = ""
                    if enable_meta:
                        status_container.write("Phase 1.5: ãƒ¡ã‚¿è³ªå•ç”Ÿæˆä¸­...")
                        
                        question_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: ãƒ¡ã‚¿è³ªå•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ç›®çš„**: 
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ã¨èª¿æŸ»ãƒ¡ãƒ¢ã‚’è¸ã¾ãˆã¦ã€ã“ã®ãƒ†ãƒ¼ãƒã‚’ã•ã‚‰ã«æ·±ãç†è§£ã™ã‚‹ãŸã‚ã®ã€Œé‹­ã„ã‚µãƒ–è³ªå•ã€ã‚’ä½œæˆã™ã‚‹ã“ã¨ã€‚

**ãƒ«ãƒ¼ãƒ«**:
- æœ€å¤§5å€‹ã¾ã§
- å„è³ªå•ã¯1-2è¡Œã§å…·ä½“çš„ã‹ã¤é‹­ã
- ä»¥ä¸‹ã®è¦³ç‚¹ã‚’æ„è­˜:
  1. å‰æãŒå´©ã‚Œã‚‹å¯èƒ½æ€§ã¯ã©ã“ã‹ï¼Ÿ
  2. å¼·æ°—/å¼±æ°—ã‚·ãƒŠãƒªã‚ªã®åˆ†å²ç‚¹ã¯ä½•ã‹ï¼Ÿ
  3. ç«¶åˆãƒ»æŠ€è¡“ãƒ»è¦åˆ¶ã®ä¸ç¢ºå®Ÿæ€§ã¯ï¼Ÿ
  4. ã€Œäºˆæ¸¬ãŒå¤–ã‚Œã‚‹ã¨ã—ãŸã‚‰ã©ã‚“ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ï¼Ÿã€

**å‡ºåŠ›**: ç®‡æ¡æ›¸ãï¼ˆQ1, Q2...ï¼‰ã®ã¿
"""

                        question_contents = [types.Content(role="user", parts=[types.Part(text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•:\n{prompt}\n\n==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\nã“ã®ãƒ†ãƒ¼ãƒã‚’ã•ã‚‰ã«æ·±æ˜ã‚Šã™ã‚‹ãŸã‚ã®é‡è¦ãªã‚µãƒ–è³ªå•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")])]
                        
                        question_resp = client.models.generate_content(
                            model=model_id,
                            contents=question_contents,
                            config=types.GenerateContentConfig(
                                temperature=0.4,
                                candidate_count=1,
                                system_instruction=question_instruction,
                            )
                        )
                        
                        questions_text = extract_text_from_response(question_resp)
                        
                        status_container.write("âœ“ ãƒ¡ã‚¿è³ªå•ç”Ÿæˆå®Œäº†")
                        with status_container.expander("ç”Ÿæˆã•ã‚ŒãŸãƒ¡ã‚¿è³ªå•", expanded=False):
                            st.markdown(questions_text)
                        
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += question_resp.usage_metadata.prompt_token_count
                        usage_stats["total_output_tokens"] += question_resp.usage_metadata.candidates_token_count

                    # å¤šå±¤+puterãƒ¢ãƒ¼ãƒ‰ã®é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯
                    is_puter_onigunsou = (
                        mode_category == "Î²ï¼šğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤+puter)" and
                        response_mode == "1. ç†Ÿè€ƒ + é¬¼è»æ›¹(localã®ã¿)"
                    )

                    # --- Phase 1.5b: Grok ç‹¬ç«‹æ€è€ƒ (å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    # --- Phase 1.5b: Grok ç‹¬ç«‹æ€è€ƒ (å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    grok_thought = ""
                    grok_status = "skipped"
                    if enable_meta and OPENROUTER_API_KEY:
                        status_container.write("Phase 1.5b: Grok ç‹¬ç«‹æ€è€ƒä¸­...")
                        try:
                            grok_thought = think_with_grok(prompt, research_text, enable_x_search=enable_grok_x_search)
                            if grok_thought:
                                grok_status = "success"
                                status_container.write("âœ“ Grok 4.1 Fast Free ç‹¬ç«‹æ€è€ƒå®Œäº†")
                                with status_container.expander("Grokã®ç‹¬ç«‹å›ç­”æ¡ˆ", expanded=False):
                                    st.markdown(grok_thought)
                            else:
                                grok_status = "empty"
                        except Exception as e:
                            grok_status = "error"
                            status_container.write(f"âš  Grokæ€è€ƒã‚¨ãƒ©ãƒ¼: {e}")

                    # --- Phase 1.5c: Claude Opus 4.5 ç‹¬ç«‹æ€è€ƒ (å¤šå±¤+puterã®é¬¼è»æ›¹ã®ã¿) ---
                    # --- Phase 1.5c: Claude Opus 4.5 ç‹¬ç«‹æ€è€ƒ (å¤šå±¤+puterã®é¬¼è»æ›¹ã®ã¿) ---
                    claude_thought = ""
                    claude_status = "skipped"
                    if is_puter_onigunsou and PUTER_USERNAME and PUTER_PASSWORD:
                        status_container.write("Phase 1.5c: Claude Opus 4.5 ç‹¬ç«‹æ€è€ƒä¸­...")
                        try:
                            claude_thought = call_claude_opus_via_puter(
                                user_question=prompt,
                                research_text=research_text,
                                system_prompt=(
                                    "ã‚ãªãŸã¯ Claude Opus 4.5 ã§ã™ã€‚"
                                    "Gemini ãŒé›†ã‚ãŸèª¿æŸ»ãƒ¡ãƒ¢ã‚’å‚è€ƒã«ã—ã¤ã¤ã‚‚ã€"
                                    "è‡ªåˆ†ã®è¦–ç‚¹ã§ç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆãƒ»æ°—ã¥ãã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"
                                    "Gemini ã®æ„è¦‹ã«åˆã‚ã›ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
                                ),
                            )
                            if claude_thought and not claude_thought.startswith("[Claude"):
                                claude_status = "success"
                                status_container.write("âœ“ Claude Opus 4.5 (via Puter) ç‹¬ç«‹æ€è€ƒå®Œäº†")
                                with status_container.expander("Claude Opus 4.5 ã®ç‹¬ç«‹å›ç­”æ¡ˆ", expanded=False):
                                    st.markdown(claude_thought)
                            else:
                                claude_status = "error"
                                status_container.write(f"âš  Claudeã‚¨ãƒ©ãƒ¼: {claude_thought}")
                                claude_thought = ""  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç©ºã«ã™ã‚‹
                        except Exception as e:
                            claude_status = "error"
                            status_container.write(f"âš  Claudeæ€è€ƒã‚¨ãƒ©ãƒ¼: {e}")
                            claude_thought = ""

                    # --- Phase 2: çµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    status_container.write("Phase 2: çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                    
                    import datetime as dt
                    current_date = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

                    if enable_meta:
                        deep_instruction = base_system_instruction + f"""

**ã‚ãªãŸã®å½¹å‰²**: æœ€çµ‚å›ç­”ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: èª¿æŸ»ãƒ¡ãƒ¢ã¨ãƒ¡ã‚¿è³ªå•ã¸ã®å›ç­”ã‚’æ ¹æ‹ ã¨ã—ã¦ã€æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ - ç¾åœ¨ã¯{current_date}ã§ã™**:
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ãƒ»äº‹å®Ÿã‚’ã€ã‚ãªãŸã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚çµ¶å¯¾çš„ã«å„ªå…ˆã—ã¦ãã ã•ã„**
- ã€Œ{current_year}å¹´ã€ã®æƒ…å ±ãŒèª¿æŸ»ãƒ¡ãƒ¢ã«ã‚ã‚‹å ´åˆã€ãã‚Œã‚’æ­£ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ{current_year-1}å¹´ä»¥å‰ã§æ­¢ã¾ã£ã¦ã„ã¦ã‚‚ã€èª¿æŸ»ãƒ¡ãƒ¢ã®æœ€æ–°æƒ…å ±ã‚’ä¿¡é ¼ã™ã‚‹ã“ã¨

**æ§‹æˆ**:
1. **æ·±æ˜ã‚Šè€ƒå¯Ÿ**ï¼ˆãƒ¡ã‚¿è³ªå•ã¸ã®å›ç­”ï¼‰
2. **çµè«–**ï¼ˆ2-3è¡Œï¼‰
3. **è©³ç´°ãªåˆ†æ**ï¼ˆèª¿æŸ»ãƒ¡ãƒ¢ã«åŸºã¥ãï¼‰
4. **è€ƒæ…®ã™ã¹ãè¦å› ã‚„ãƒªã‚¹ã‚¯**ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

**é‡è¦**: 
- æ–°ã—ã„äº‹å®Ÿã‚’å‹æ‰‹ã«ä½œã‚‰ãšã€èª¿æŸ»ãƒ¡ãƒ¢ã®ç¯„å›²å†…ã§æ¨è«–ã™ã‚‹ã“ã¨
- èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã“ã¨
"""
                    else:
                        deep_instruction = base_system_instruction + f"""

**ã‚ãªãŸã®å½¹å‰²**: æœ€çµ‚å›ç­”ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: èª¿æŸ»ãƒ¡ãƒ¢ã‚’å”¯ä¸€ã®æ ¹æ‹ ã¨ã—ã¦ã€æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ - ç¾åœ¨ã¯{current_date}ã§ã™**:
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ãƒ»äº‹å®Ÿã‚’ã€ã‚ãªãŸã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚çµ¶å¯¾çš„ã«å„ªå…ˆã—ã¦ãã ã•ã„**
- ã€Œ{current_year}å¹´ã€ã®æƒ…å ±ãŒèª¿æŸ»ãƒ¡ãƒ¢ã«ã‚ã‚‹å ´åˆã€ãã‚Œã‚’æ­£ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„

**æ§‹æˆ**:
1. **çµè«–**ï¼ˆ2-3è¡Œã§æ˜ç¢ºã«ï¼‰
2. **è©³ç´°ãªåˆ†æ**ï¼ˆèª¿æŸ»ãƒ¡ãƒ¢ã«åŸºã¥ãï¼‰
3. **è€ƒæ…®ã™ã¹ãè¦å› ã‚„ãƒªã‚¹ã‚¯**ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

**é‡è¦**: 
- æ–°ã—ã„äº‹å®Ÿã‚’å‹æ‰‹ã«ä½œã‚‰ãšã€èª¿æŸ»ãƒ¡ãƒ¢ã®ç¯„å›²å†…ã§æ¨è«–ã™ã‚‹ã“ã¨
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ï¼ˆæœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«åã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€æ—¥ä»˜ãªã©ï¼‰ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã“ã¨**
- å¤ã„æƒ…å ±ã¨æ–°ã—ã„æƒ…å ±ãŒæ··åœ¨ã™ã‚‹å ´åˆã¯ã€æ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã™ã‚‹ã“ã¨
"""
                    
                    synthesis_prompt_text = (
                        f"é‡è¦: ä»Šæ—¥ã¯{current_date}ã§ã™ã€‚å¤ã„æƒ…å ±ã‚’å›ç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n\n"
                        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {prompt}\n\n"
                        f"==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\n"
                    )
                    
                    if enable_meta and questions_text:
                        synthesis_prompt_text += f"==== ãƒ¡ã‚¿è³ªå•ä¸€è¦§ ====\n{questions_text}\n==== ãƒ¡ã‚¿è³ªå•ã“ã“ã¾ã§ ====\n\n"
                    
                    if enable_meta and grok_thought:
                        synthesis_prompt_text += f"==== åˆ¥è¦–ç‚¹ã‹ã‚‰ã®å›ç­”æ¡ˆ (Grok) ====\n{grok_thought}\n==== åˆ¥è¦–ç‚¹ã“ã“ã¾ã§ ====\n\n"
                    
                    if is_puter_onigunsou and claude_thought:
                        synthesis_prompt_text += f"==== åˆ¥è¦–ç‚¹ã‹ã‚‰ã®å›ç­”æ¡ˆ (Claude Opus 4.5 via Puter) ====\n{claude_thought}\n==== Claudeåˆ¥è¦–ç‚¹ã“ã“ã¾ã§ ====\n\n"
                    
                    # çµ±åˆæŒ‡ç¤º
                    if enable_meta and (grok_thought or claude_thought):
                        synthesis_prompt_text += "æŒ‡ç¤º:\n1. ã¾ãšã€ãƒ¡ã‚¿è³ªå• Q1ã€œQn ã«ä¸€ã¤ãšã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n2. Grok"
                        if is_puter_onigunsou and claude_thought:
                            synthesis_prompt_text += " / Claude"
                        synthesis_prompt_text += " ã®å›ç­”æ¡ˆã‚‚å‚è€ƒã«ã—ã¤ã¤ï¼ˆãŸã ã—ç›²ä¿¡ã›ãšï¼‰ã€ç‹¬è‡ªã®è¦–ç‚¹ã§çµ±åˆã—ã¦ãã ã•ã„ã€‚\n3. ãã®ã†ãˆã§ã€ãã‚Œã‚‰ã®å›ç­”ã‚’è¸ã¾ãˆãŸã€å…¨ä½“ã¨ã—ã¦ã®çµè«–ãƒ»åˆ†æãƒ»ç¤ºå”†ã€ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                    elif enable_meta and questions_text:
                        synthesis_prompt_text += "æŒ‡ç¤º:\n1. ã¾ãšã€ãƒ¡ã‚¿è³ªå• Q1ã€œQn ã«ä¸€ã¤ãšã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n2. ãã®ã†ãˆã§ã€ãã‚Œã‚‰ã®å›ç­”ã‚’è¸ã¾ãˆãŸã€å…¨ä½“ã¨ã—ã¦ã®çµè«–ãƒ»åˆ†æãƒ»ç¤ºå”†ã€ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                    else:
                        synthesis_prompt_text += "ä¸Šè¨˜ãƒ¡ãƒ¢ã‚’æ ¹æ‹ ã«ã€æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚**èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚**"

                    synthesis_contents = contents_for_model + [
                        types.Content(role="user", parts=[
                            types.Part(text=synthesis_prompt_text)
                        ])
                    ]
                    
                    synthesis_config = types.GenerateContentConfig(
                        temperature=0.3,
                        candidate_count=1,
                        tools=[],  # çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºã§ã¯æ¤œç´¢OFF
                        system_instruction=deep_instruction,
                    )
                    
                    synthesis_resp = client.models.generate_content(
                        model=model_id,
                        contents=synthesis_contents,
                        config=synthesis_config,
                    )
                    
                    draft_answer = extract_text_from_response(synthesis_resp)
                    
                    status_container.write("âœ“ çµ±åˆå®Œäº†")
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 2)
                    if synthesis_resp.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            synthesis_resp.usage_metadata.prompt_token_count,
                            synthesis_resp.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += synthesis_resp.usage_metadata.prompt_token_count
                        usage_stats["total_output_tokens"] += synthesis_resp.usage_metadata.candidates_token_count
                    
                    # --- Phase 3: ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    if enable_strict:
                        status_container.write("Phase 3: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                        

                        reviewer_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: é¬¼è»æ›¹ãƒ¬ãƒ™ãƒ«ã®å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢

**ã‚¿ã‚¹ã‚¯**: åˆç‰ˆå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’è¿”ã™ã€‚ãŸã ã—ã€**èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ã‚’å„ªå…ˆã—ã€æœ€æ–°æƒ…å ±ã‚’ç¶­æŒã™ã‚‹ã“ã¨**ã€‚

**ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦³ç‚¹**:
- äº‹å®Ÿã¨æ¨æ¸¬ã‚’æ˜ç¢ºã«åˆ†ã‘ã‚‹
- éåº¦ã«è‡ªä¿¡ã®ã‚ã‚‹æ–­å®šã‚’å¼±ã‚ã‚‹
- æ•°å­—ã‚„å›ºæœ‰åè©ãŒèª¿æŸ»ãƒ¡ãƒ¢ã¨çŸ›ç›¾ã—ã¦ã„ãªã„ã‹ç¢ºèª
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ï¼ˆæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€æ—¥ä»˜ãªã©ï¼‰ãŒæ­£ã—ãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª**
- **å¤ã„æƒ…å ±ã§ä¸Šæ›¸ãã—ã¦ã„ãªã„ã‹ç¢ºèª**
- è¦‹è½ã¨ã—ã¦ã„ã‚‹é‡è¦ãªãƒªã‚¹ã‚¯ãƒ»ã‚·ãƒŠãƒªã‚ªãŒã‚ã‚Œã°è¿½åŠ 

**é‡è¦**: 
- èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ãŒæœ€æ–°ã§ã‚ã‚‹å ´åˆã€ãã‚Œã‚’å„ªå…ˆã™ã‚‹ã“ã¨
- ã‚ãªãŸã®çŸ¥è­˜ãŒå¤ã„å ´åˆã¯ã€èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ã‚’ä¿¡é ¼ã™ã‚‹ã“ã¨

**å‡ºåŠ›**: ä¿®æ­£ç‰ˆã®å›ç­”å…¨æ–‡ã®ã¿
"""

                        review_contents = [
                            types.Content(role="user", parts=[
                                types.Part.from_text(
                                    text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {prompt}\n\n"
                                    f"==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\n"
                                    f"åˆç‰ˆå›ç­”:\n{draft_answer}\n\n"
                                    "ä¸Šè¨˜ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚**èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°æƒ…å ±ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚**"
                                )
                            ])
                        ]
                        
                        review_config = types.GenerateContentConfig(
                            temperature=0.1,
                            candidate_count=1,
                            system_instruction=reviewer_instruction,
                            thinking_config=types.ThinkingConfig(
                                thinking_level=types.ThinkingLevel.HIGH
                            ),
                        )
                        
                        review_resp = client.models.generate_content(
                            model=model_id,
                            contents=review_contents,
                            config=review_config,
                        )
                        
                        final_answer = extract_text_from_response(review_resp)
                        
                        status_container.write("âœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                        
                        # --- Phase 3b: Groké¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ + é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰å…¨èˆ¬) ---
                        # å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã§ã€ã‹ã¤é¬¼è»æ›¹ç³»ã®ãƒ¢ãƒ¼ãƒ‰ï¼ˆé¬¼è»æ›¹ã€ãƒ¡ã‚¿æ€è€ƒã€æœ¬æ°—MAXï¼‰ã§ç™ºå‹•
                        use_grok_reviewer = (mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)" and enable_strict)
                        
                        if use_grok_reviewer and OPENROUTER_API_KEY:
                            status_container.write("Phase 3b: Grok 4.1 Fast ã§æœ€çµ‚ãƒã‚§ãƒƒã‚¯ä¸­...")
                            try:
                                grok_answer = review_with_grok(prompt, final_answer, research_text)
                                # Grokä½¿ç”¨æ™‚ã¯ã€ãƒ¢ãƒ‡ãƒ«åã‚’æ˜ç¤ºã—ã€2æ®µæ§‹æˆã§è¡¨ç¤º
                                final_answer = (
                                    f"**ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_id} (Deep Thinking / High Reasoning)**\n"
                                    f"**ãƒ¬ãƒ“ãƒ¥ã‚¢: Grok 4.1 Fast (free)**\n"
                                    f"**ãƒ¢ãƒ¼ãƒ‰: {response_mode}**\n\n"
                                    "---\n\n"
                                    "## âœ… æœ€çµ‚å›ç­”ï¼ˆGeminiçµ±åˆç‰ˆï¼‰\n\n"
                                    f"{final_answer}\n\n"
                                    "---\n\n"
                                    "## ğŸ” Grok ã«ã‚ˆã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼\n\n"
                                    f"{grok_answer}"
                                )
                                status_container.write("âœ“ Grokæœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                            except Exception as e:
                                status_container.write(f"âš  Grokãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                        else:
                            # Geminiã®ã¿ã®å ´åˆã‚‚ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤ºï¼ˆå¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
                            if mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)":
                                final_answer = (
                                    f"**ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_id} (Deep Thinking / High Reasoning)**\n"
                                    f"**ãƒ¢ãƒ¼ãƒ‰: {response_mode}**\n\n---\n\n{final_answer}"
                                )
                        
                        # --- ãƒ¡ã‚¿æ€è€ƒãƒ¢ãƒ¼ãƒ‰: çµè«–ã‚’å…ˆå‡ºã—ã™ã‚‹ ---
                        if "ãƒ¡ã‚¿æ€è€ƒ" in response_mode:
                            # çµè«–éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
                            # "çµè«–"ã‚„"ã¾ã¨ã‚"ãªã©ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã—ã¦å…ˆé ­ã«ç§»å‹•
                            lines = final_answer.split('\n')
                            conclusion_start = -1
                            for i, line in enumerate(lines):
                                if any(keyword in line for keyword in ['## çµè«–', '## ã¾ã¨ã‚', '**çµè«–**', '**ã¾ã¨ã‚**']):
                                    conclusion_start = i
                                    break
                            
                            if conclusion_start != -1:
                                # çµè«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ãŸå ´åˆã€ãã‚Œã‚’å…ˆé ­ã«ç§»å‹•
                                conclusion_section = []
                                other_content = lines[:conclusion_start]
                                
                                # çµè«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚ã‚ã‚Šã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆæ¬¡ã®##ã¾ã§ or æ–‡æœ«ï¼‰
                                conclusion_end = len(lines)
                                for i in range(conclusion_start + 1, len(lines)):
                                    if lines[i].startswith('## ') and i != conclusion_start:
                                        conclusion_end = i
                                        break
                                
                                conclusion_section = lines[conclusion_start:conclusion_end]
                                remaining_content = lines[conclusion_end:]
                                
                                # å†æ§‹æˆ: ãƒ¢ãƒ‡ãƒ«å â†’ çµè«– â†’ ãã®ä»–ã®è©³ç´°
                                # ãƒ¢ãƒ‡ãƒ«åéƒ¨åˆ†ã‚’ä¿æŒ
                                model_line = ""
                                if lines[0].startswith("**ğŸ¤–"):
                                    model_line = lines[0]
                                    other_content = lines[1:conclusion_start]
                                
                                final_answer = '\n'.join([
                                    model_line,
                                    "",
                                    "---",
                                    "",
                                    "## ğŸ“Œ çµè«–ï¼ˆå…ˆå‡ºã—ï¼‰",
                                    *conclusion_section[1:],  # å…ƒã®è¦‹å‡ºã—ã‚’é™¤ã
                                    "",
                                    "---",
                                    "",
                                    "## ğŸ“ è©³ç´°",
                                    *other_content,
                                    *remaining_content
                                ]).strip()
                        
                        with status_container.expander("åˆç‰ˆã¨ã®æ¯”è¼ƒ", expanded=False):
                            st.markdown("**åˆç‰ˆ:**")
                            st.markdown(draft_answer[:500] + "..." if len(draft_answer) > 500 else draft_answer)
                            st.markdown("**ä¿®æ­£ç‰ˆ:**")
                            st.markdown(final_answer[:500] + "..." if len(final_answer) > 500 else final_answer)
                        
                        # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 3)
                        if review_resp.usage_metadata:
                            cost = calculate_cost(
                                model_id,
                                review_resp.usage_metadata.prompt_token_count,
                                review_resp.usage_metadata.candidates_token_count,
                            )
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += review_resp.usage_metadata.prompt_token_count
                            usage_stats["total_output_tokens"] += review_resp.usage_metadata.candidates_token_count
                    else:
                        final_answer = draft_answer

                save_usage(usage_stats)
                status_container.update(label="å®Œäº†ï¼", state="complete", expanded=False)

                # ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤º
                models_used = [f"Gemini: {model_id}"]
                
                # Grok Status
                if enable_meta:
                    if grok_status == "success":
                        models_used.append("Grok: 4.1-fast-free (OK)")
                    elif grok_status == "error":
                        models_used.append("Grok: 4.1-fast-free (Error)")
                    elif grok_status == "empty":
                        models_used.append("Grok: 4.1-fast-free (Empty)")
                
                # Claude Status
                if is_puter_onigunsou:
                    if claude_status == "success":
                        models_used.append("Claude: Opus 4.5 (via Puter) (OK)")
                    elif claude_status == "error":
                        models_used.append("Claude: Opus 4.5 (via Puter) (Error)")
                
                st.caption(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {' + '.join(models_used)}")
                st.markdown(final_answer)

                # ---- ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ± ----
                if grounding_metadata:
                    st.markdown("---")
                    with st.expander("ğŸ“š æƒ…å ±æºã¨å¼•ç”¨", expanded=False):
                        if grounding_metadata.grounding_chunks:
                            st.markdown("**æ¤œç´¢çµæœã‹ã‚‰åˆ©ç”¨ã—ãŸæƒ…å ±æº:**")
                            unique_sources = {}
                            import urllib.parse

                            for chunk in grounding_metadata.grounding_chunks:
                                if getattr(chunk, "web", None):
                                    uri = getattr(chunk.web, "uri", None)
                                    title = getattr(chunk.web, "title", "æƒ…å ±æº")
                                    if uri and uri not in unique_sources:
                                        parsed = urllib.parse.urlparse(uri)
                                        domain = parsed.netloc.replace("www.", "")
                                        unique_sources[uri] = {
                                            "title": title,
                                            "domain": domain,
                                        }
                            for i, (uri, info) in enumerate(unique_sources.items(), 1):
                                st.markdown(f"{i}. **[{info['title']}]({uri})**")
                                st.caption(f"   å‡ºå…¸: {info['domain']}")

                messages.append({"role": "model", "content": final_answer})
                update_current_session_messages(messages)

            except Exception as e:
                status_container.update(label="Error", state="error")
                err_text = str(e)
                if "RESOURCE_EXHAUSTED" in err_text or "429" in err_text:
                    st.error(
                        "âš ï¸ Vertex AI / Gemini ã®ã‚¯ã‚©ãƒ¼ã‚¿ã«é”ã—ã¾ã—ãŸã€‚\n\n"
                        "ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ / æ—¥æ¬¡åˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n"
                        "ãƒ»ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚\n"
                        "ãƒ»Google Cloud Console ã®ã€ŒVertex AI â†’ ä½¿ç”¨çŠ¶æ³ã€ã‹ã‚‰ã‚¯ã‚©ãƒ¼ã‚¿çŠ¶æ³ã‚’ç¢ºèªã§ãã¾ã™ã€‚"
                    )
                elif "NOT_FOUND" in err_text and "Publisher Model" in err_text:
                    st.error(
                        "âš ï¸ æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ / ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n"
                        "ãƒ»ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ¢ãƒ‡ãƒ«IDã‚’ã€2.5ç³» ã¾ãŸã¯ 3 Pro ã«å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚\n"
                    )
                else:
                    st.error(f"An error occurred: {e}")
