import os
import uuid
import datetime
import streamlit as st
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
import io
from logic import (
    USAGE_FILE, SESSIONS_FILE, MAX_BUDGET_USD, PRICING,
    VERTEX_PROJECT, VERTEX_LOCATION,
    load_usage, save_usage, calculate_cost, get_mime_type,
    extract_youtube_id, get_youtube_transcript, get_relevant_context,
    extract_text_from_response, load_sessions, save_sessions, get_client
)

try:
    from st_img_pastebutton import paste
    import_error_msg = None
except ImportError as e:
    paste = None
    import_error_msg = str(e)

# =========================
# ç’°å¢ƒå¤‰æ•° & å®šæ•°
# =========================

load_dotenv()



st.set_page_config(page_title="Gemini 3 Web Studio", layout="wide")

# ğŸ” ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ­ãƒƒã‚¯ + URLãƒˆãƒ¼ã‚¯ãƒ³æ°¸ç¶šåŒ–
# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
try:
    if "APP_PASSWORD" in st.secrets:
        APP_PASSWORD = st.secrets["APP_PASSWORD"]
        SECRET_TOKEN = st.secrets.get("SECRET_TOKEN", "access_granted_default")
    else:
        APP_PASSWORD = os.getenv("APP_PASSWORD", "198501")  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé–‹ç™ºç”¨ï¼‰
        SECRET_TOKEN = os.getenv("SECRET_TOKEN", "access_granted_198501")
except:
    APP_PASSWORD = os.getenv("APP_PASSWORD", "198501")
    SECRET_TOKEN = os.getenv("SECRET_TOKEN", "access_granted_198501")

# 1. URLãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
query_params = st.query_params
url_token = query_params.get("auth", None)

if url_token == SECRET_TOKEN:
    st.session_state.authenticated = True
elif "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# 2. æœªèªè¨¼ãªã‚‰ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ç”»é¢
if not st.session_state.authenticated:
    st.title("Gemini 3 Studio")
    st.write("ã“ã®ã‚¢ãƒ—ãƒªã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™ã€‚")

    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")

    if st.button("ãƒ­ã‚°ã‚¤ãƒ³"):
        if password == APP_PASSWORD:
            st.session_state.authenticated = True
            # URLã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä»˜ä¸ã—ã¦ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã§ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯å¯èƒ½ã«ãªã‚‹ï¼‰
            st.query_params["auth"] = SECRET_TOKEN
            st.rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™ã€‚")

    st.stop()

# =========================
# Helper Functions
# =========================

# =========================
# Helper Functions
# =========================
# Moved to logic.py

# =========================
# Grok Review Function (OpenRouter API)
# =========================

import requests
from curl_cffi import requests as crequests  # Puterç”¨

# OpenRouter API Keyã®å–å¾— (st.secretså„ªå…ˆã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°)
try:
    if "OPENROUTER_API_KEY" in st.secrets:
        OPENROUTER_API_KEY = st.secrets["OPENROUTER_API_KEY"]
    else:
        OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")
except:
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")

# Puterèªè¨¼æƒ…å ±ã¯å‰Šé™¤ï¼ˆAWS Bedrockã«ç§»è¡Œï¼‰


# â–¼â–¼â–¼ AWS Bedrock (Claude 4.5 Sonnetç”¨) â–¼â–¼â–¼
try:
    import boto3
    HAS_BOTO3 = True
except ImportError:
    HAS_BOTO3 = False

# AWSèªè¨¼æƒ…å ±å–å¾—
try:
    if "AWS_ACCESS_KEY_ID" in st.secrets:
        AWS_ACCESS_KEY_ID = st.secrets["AWS_ACCESS_KEY_ID"]
        AWS_SECRET_ACCESS_KEY = st.secrets["AWS_SECRET_ACCESS_KEY"]
    else:
        AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID", "")
        AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY", "")
except:
    AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID", "")
    AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY", "")

# Claude 4.5 Sonnet ã® inference profile ID
CLAUDE_MODEL_ID = "us.anthropic.claude-sonnet-4-5-20250929-v1:0"
CLAUDE_REGION = "us-east-1"
# â–²â–²â–² è¿½åŠ ã“ã“ã¾ã§ â–²â–²â–²

# â–¼â–¼â–¼ GitHub Models (o4-miniç”¨) â–¼â–¼â–¼
try:
    if "GITHUB_TOKEN" in st.secrets:
        GITHUB_TOKEN = st.secrets["GITHUB_TOKEN"]
    else:
        GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
except:
    GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")

GITHUB_MODEL_ID = "o4-mini"
# â–²â–²â–² GitHub Models ã“ã“ã¾ã§ â–²â–²â–²


# =========================
# Session Management
# =========================

def compact_newlines(text: str) -> str:
    """
    3è¡Œä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2è¡Œï¼ˆç©ºè¡Œ1ã¤ï¼‰ã«åœ§ç¸®ã™ã‚‹
    """
    import re
    return re.sub(r"\n{3,}", "\n\n", text)

def think_with_grok(user_question: str, research_text: str, enable_x_search: bool = False, mode: str = "default") -> str:
    """
    Grok 4.1 Fast Free ã‚’ä½¿ã£ã¦ã€ãƒªã‚µãƒ¼ãƒãƒ¡ãƒ¢ã‚’å…ƒã«ç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã™ã‚‹
    enable_x_search=True ã®å ´åˆã€X/Twitteræƒ…å ±ã®æ´»ç”¨ã‚’ä¿ƒã™
    mode="full_max" ã®å ´åˆã€ç‹¬ç«‹ã—ãŸãƒªãƒ¼ãƒ‰ç ”ç©¶è€…ã¨ã—ã¦æŒ¯ã‚‹èˆã†
    """
    if not OPENROUTER_API_KEY:
        return "OpenRouter API Key is missing."

    # Xæ¤œç´¢å¼·åŒ–ç‰ˆã®å ´åˆã€ç‰¹åˆ¥ãªæŒ‡ç¤ºã‚’è¿½åŠ 
    x_search_instruction = ""
    if enable_x_search:
        x_search_instruction = (
            "\n\n**é‡è¦**: ã‚ãªãŸã¯Grokã¨ã—ã¦Xï¼ˆTwitterï¼‰ã®æƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚\n"
            "ä¸Šè¨˜ã®èª¿æŸ»ãƒ¡ãƒ¢ã«åŠ ãˆã¦ã€Xä¸Šã®æœ€æ–°ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»è­°è«–ãƒ»åå¿œã‚’è€ƒæ…®ã—ã€\n"
            "ãã‚Œã‚‰ã‚’å«ã‚ãŸç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n"
            "æ³¨æ„: Xä¸Šã®æƒ…å ±ãŒç¢ºèªã§ããªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ­£ç›´ã«è¿°ã¹ã¦ãã ã•ã„ã€‚\n"
            "æ¶ç©ºã®æŠ•ç¨¿ã‚„å­˜åœ¨ã—ãªã„åå¿œã‚’ä½œæˆã—ãªã„ã“ã¨ã€‚"
        )
    
    if mode == "full_max":
        user_content = (
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
            f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
            "æŒ‡ç¤º:\n"
            "ã‚ãªãŸã¯ Gemini ã¨ã¯ç‹¬ç«‹ã—ãŸç«‹å ´ã®ãƒªãƒ¼ãƒ‰ç ”ç©¶è€…ã§ã™ã€‚\n"
            "Gemini ã«é æ…®ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚èª¿æŸ»ãƒ¡ãƒ¢ã®äº‹å®Ÿã‚’æœ€å„ªå…ˆã—ã¤ã¤ã€\n"
            "ç‰¹ã«ã€è¦‹è½ã¨ã•ã‚ŒãŒã¡ãªãƒªã‚¹ã‚¯ãƒ»åå¯¾æ„è¦‹ãƒ»å‰æã®ç©´ã€ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚\n"
            "1) ã‚ãªãŸãªã‚Šã®çµè«–ï¼ˆçŸ­ãï¼‰\n"
            "2) Gemini ãŒå–ã‚Šãã†ãªçµè«–ã¨ã®é•ã„\n"
            "3) è¿½åŠ ã§è€ƒæ…®ã™ã¹ããƒªã‚¹ã‚¯ã‚„æ¡ä»¶\n"
            f"{x_search_instruction}"
        )
    else:
        user_content = (
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
            f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
            "æŒ‡ç¤º:\n"
            "ã‚ãªãŸã¯Geminiã¨ã¯åˆ¥ã®ç‹¬ç«‹ã—ãŸAIã§ã™ã€‚\n"
            f"{x_search_instruction}\n"
            "èª¿æŸ»ãƒ¡ãƒ¢ã‚’å…ƒã«ã€ã‚ãªãŸè‡ªèº«ã®è¦–ç‚¹ã§å›ç­”æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
            "Geminiã®æ„è¦‹ã«åˆã‚ã›ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
            "ç‰¹ã«ã€èª¿æŸ»ãƒ¡ãƒ¢ã®ä¸­ã§é‡è¦ã ã¨æ€ã†äº‹å®Ÿã‚„ã€åˆ¥ã®è¦–ç‚¹ãŒã‚ã‚Œã°å¼·èª¿ã—ã¦ãã ã•ã„ã€‚"
        )

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://gemini-app.streamlit.app/", 
        "X-Title": "Gemini Web Studio",
    }
    
    data = {
        "model": "x-ai/grok-4.1-fast:free",  # Grok 4.1 Fast (free)
        "messages": [
            {"role": "user", "content": user_content}
        ],
        "temperature": 0.7,
        "max_tokens": 2000,
    }

    try:
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Error calling Grok: {e}"

def review_with_grok(user_question: str, gemini_answer: str, research_text: str, mode: str = "normal") -> str:
    """
    Grok 4.1 Fast Free ã‚’ä½¿ã£ã¦ã€Geminiã®æœ€çµ‚å›ç­”ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹
    mode="onigunsou": å³æ ¼ãªæ¤œå¯Ÿå®˜ã¨ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼
    mode="full_max": ãƒ€ãƒ–ãƒ«é¬¼è»æ›¹ã¨ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼
    """
    if not OPENROUTER_API_KEY:
        return "OpenRouter API Key is missing."

    # å…±é€š: Grokã®å½¹å‰²ã‚’ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆå°‚ç”¨ã€ã«å³ã—ãåˆ¶é™
    system_content = (
        "ã‚ãªãŸã¯Geminiã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ã§ã™ã€‚\n"
        "ã€é‡è¦ãªåˆ¶ç´„ã€‘\n"
        "ãƒ»Geminiã®å›ç­”ãã®ã‚‚ã®ã‚’æ›¸ãç›´ã—ãŸã‚Šã€ç‹¬è‡ªã®æœ€çµ‚å›ç­”ã‚’å…¨æ–‡å‡ºã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
        "ãƒ»ã‚ãªãŸã®å½¹å‰²ã¯ã€å±é™ºãªç®‡æ‰€ãƒ»æ›–æ˜§ãªç®‡æ‰€ãƒ»å¼±ã‚ã‚‹ã¹ãè¡¨ç¾ã€ã‚’æŒ‡æ‘˜ã™ã‚‹ã“ã¨ã ã‘ã§ã™ã€‚\n"
        "ãƒ»2024å¹´11æœˆã¾ã§ã®çŸ¥è­˜ã—ã‹æŒã£ã¦ã„ãªã„ãŸã‚ã€ãã‚Œä»¥é™ã®API/ãƒ¢ãƒ‡ãƒ«ã®å®Ÿåœ¨ã‚’å¦å®šã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
        "ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¸¡ã—ãŸèª¿æŸ»ãƒ¡ãƒ¢ã‚„ã‚³ãƒ¼ãƒ‰ã®äº‹å®Ÿã‚’ã€ã‚ãªãŸè‡ªèº«ã®çŸ¥è­˜ã‚ˆã‚Šã‚‚å„ªå…ˆã—ã¾ã™ã€‚\n"
        "ãƒ»Webã‚µã‚¤ãƒˆã‚’ã€ä»Šè¦‹ãŸã€ã‹ã®ã‚ˆã†ãªè¡¨ç¾ï¼ˆä¾‹:ã€å…¬å¼ã‚µã‚¤ãƒˆã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€œã€ï¼‰ã‚’ä½¿ã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
        "ãƒ»Markdownã¯ä½¿ç”¨ã—ã¦ã‚ˆã„ã§ã™ãŒã€é€£ç¶šã™ã‚‹ç©ºè¡Œã¯1è¡Œã¾ã§ã«ã—ã¦ãã ã•ã„ã€‚\n"
    )
    
    if mode == "onigunsou":
        system_content += (
            "\nâš ï¸ é‡è¦ãªæ³¨æ„:\n"
            "ãƒ»ã‚ãªãŸã®çŸ¥è­˜ã¯2024å¹´11æœˆã¾ã§ã§ã™\n"
            "ãƒ»2024å¹´12æœˆä»¥é™ã®æƒ…å ±ã¯ã€ŒçŸ¥è­˜ç¯„å›²å¤–ã€ã¨æ˜è¨˜ã—ã€ã€Œå­˜åœ¨ã—ãªã„ã€ã¨æ–­å®šã—ãªã„ã§ãã ã•ã„\n"
            "ãƒ»æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã«ã¤ã„ã¦ã¯ã€Œç¢ºèªãŒå¿…è¦ã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„"
        )
        instruction = (
            "ä»¥ä¸‹ã®å½¢å¼ã§ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
            "## è©•ä¾¡æ¦‚è¦\n"
            "- å›ç­”ã¯ OK / è¦ä¿®æ­£ / å±é™º ã®ã„ãšã‚Œã‹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚\n\n"
            "## å•é¡Œç‚¹\n"
            "- ç®‡æ¡æ›¸ãã§ã€å±é™ºãªèª¤ã‚Šãƒ»éåº¦ãªæ–­å®šãƒ»è«–ç†ã®é£›èºãªã©ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n\n"
            "## ä¿®æ­£ã®ãƒã‚¤ãƒ³ãƒˆ\n"
            "- ã©ã®éƒ¨åˆ†ã‚’ã©ã†å¼±ã‚ã‚‹ï¼æ›¸ãæ›ãˆã‚‹ã¹ãã‹ã ã‘ã‚’ç°¡æ½”ã«ç¤ºã—ã¦ãã ã•ã„ã€‚\n\n"
            "â€» Geminiã®å›ç­”å…¨æ–‡ã‚’æ›¸ãç›´ã—ãŸã‚Šã€ç‹¬è‡ªã®æœ€çµ‚å›ç­”ã‚’ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚"
        )
    elif mode == "full_max":
        system_content += (
            "\nâš ï¸ é‡è¦ãªæ³¨æ„:\n"
            "ãƒ»ã‚ãªãŸã®çŸ¥è­˜ã¯2024å¹´11æœˆã¾ã§ã§ã™\n"
            "ãƒ»2024å¹´12æœˆä»¥é™ã®æœ€æ–°æƒ…å ±ï¼ˆæ–°ãƒ¢ãƒ‡ãƒ«ã€æ–°ã‚µãƒ¼ãƒ“ã‚¹ç­‰ï¼‰ã¯çŸ¥è­˜ç¯„å›²å¤–ã§ã™\n"
            "ãƒ»æœ€æ–°æƒ…å ±ã‚’ã€Œæ¶ç©ºã€ã€Œéå­˜åœ¨ã€ã¨æ–­å®šã›ãšã€ã€Œ2024å¹´11æœˆæ™‚ç‚¹ã§ç¢ºèªã§ããšã€æœ€æ–°æƒ…å ±ã®å¯èƒ½æ€§ã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„\n"
            "ãƒ»ãƒªãƒ³ã‚¯å…ˆç¢ºèªã‚’æ±‚ã‚ã‚‹å ´åˆã‚‚ã€ã€Œå­˜åœ¨ã—ãªã„ã€ã§ã¯ãªãã€Œç¢ºèªæ¨å¥¨ã€ã¨è¡¨ç¾ã—ã¦ãã ã•ã„"
        )
        instruction = (
            "ä»¥ä¸‹ã®å½¢å¼ã§ã€å³ã—ã‚ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
            "## Grokè©•ä¾¡æ¦‚è¦\n"
            "- OK / è¦ä¿®æ­£ / å±é™º ã®ã„ãšã‚Œã‹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚\n\n"
            "## é‡å¤§ãªå•é¡Œç‚¹\n"
            "- ç®‡æ¡æ›¸ãã§ã€ç‰¹ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’èª¤èª˜å°ã—ãã†ãªç‚¹ã ã‘æŒ™ã’ã¦ãã ã•ã„ã€‚\n\n"
            "## æ”¹å–„ã®ãƒ’ãƒ³ãƒˆ\n"
            "- ã©ã®è«–ç‚¹ã‚’å¼±ã‚ãŸã‚Šã€è¿½åŠ ã§æ³¨æ„æ›¸ãã™ã¹ãã‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n\n"
            "â€» Geminiã®å›ç­”å…¨æ–‡ã‚’æ›¸ãç›´ã—ãŸã‚Šã€ç‹¬è‡ªã®æœ€çµ‚å›ç­”ã‚’ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚"
        )
    else:
        instruction = (
            "ä»¥ä¸‹ã®Geminiã®å›ç­”ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€è«–ç†çš„ãªèª¤ã‚Šã‚„ä¸è¶³ã—ã¦ã„ã‚‹è¦–ç‚¹ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚\n"
            "ã¾ãŸã€ã‚ˆã‚Šè‰¯ã„å›ç­”ã«ã™ã‚‹ãŸã‚ã®æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"
        )

    user_content = (
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
        f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
        f"Geminiã®å›ç­”:\n{gemini_answer}\n\n"
        f"æŒ‡ç¤º:\n{instruction}"
    )

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://gemini-app.streamlit.app/", 
        "X-Title": "Gemini Web Studio",
    }
    
    data = {
        "model": "x-ai/grok-4.1-fast:free",  # Grok 4.1 Fast (free)
        "messages": [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content}
        ],
        "temperature": 0.5, # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãªã®ã§å°‘ã—æŠ‘ãˆã‚
        "max_tokens": 2000,
    }
    try:
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Error calling Grok: {e}"


def think_with_claude45_bedrock(user_question: str, research_text: str) -> tuple[str, dict]:
    """
    AWS Bedrock çµŒç”±ã§ Claude Sonnet 4.5 ã‚’ä½¿ã£ã¦ç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã™ã‚‹
    Returns: (å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ, usageè¾æ›¸)
    """
    if not HAS_BOTO3:
        return ("Error: boto3 library not installed. (pip install boto3)", {})
    if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:
        return ("Error: AWS credentials are missing.", {})

    # Claude 4.5 ã¸ã®å½¹å‰²ä»˜ä¸: è«–ç†çš„æ¨è«–ã¨ãƒªã‚¹ã‚¯æŒ‡æ‘˜ã«ç‰¹åŒ–
    system_prompt = (
        "ã‚ãªãŸã¯Geminiã¨ã¯ç•°ãªã‚‹ç‹¬ç«‹ã—ãŸAIã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚\n"
        "æä¾›ã•ã‚ŒãŸèª¿æŸ»ãƒ¡ãƒ¢ã‚’äº‹å®Ÿã®ãƒ™ãƒ¼ã‚¹ã¨ã—ã¤ã¤ã‚‚ã€ã‚ãªãŸã®å¼·ã¿ã§ã‚ã‚‹ã€Œè«–ç†çš„æ¨è«–(Reasoning)ã€ã‚’æ´»ã‹ã—ã¦ã€\n"
        "GeminiãŒè¦‹è½ã¨ã—ãŒã¡ãªã€å‰æã®èª¤ã‚Šã€ã€éš ã‚ŒãŸãƒªã‚¹ã‚¯ã€ã€åˆ¥ã®å¯èƒ½æ€§ã€ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚\n"
        "å›ç­”ã¯ç°¡æ½”ã«ã€ç®‡æ¡æ›¸ãã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    )

    user_content = (
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
        f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
        "æŒ‡ç¤º:\n"
        "èª¿æŸ»ãƒ¡ãƒ¢ã‚’å…ƒã«ã€ã‚ãªãŸè‡ªèº«ã®è¦–ç‚¹ã§å›ç­”æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
    )

    try:
        # AWS Bedrock ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
        bedrock = boto3.client(
            service_name='bedrock-runtime',
            region_name=CLAUDE_REGION,
            aws_access_key_id=AWS_ACCESS_KEY_ID,
            aws_secret_access_key=AWS_SECRET_ACCESS_KEY
        )

        # Bedrock converse API ã‚’ä½¿ç”¨ï¼ˆinference profileå¯¾å¿œ + Extended Thinkingï¼‰
        resp = bedrock.converse(
            modelId=CLAUDE_MODEL_ID,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"text": f"{system_prompt}\n\n{user_content}"}
                    ],
                }
            ],
            inferenceConfig={
                "maxTokens": 5000,  # Thinking modeã§ã¯å¤šã‚ã«ç¢ºä¿
                "temperature": 1.0,  # Extended Thinking mode ã§ã¯å¿…é ˆ
            },
            # Extended Thinking Mode ã‚’æœ‰åŠ¹åŒ–
            additionalModelRequestFields={
                "thinking": {
                    "type": "enabled",
                    "budget_tokens": 3000  # æ€è€ƒç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                }
            }
        )

        # æ€è€ƒãƒ–ãƒ­ãƒƒã‚¯ã¨å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã®å–ã‚Šå‡ºã— (reasoningContentå¯¾å¿œ)
        thinking_blocks = []
        text_chunks = []
        output = resp.get("output", {})
        message = output.get("message", {})
        
        for part in message.get("content", []):
            # Extended Thinking ã®æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ (reasoningContent)
            if "reasoningContent" in part:
                rc = part["reasoningContent"]
                if isinstance(rc, dict):
                    # reasoningText.text ã‚’å–å¾—
                    rt = rc.get("reasoningText", {})
                    if isinstance(rt, dict):
                        t = rt.get("text")
                        if t:
                            thinking_blocks.append(t)
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: rc["text"] ã‚‚è©¦ã™
                    elif "text" in rc:
                        thinking_blocks.append(rc["text"])
            # æœ€çµ‚å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
            elif "text" in part:
                text_chunks.append(part["text"])

        # ä½¿ç”¨é‡æƒ…å ±ã®å–å¾—
        usage = resp.get("usage", {})
        usage_dict = {
            "inputTokens": usage.get("inputTokens", 0),
            "outputTokens": usage.get("outputTokens", 0)
        }

        result_text = "".join(text_chunks) if text_chunks else "[Claude 4.5 Sonnetã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã—ãŸ]"
        
        # æ€è€ƒãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯å†’é ­ã«è¿½åŠ 
        if thinking_blocks:
            thinking_text = "\n\n".join([f"**ğŸ§  æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ {i+1}:**\n{block}" for i, block in enumerate(thinking_blocks)])
            result_text = f"{thinking_text}\n\n---\n\n**ğŸ’¡ æœ€çµ‚å›ç­”:**\n{result_text}"
        
        return (result_text, usage_dict)

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¿”ã™
        return (f"Error calling Claude 4.5 Sonnet (Bedrock): {e}", {})




def think_with_o4_mini(user_question: str, research_text: str) -> tuple[str, dict]:
    """
    GitHub ModelsçµŒç”±ã§o4-miniã‚’ä½¿ã£ã¦ç‹¬ç«‹ã—ãŸå›ç­”æ¡ˆã‚’ä½œæˆã™ã‚‹
    åˆ¶é™: input 4000ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸‹ã®å ´åˆã®ã¿ä½¿ç”¨
    Returns: (å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ, ç©ºdict - GitHub Modelsã¯usageæƒ…å ±ã‚’è¿”ã•ãªã„)
    """
    if not GITHUB_TOKEN:
        return ("Error: GitHub Token is missing.", {})
    
    # é•·ã•ãƒã‚§ãƒƒã‚¯ã¯å‘¼ã³å‡ºã—å´ã§å®Ÿæ–½æ¸ˆã¿ï¼ˆ3800æ–‡å­—ä»¥ä¸‹ã‚’ä¿è¨¼ï¼‰
    
    
    system_prompt = (
        "ã‚ãªãŸã¯Geminiã¨ã¯ç‹¬ç«‹ã—ãŸAIã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚\n"
        "æä¾›ã•ã‚ŒãŸèª¿æŸ»ãƒ¡ãƒ¢ã‚’äº‹å®Ÿã®ãƒ™ãƒ¼ã‚¹ã¨ã—ã¤ã¤ã‚‚ã€æ¨è«–èƒ½åŠ›ã‚’æ´»ã‹ã—ã¦ã€\n"
        "GeminiãŒè¦‹è½ã¨ã—ãŒã¡ãªã€å‰æã®èª¤ã‚Šã€ã€éš ã‚ŒãŸãƒªã‚¹ã‚¯ã€ã€åˆ¥ã®å¯èƒ½æ€§ã€ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚\n"
        "å›ç­”ã¯ç°¡æ½”ã«ã€ç®‡æ¡æ›¸ãã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    )
    
    user_content = (
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
        f"èª¿æŸ»ãƒ¡ãƒ¢:\n{research_text}\n\n"
        "æŒ‡ç¤º:\n"
        "èª¿æŸ»ãƒ¡ãƒ¢ã‚’å…ƒã«ã€ã‚ãªãŸè‡ªèº«ã®è¦–ç‚¹ã§å›ç­”æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
    )
    
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Content-Type": "application/json",
    }
    
    data = {
        "model": "gpt-4o-mini",  # GitHub Modelsç”¨ã®ãƒ¢ãƒ‡ãƒ«å
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ],
        "temperature": 0.7,
        "max_tokens": 2000,
    }
    
    try:
        import requests
        response = requests.post(
            f"https://models.inference.ai.azure.com/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()
        result = response.json()
        answer_text = result["choices"][0]["message"]["content"]
        return (answer_text, {})  # GitHub Modelsã¯usageæƒ…å ±ã‚’è¿”ã•ãªã„
    except Exception as e:
        return (f"Error calling o4-mini (GitHub Models): {e}", {})


# Puteré–¢é€£ã®é–¢æ•°ã¯å‰Šé™¤ï¼ˆAWS Bedrockã«ç§»è¡Œï¼‰

def create_new_session():
    current_sessions = load_sessions()
    
    # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒç©ºãªã‚‰ã€æ–°ã—ãä½œã‚‰ãšã«ãã‚Œã‚’å†åˆ©ç”¨ã™ã‚‹ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
    if st.session_state.get("current_session_id"):
        for s in current_sessions:
            if s["id"] == st.session_state.current_session_id:
                if len(s["messages"]) == 0:
                    st.toast("ã™ã§ã«æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã§ã™")
                    return

    new_id = str(uuid.uuid4())
    new_session = {
        "id": new_id,
        "title": "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ",
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": [],
    }
    current_sessions.insert(0, new_session)
    save_sessions(current_sessions)
    st.session_state.sessions = current_sessions
    st.session_state.current_session_id = new_id
    st.rerun()

def switch_session(session_id):
    st.session_state.current_session_id = session_id
    st.rerun()

def update_current_session_messages(messages):
    if st.session_state.current_session_id:
        current_sessions = load_sessions()
        target_index = -1
        for i, session in enumerate(current_sessions):
            if session["id"] == st.session_state.current_session_id:
                session["messages"] = messages
                if session["title"] == "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ" and len(messages) > 0:
                    first_msg = messages[0]["content"]
                    session["title"] = (first_msg[:20] + "...") if len(first_msg) > 20 else first_msg
                session["timestamp"] = datetime.datetime.now().isoformat()
                target_index = i
                break
        
        if target_index != -1:
            # æœ€æ–°ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚¹ãƒˆã®å…ˆé ­ã«ç§»å‹•
            updated_session = current_sessions.pop(target_index)
            current_sessions.insert(0, updated_session)
            
        save_sessions(current_sessions)
        st.session_state.sessions = current_sessions

def get_current_messages():
    if st.session_state.current_session_id:
        for session in st.session_state.sessions:
            if session["id"] == st.session_state.current_session_id:
                return session["messages"]
    return []

def delete_session(session_id):
    current_sessions = load_sessions()
    current_sessions = [s for s in current_sessions if s["id"] != session_id]
    save_sessions(current_sessions)
    st.session_state.sessions = current_sessions
    if st.session_state.current_session_id == session_id:
        st.session_state.current_session_id = None
        if st.session_state.sessions:
            st.session_state.current_session_id = st.session_state.sessions[0]["id"]
    st.rerun()

def branch_session():
    """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’åˆ†å²"""
    current_messages = get_current_messages()
    current_sessions = load_sessions()
    
    # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    current_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
    for session in current_sessions:
        if session["id"] == st.session_state.current_session_id:
            current_title = session["title"]
            break
    
    # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    new_id = str(uuid.uuid4())
    new_session = {
        "id": new_id,
        "title": f"{current_title} (åˆ†å²)",
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": current_messages.copy(),  # ç¾åœ¨ã®å±¥æ­´ã‚’ã‚³ãƒ”ãƒ¼
    }
    current_sessions.insert(0, new_session)
    save_sessions(current_sessions)
    
    st.session_state.sessions = current_sessions
    st.session_state.current_session_id = new_id
    st.session_state.session_cost = 0.0  # ã‚³ã‚¹ãƒˆãƒªã‚»ãƒƒãƒˆ
    st.rerun()

# =========================
# Initialization
# =========================

if "sessions" not in st.session_state:
    st.session_state.sessions = load_sessions()

if "current_session_id" not in st.session_state:
    # Always create a new session when the app starts
    create_new_session()

if "session_cost" not in st.session_state:
    st.session_state.session_cost = 0.0

usage_stats = load_usage()

# ==========================
# ã‚³ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã®å‰ã«å®Ÿè¡Œï¼‰
# ==========================
usage_stats = load_usage()
stop_generation = usage_stats["total_cost_usd"] >= MAX_BUDGET_USD

# =========================
# Sidebar
# =========================

with st.sidebar:
    # ğŸ” ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³
    if st.button("ğŸ”’ ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", use_container_width=True):
        st.session_state.authenticated = False
        st.query_params.clear()  # URLãƒˆãƒ¼ã‚¯ãƒ³ã‚‚å‰Šé™¤
        st.rerun()

    st.markdown("""
    <div style="text-align: center; padding: 0; margin: 0; margin-top: -1rem;">
        <h1 style="font-size: 18px; font-weight: 700; margin: 0; padding: 0; letter-spacing: 1px;">
            Gemini 3<br/>Studio
        </h1>
    </div>
    """, unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• æ–°è¦", use_container_width=True):
            create_new_session()
    with col2:
        if st.button("ğŸŒ± åˆ†å²", use_container_width=True):
            branch_session()

    # ---- å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ ----
    # ---- å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ ----
    current_messages = get_current_messages()
    if current_messages:
        with st.expander("ğŸ”— å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ", expanded=False):
            export_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
            for s in st.session_state.sessions:
                if s["id"] == st.session_state.current_session_id:
                    export_title = s["title"]
                    break

            export_md = f"# {export_title}\n\n"
            export_md += f"**ä½œæˆæ—¥æ™‚**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
            for msg in current_messages:
                role_label = "ğŸ§‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg["role"] == "user" else "ğŸ¤– AI"
                export_md += f"## {role_label}\n\n{msg['content']}\n\n---\n\n"

            if st.button("ãƒªãƒ³ã‚¯ç”Ÿæˆ", use_container_width=True):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    try:
                        import urllib.request
                        import urllib.parse

                        data = urllib.parse.urlencode(
                            {"content": export_md, "expiry_days": 7, "syntax": "md"}
                        ).encode()

                        req = urllib.request.Request("https://dpaste.org/api/", data=data)
                        req.add_header("User-Agent", "Mozilla/5.0 (Gemini3Studio)")

                        with urllib.request.urlopen(req) as response:
                            share_url = response.read().decode("utf-8").strip()
                            st.success("ä½œæˆå®Œäº†ï¼")
                            st.code(share_url)
                            st.caption("â€»7æ—¥é–“æœ‰åŠ¹")
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    st.markdown("---")

    # ---- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ ----
    # ---- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ ----
    with st.expander("ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«", expanded=False):
        uploaded_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«",
            accept_multiple_files=True,
            type=["png", "jpg", "jpeg", "mp4", "mov", "txt", "pdf", "csv"],
            label_visibility="collapsed"
        )

    # ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ï¼ˆExpanderã®å¤–ã«å‡ºã™ï¼‰
    pasted_image_bytes = None  # åˆæœŸåŒ–ã—ã¦NameErrorã‚’é˜²ã
    if paste:
        if "paste_key" not in st.session_state:
            st.session_state.paste_key = 0
        try:
            pasted_image_bytes = paste(
                label="ğŸ“‹ ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰è²¼ä»˜",
                key=f"paste_btn_{st.session_state.paste_key}",
            )
        except Exception as e:
            st.error(f"Error: {e}")
        
        if pasted_image_bytes:
            st.success("è²¼ä»˜å®Œäº†")
            st.image(pasted_image_bytes, caption="ç”»åƒ", use_container_width=True)
            if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", key="clear_paste"):
                st.session_state.paste_key += 1
                st.rerun()
    else:
        st.warning("Clipboard lib missing")

    # ---- YouTube URL ----
    with st.expander("ğŸ“º YouTubeåˆ†æ", expanded=False):
        youtube_url = st.text_input("URL", placeholder="https://youtu.be/...", label_visibility="collapsed")

    st.markdown("---")
    
    st.markdown("---")
    
    # ---- ãƒ¢ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªé¸æŠ ----
    mode_category = st.radio(
        "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰",
        ["ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)", "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(é€šå¸¸)"],
        index=0,  #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
        horizontal=True,
    )
    
    # ---- å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ ----
    if mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)":
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(å¤šå±¤)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["grokå¼·åŒ–(+mz/Az)", "groké€šå¸¸ãƒ¢ãƒ¼ãƒ‰", "grokå¼·åŒ–ãƒ¢ãƒ¼ãƒ‰", "ãã®ä»–"],
                index=0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’grokå¼·åŒ–(+mz/Az)ã«
                horizontal=True,
                label_visibility="collapsed"
            )
            
            if mode_type == "grokå¼·åŒ–(+mz/Az)":
                response_mode = st.radio(
                   "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "ç†Ÿè€ƒ (æœ¬æ°—MAX)Az",
                        "ç†Ÿè€ƒ (æœ¬æ°—MAX)ms/Az",
                        "ç†Ÿè€ƒ(ãƒ¡ã‚¿æ€è€ƒ)+grokæ¤œç´¢å¼·åŒ–ç‰ˆ",
                    ],
                    index=2  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ms/Az (o4-miniä»˜ã)
                )
            elif mode_type == "groké€šå¸¸ãƒ¢ãƒ¼ãƒ‰":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "(è©¦é¨“ä¸­)ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "(è©¦é¨“ä¸­)ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                    ],
                    index=1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ¡ã‚¿æ€è€ƒ
                )
            elif mode_type == "grokå¼·åŒ–ãƒ¢ãƒ¼ãƒ‰":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                        "ç†Ÿè€ƒ(ãƒ¡ã‚¿æ€è€ƒ)+grokæ¤œç´¢å¼·åŒ–ç‰ˆ",
                    ],
                    index=1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ¬æ°—MAX
                )
            else:  # ãã®ä»–
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    # Puterãƒ¢ãƒ¼ãƒ‰ã¯å‰Šé™¤ï¼ˆAWS Bedrockã«ç§»è¡Œï¼‰
    
    # ---- é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ ----
    else:
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(é€šå¸¸)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["é¸æŠ1 (å®Œå…¨ç‰ˆ)", "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)", "ãƒ™ãƒ¼ã‚¿ç‰ˆ"],
                index=0,
                horizontal=True,
                label_visibility="collapsed"
            )
            
            if mode_type == "é¸æŠ1 (å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "2. ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "3. ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                    ],
                    index=0
                )
            elif mode_type == "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                    ],
                    index=0
                )
            else:
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    strict_mode = False
    
    # ---- è¨­å®š (ãƒ¢ãƒ‡ãƒ«ãªã©) ----
    with st.expander("âš™ï¸ è¨­å®š", expanded=False):
        model_options = [
            "gemini-3-pro-preview",
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.0-flash",
        ]
        model_id = st.selectbox("ãƒ¢ãƒ‡ãƒ«ID", options=model_options, index=0)

        use_search = st.toggle("Googleæ¤œç´¢", value=True)
        candidate_count = st.slider("å€™è£œæ•°", min_value=1, max_value=3, value=3)

    st.markdown("---")

    # ---- å±¥æ­´æ¤œç´¢ ----
    search_query = st.text_input("ğŸ” å±¥æ­´æ¤œç´¢", placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰...")
    if search_query:
        filtered_sessions = []
        for s in st.session_state.sessions:
            if search_query.lower() in s["title"].lower():
                filtered_sessions.append(s)
                continue
            found = False
            for m in s["messages"]:
                if search_query.lower() in m["content"].lower():
                    filtered_sessions.append(s)
                    found = True
                    break
            if not found:
                pass
    else:
        # æ¤œç´¢ã—ã¦ã„ãªã„å ´åˆ: ç©ºã®ã€Œæ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã€ã‚’é™¤å¤–ï¼ˆç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯é™¤ãï¼‰
        filtered_sessions = []
        for s in st.session_state.sessions:
            if s["id"] == st.session_state.current_session_id:
                filtered_sessions.append(s)
                continue
            if s["title"] == "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ" and len(s["messages"]) == 0:
                continue
            filtered_sessions.append(s)

    # CSSã§è¦‹ã‚„ã™ãä½¿ã„ã‚„ã™ãæœ€é©åŒ–
    st.markdown("""
    <style>
    /* å…¨ä½“ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºç¸®å° */
    .stApp, .stMarkdown, .stButton, .stSelectbox, .stTextInput, .stTextArea {
        font-size: 12px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä½™ç™½ã‚’æ¥µé™ã¾ã§è©°ã‚ã‚‹ */
    section[data-testid="stSidebar"] .block-container {
        padding-top: 0.3rem !important;
        padding-bottom: 0.3rem !important;
        padding-left: 0.5rem !important;
        padding-right: 0.5rem !important;
    }
    /* å„è¦ç´ é–“ã®éš™é–“ã‚’è©°ã‚ã‚‹ */
    div[data-testid="stVerticalBlock"] > div {
        gap: 0.1rem !important;
    }
    /* Expanderã®ä½™ç™½å‰Šæ¸› */
    .streamlit-expanderHeader {
        font-size: 11px !important;
        padding-top: 0px !important;
        padding-bottom: 0px !important;
        min-height: 1.3rem !important;
    }
    .streamlit-expanderContent {
        padding-top: 0px !important;
        padding-bottom: 0px !important;
    }
    /* éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ã‚¿ã‚¤ãƒˆãƒ« */
    div[data-testid="stExpander"] summary p {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ */
    section[data-testid="stSidebar"] button p {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã« */
    section[data-testid="stSidebar"] button {
        padding: 0rem 0.2rem !important;
        min-height: 1.4rem !important;
        font-size: 10px !important;
        margin-bottom: 0px !important;
    }
    /* ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®ãƒ©ãƒ™ãƒ«ã‚’å°ã•ã */
    section[data-testid="stSidebar"] .stRadio label {
        font-size: 10px !important;
    }
    section[data-testid="stSidebar"] .stRadio > label > div {
        font-size: 10px !important;
    }
    /* ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ã‚‹ï¼ˆæ”¹è¡Œã•ã›ãªã„ï¼‰ */
    section[data-testid="stSidebar"] label, 
    section[data-testid="stSidebar"] p,
    section[data-testid="stSidebar"] span {
        white-space: nowrap !important;
        overflow: hidden !important;
        text-overflow: ellipsis !important;
    }
    /* åŒºåˆ‡ã‚Šç·š */
    hr {
        margin-top: 0.1rem !important;
        margin-bottom: 0.1rem !important;
    }
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ç¸®å° */
    h1, h2, h3 {
        padding-top: 0rem !important;
        padding-bottom: 0rem !important;
        margin-top: 0rem !important;
        margin-bottom: 0rem !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§é™é †ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°ãŒå…ˆé ­ï¼‰
    filtered_sessions.sort(key=lambda s: s.get("timestamp", ""), reverse=True)
    
    # ç›´è¿‘5ä»¶ã¨éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«åˆ†å‰²
    recent_sessions = filtered_sessions[:5] if len(filtered_sessions) > 5 else filtered_sessions
    archive_sessions = filtered_sessions[5:] if len(filtered_sessions) > 5 else []
    
    # ç›´è¿‘5ä»¶ï¼ˆå¸¸ã«å±•é–‹ï¼‰
    if recent_sessions:
        st.markdown("**ğŸ“Œ ç›´è¿‘ã®ãƒãƒ£ãƒƒãƒˆ**")
        for session in recent_sessions:
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                if st.button(session["title"], key=f"btn_{session['id']}", use_container_width=True):
                    switch_session(session["id"])
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_{session['id']}"):
                    delete_session(session["id"])
    
    # éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
    if archive_sessions:
        with st.expander(f"ğŸ“œ éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– ({len(archive_sessions)}ä»¶)", expanded=False):
            for session in archive_sessions:
                col1, col2 = st.columns([0.8, 0.2])
                with col1:
                    if st.button(session["title"], key=f"btn_{session['id']}", use_container_width=True):
                        switch_session(session["id"])
                with col2:
                    if st.button("ğŸ—‘ï¸", key=f"del_{session['id']}"):
                        delete_session(session["id"])

    st.markdown("---")

    # ---- ç”»åƒç”Ÿæˆ ----
    with st.expander("ğŸ¨ ç”»åƒç”Ÿæˆ", expanded=False):
        img_prompt = st.text_area("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", placeholder="æœªæ¥çš„ãªéƒ½å¸‚...")
        aspect_ratio = st.selectbox("æ¯”ç‡", ["16:9", "1:1", "4:3", "3:4", "9:16"])

        generate_img_btn = st.button("ç”Ÿæˆ", type="primary", disabled=not img_prompt)
        if generate_img_btn and img_prompt:
            st.session_state.generate_image_trigger = {
                "prompt": img_prompt,
                "aspect_ratio": aspect_ratio,
            }

    st.markdown("---")

    # ---- è©•ä¾¡åˆ†æ ----
    with st.expander("ğŸ“Š å“è³ªåˆ†æ", expanded=False):
        total_ratings = 0
        positive_ratings = 0
        model_ratings = {}

        for s in st.session_state.sessions:
            for m in s["messages"]:
                if "rating" in m and m["rating"] is not None:
                    total_ratings += 1
                    if m["rating"] == 1:
                        positive_ratings += 1
                    if "metadata" in m and "model" in m["metadata"]:
                        mod = m["metadata"]["model"]
                        if mod not in model_ratings:
                            model_ratings[mod] = {"total": 0, "positive": 0}
                        model_ratings[mod]["total"] += 1
                        if m["rating"] == 1:
                            model_ratings[mod]["positive"] += 1

        if total_ratings > 0:
            approval_rate = (positive_ratings / total_ratings) * 100
            st.metric("Positive Ratings", f"{positive_ratings}/{total_ratings}", f"{approval_rate:.1f}%")
            best_model = "N/A"
            best_rate = -1
            for mod, stats in model_ratings.items():
                rate = stats["positive"] / stats["total"]
                if rate > best_rate:
                    best_rate = rate
                    best_model = mod
            if best_model != "N/A":
                st.caption(f"ğŸ† Best: **{best_model}** ({best_rate*100:.0f}%)")
        else:
            st.caption("ãƒ‡ãƒ¼ã‚¿ãªã—")

    st.markdown("---")

    # ---- ã‚³ã‚¹ãƒˆè¡¨ç¤º ----
    from logic import load_manual_cost, save_manual_cost, MAX_BUDGET_JPY, TRIAL_LIMIT_JPY, TRIAL_EXPIRY
    
    st.subheader("ğŸ’° Cost")
    st.caption(f"äºˆç®—: Â¥{MAX_BUDGET_JPY:,.0f}")
    st.caption(f"ä¸Šé™: Â¥{TRIAL_LIMIT_JPY:,.0f}")
    st.caption(f"æœ‰åŠ¹æœŸé™ (GCP): {TRIAL_EXPIRY}")
    st.caption("ğŸ†“ AWS Free Tier: Jun 02, 2026")
    
    # æ‰‹å‹•ã‚³ã‚¹ãƒˆå…¥åŠ›ï¼ˆæ°¸ç¶šåŒ–ï¼‰
    current_manual_cost = load_manual_cost()
    manual_cost = st.number_input(
        "æ‰‹å‹•å…¥åŠ› (Â¥)",
        min_value=0.0,
        value=current_manual_cost,
        step=10.0,
        format="%.0f",
        key="manual_cost_persistent",
        help="Google Cloud Consoleã§ç¢ºèªã—ãŸå®Ÿéš›ã®ã‚³ã‚¹ãƒˆï¼ˆå††ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ã“ã®å€¤ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¦ã‚‚ä¿æŒã•ã‚Œã¾ã™ã€‚"
    )
    
    # å€¤ãŒå¤‰æ›´ã•ã‚ŒãŸã‚‰ä¿å­˜
    if manual_cost != current_manual_cost:
        save_manual_cost(manual_cost)
    
    st.link_button("ğŸ’° Google Cloud Console", "https://console.cloud.google.com/welcome/new?_gl=1*kmr691*_up*MQ..&gclid=CjwKCAiAraXJBhBJEiwAjz7MZT0vQsfDK5zunRBCQmuN5iczgI4bP1lHo1Tcrcbqu1KCBE1D22GpFhoCOdgQAvD_BwE&gclsrc=aw.ds&hl=ja&authuser=5&project=sigma-task-479704-r6")
    st.link_button("â˜ï¸ AWS Free Tier Dashboard", "https://us-east-1.console.aws.amazon.com/costmanagement/home?region=us-east-1#/freetier")
    st.caption("ğŸ“˜ GitHub Models: ä½¿ç”¨çŠ¶æ³ã¯ [Settings â†’ Developer settings â†’ Tokens](https://github.com/settings/tokens) ã§ç¢ºèª")
    
    # â–¼â–¼â–¼ Debug: API Key Status â–¼â–¼â–¼
    with st.expander("ğŸ” API Status (Debug)", expanded=False):
        st.caption(f"AWS: {'âœ…' if AWS_ACCESS_KEY_ID else 'âŒ'}")
        st.caption(f"OpenRouter: {'âœ…' if OPENROUTER_API_KEY else 'âŒ'}")
        st.caption(f"GitHub: {'âœ…' if GITHUB_TOKEN else 'âŒ'}")
        # Puterã¯éè¡¨ç¤ºï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®ç†ç”±ï¼‰
    # â–²â–²â–² Debug â–²â–²â–²

    st.markdown("---")
    st.code(f"PROJECT: {VERTEX_PROJECT}\nLOCATION: {VERTEX_LOCATION} (Vertex AI)")

# =========================
# Main UI
# =========================

current_session_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
for s in st.session_state.sessions:
    if s["id"] == st.session_state.current_session_id:
        current_session_title = s["title"]
        break

st.header(current_session_title)
st.markdown(
    "ä»¥ä¸‹ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€YouTubeåˆ†æã€æ¤œç´¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
)


# ---- ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³ (Floating) ----
st.markdown("""
    <style>
        .scroll-btn-container {
            position: fixed;
            bottom: 100px;
            right: 20px;
            z-index: 9999;
            display: flex;
            flex-direction: column;
            gap: 10px;
        }
        .scroll-btn {
            background-color: #f0f2f6;
            color: #31333F;
            border: 1px solid #d6d6d8;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            font-size: 20px;
            padding: 0;
            line-height: 1;
        }
        .scroll-btn:hover {
            background-color: #e0e2e6;
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
        }
    </style>
    
    <script>
        window.scrollStreamlit = function(direction) {
            console.log("Scroll triggered: " + direction);
            
            // ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹è¦ç´ ã‚’é †ç•ªã«è©¦ã™
            var targets = [];
            
            try {
                if (window.parent && window.parent.document) {
                    targets.push(window.parent.document.querySelector('section[data-testid="stAppViewContainer"]'));
                    targets.push(window.parent.document.querySelector('.main'));
                    targets.push(window.parent.document.documentElement);
                }
            } catch (e) {
                console.log("Access to parent window denied");
            }
            
            // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆiframeå†…ï¼‰
            targets.push(document.querySelector('section[data-testid="stAppViewContainer"]'));
            targets.push(document.documentElement);

            var scrolled = false;
            for (var i = 0; i < targets.length; i++) {
                var el = targets[i];
                if (el) {
                    try {
                        // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªè¦ç´ ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“çš„ï¼‰
                        if (el.scrollHeight > el.clientHeight || el === window.parent.document.documentElement) {
                            console.log("Scrolling element:", el);
                            if (direction === 'top') {
                                el.scrollTo({top: 0, behavior: 'smooth'});
                            } else {
                                el.scrollTo({top: el.scrollHeight, behavior: 'smooth'});
                            }
                            scrolled = true;
                        }
                    } catch (e) {
                        console.error("Error scrolling element:", e);
                    }
                }
            }
            
            if (!scrolled) {
                console.log("No scrollable container found, trying window scroll");
                try {
                    if (direction === 'top') {
                        window.parent.scrollTo({top: 0, behavior: 'smooth'});
                    } else {
                        window.parent.scrollTo({top: window.parent.document.body.scrollHeight, behavior: 'smooth'});
                    }
                } catch(e) {
                    window.scrollTo({top: 0, behavior: 'smooth'});
                }
            }
        }
    </script>

    <div class="scroll-btn-container">
        <button class="scroll-btn" onclick="window.scrollStreamlit('top')" title="Top">â¬†ï¸</button>
        <button class="scroll-btn" onclick="window.scrollStreamlit('bottom')" title="Bottom">â¬‡ï¸</button>
    </div>
    """, unsafe_allow_html=True)

# ---- Vertex AI Client ----



client = get_client()

# ---- å±¥æ­´è¡¨ç¤º ----

messages = get_current_messages()
for idx, msg in enumerate(messages):
    with st.chat_message(msg["role"]):
        # Display timestamp if available
        if "timestamp" in msg:
            st.caption(f"ğŸ•’ {msg['timestamp']}")
        st.markdown(msg["content"])
        if msg["role"] == "model":
            col1, col2, col3 = st.columns([0.1, 0.1, 0.8])
            current_rating = msg.get("rating")
            with col1:
                if st.button("ğŸ‘", key=f"up_{idx}"):
                    messages[idx]["rating"] = 1
                    update_current_session_messages(messages)
                    st.rerun()
            with col2:
                if st.button("ğŸ‘", key=f"down_{idx}"):
                    messages[idx]["rating"] = -1
                    update_current_session_messages(messages)
                    st.rerun()
            with col3:
                if current_rating == 1:
                    st.caption("âœ… é«˜è©•ä¾¡")
                elif current_rating == -1:
                    st.caption("âŒ ä½è©•ä¾¡")

# ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³ï¼ˆé•·ã„ãƒãƒ£ãƒƒãƒˆç”¨ï¼‰
if len(messages) > 5:
    st.markdown("""
    <div style="position: fixed; right: 20px; bottom: 100px; z-index: 999;">
        <button onclick="window.scrollTo({top: 0, behavior: 'smooth'})" 
                style="display: block; margin: 5px; padding: 10px 15px; font-size: 24px; cursor: pointer; 
                       border: 2px solid #ccc; border-radius: 50%; background: white; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">
            â¬†ï¸
        </button>
        <button onclick="window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'})" 
                style="display: block; margin: 5px; padding: 10px 15px; font-size: 24px; cursor: pointer; 
                       border: 2px solid #ccc; border-radius: 50%; background: white; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">
            â¬‡ï¸
        </button>
    </div>
    """, unsafe_allow_html=True)


# =========================
# ç”»åƒç”Ÿæˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
# =========================

if hasattr(st.session_state, "generate_image_trigger") and st.session_state.generate_image_trigger:
    img_data = st.session_state.generate_image_trigger
    del st.session_state.generate_image_trigger

    with st.chat_message("user"):
        st.markdown(f"ğŸ¨ ç”»åƒç”Ÿæˆ: {img_data['prompt']}")
        st.caption(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”: {img_data['aspect_ratio']}")

    messages.append({
        "role": "user",
        "content": f"ğŸ¨ ç”»åƒç”Ÿæˆ: {img_data['prompt']}",
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    })
    update_current_session_messages(messages)

    with st.chat_message("assistant"):
        with st.status("ç”»åƒã‚’ç”Ÿæˆä¸­...", expanded=True) as status:
            try:
                config = types.GenerateContentConfig(
                    response_modalities=["IMAGE"],
                    image_config=types.ImageConfig(
                        aspect_ratio=img_data["aspect_ratio"]
                    ),
                )
                status.write("Gemini 3 Pro Imageã§ç”Ÿæˆä¸­...")
                response = client.models.generate_content(
                    model="gemini-3-pro-image-preview",
                    contents=img_data["prompt"],
                    config=config,
                )

                generated_image = None

                # ç”»åƒã‚’å–ã‚Šå‡ºã™
                if getattr(response, "candidates", None):
                    for candidate in response.candidates:
                        if getattr(candidate, "content", None) and candidate.content.parts:
                            for part in candidate.content.parts:
                                if getattr(part, "inline_data", None):
                                    try:
                                        generated_image = part.as_image()
                                    except AttributeError:
                                        image_bytes_raw = part.inline_data.data
                                        generated_image = Image.open(io.BytesIO(image_bytes_raw))
                                    break
                        if generated_image is not None:
                            break

                if generated_image is not None:
                    st.image(generated_image, caption=img_data["prompt"])
                    buf = io.BytesIO()
                    generated_image.save(buf, format="PNG")
                    image_bytes = buf.getvalue()
                    st.download_button(
                        label="ğŸ’¾ ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=image_bytes,
                        file_name=f"generated_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                        mime="image/png",
                    )
                    messages.append(
                        {"role": "model",
                        "content": f"âœ… ç”»åƒã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {img_data['prompt']}",
                        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                    )
                    update_current_session_messages(messages)
                    status.update(label="âœ… ç”»åƒç”Ÿæˆå®Œäº†", state="complete")
                else:
                    st.error("ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error")

            except Exception as e:
                status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error")
                st.error(f"ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

# =========================
# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
# =========================

prompt = st.chat_input("ä½•ã‹èã„ã¦ãã ã•ã„...", disabled=stop_generation)

if prompt:
    if stop_generation:
        st.error("äºˆç®—ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚")
    else:
        # ---- ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€è¡¨ç¤º ----
        with st.chat_message("user"):
            # ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ä»˜ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
            import html
            escaped_prompt = html.escape(prompt)
            message_id = f"user_msg_{len(messages)}"
            
            st.markdown(f"""
<div style="position: relative;">
    <div id="{message_id}" style="padding-right: 40px;">{escaped_prompt}</div>
    <button onclick="copyToClipboard('{message_id}')" style="position: absolute; right: 0; top: 0; background: transparent; border: 1px solid #444; border-radius: 4px; cursor: pointer; padding: 4px 8px; color: #aaa; font-size: 12px;" title="ã‚³ãƒ”ãƒ¼">
        ğŸ“‹
    </button>
</div>
<script>
function copyToClipboard(elementId) {{
    const element = document.getElementById(elementId);
    const text = element.innerText;
    navigator.clipboard.writeText(text).then(() => {{
        // ã‚³ãƒ”ãƒ¼æˆåŠŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        const button = event.target;
        button.textContent = 'âœ“';
        setTimeout(() => {{ button.textContent = 'ğŸ“‹'; }}, 1000);
    }});
}}
</script>
""", unsafe_allow_html=True)
            if uploaded_files:
                for uf in uploaded_files:
                    st.caption(f"ğŸ“ æ·»ä»˜: {uf.name}")
            if youtube_url:
                st.caption(f"ğŸ“º YouTube: {youtube_url}")
            if pasted_image_bytes:
                st.caption("ğŸ“‹ ç”»åƒãŒè²¼ã‚Šä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ")

        messages.append({
            "role": "user",
            "content": prompt,
            "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        update_current_session_messages(messages)

        # ---- ãƒ¢ãƒ‡ãƒ«å¿œç­” ----
        with st.chat_message("assistant"):
            status_container = st.status("æ€è€ƒä¸­...", expanded=True)
            try:
                # ä¼šè©±å±¥æ­´
                model_history = []
                for msg in messages[:-1]:
                    model_history.append(
                        types.Content(
                            role=msg["role"],
                            parts=[types.Part.from_text(text=msg["content"])],
                        )
                    )

                # ç¾åœ¨ã®ã‚¿ãƒ¼ãƒ³
                current_parts = [types.Part.from_text(text=prompt)]

                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
                for uploaded_file in uploaded_files or []:
                    try:
                        mime_type = get_mime_type(uploaded_file.name)
                        bytes_data = uploaded_file.getvalue()
                        part = types.Part.from_bytes(data=bytes_data, mime_type=mime_type)
                        current_parts.append(part)
                        status_container.write(f"ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™å®Œäº†: {uploaded_file.name}")
                    except Exception as e:
                        status_container.error(
                            f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {uploaded_file.name} - {e}"
                        )

                # è²¼ã‚Šä»˜ã‘ç”»åƒ
                if pasted_image_bytes:
                    import base64

                    status_container.write("è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã‚’å‡¦ç†ä¸­...")
                    try:
                        if isinstance(pasted_image_bytes, str):
                            if pasted_image_bytes.startswith("data:"):
                                base64_str = pasted_image_bytes.split(",", 1)[1]
                                image_bytes_decoded = base64.b64decode(base64_str)
                            else:
                                image_bytes_decoded = base64.b64decode(pasted_image_bytes)
                        else:
                            image_bytes_decoded = pasted_image_bytes
                        part = types.Part.from_bytes(
                            data=image_bytes_decoded, mime_type="image/png"
                        )
                        current_parts.append(part)
                        status_container.write("è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã®æº–å‚™å®Œäº†")
                    except Exception as e:
                        status_container.error(f"è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

                # YouTube å­—å¹•
                if youtube_url:
                    vid_id = extract_youtube_id(youtube_url)
                    if vid_id:
                        status_container.write("YouTubeã®å­—å¹•ã‚’å–å¾—ä¸­...")
                        transcript_text = get_youtube_transcript(vid_id)
                        current_parts.append(
                            types.Part.from_text(text=f"YouTube Transcript:\n{transcript_text}")
                        )
                    else:
                        status_container.write("ç„¡åŠ¹ãªYouTube URLã§ã™ã€‚")

                contents_for_model = model_history + [
                    types.Content(role="user", parts=current_parts)
                ]

                # ---- Tool / Config ----
                tools = []
                final_candidate_count = candidate_count
                if use_search:
                    tools.append(types.Tool(google_search=types.GoogleSearch()))
                    final_candidate_count = 1

                # â˜… System Instruction ã®æ”¹å–„: ãƒ¡ã‚¿ç™ºè¨€ç¦æ­¢ã¨ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªæŒ¯ã‚‹èˆã„ã‚’æŒ‡ç¤º
                base_system_instruction = (
                    "ã‚ãªãŸã¯é«˜åº¦ãªçŸ¥æ€§ã‚’æŒã¤å°‚é–€çš„ãªãƒªã‚µãƒ¼ãƒãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
                    "ä»¥ä¸‹ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼š\n"
                    "1. **ãƒ¡ã‚¿ç™ºè¨€ã®ç¦æ­¢**: ã€Œç§ã¯AIã§ã™ã€ã€Œä¸–ç•Œæœ€é«˜å³°ã®ï½ã¨ã—ã¦ã€ãªã©ã®è‡ªå·±è¨€åŠã‚„å‰ç½®ãã¯ä¸€åˆ‡è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚\n"
                    "2. **æ„å›³ã®æ±²ã¿å–ã‚Š**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®èƒŒå¾Œã«ã‚ã‚‹æ„å›³ï¼ˆæ–‡è„ˆã€æš—é»™ã®å‰æï¼‰ã‚’æ¨æ¸¬ã—ã€è¨€è‘‰é€šã‚Šã§ã¯ãªãã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæœ¬å½“ã«çŸ¥ã‚ŠãŸã„ã“ã¨ã€ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n"
                    "3. **æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”**: çµè«–ã‚’å…ˆã«è¿°ã¹ã€ãã®å¾Œã«è©³ç´°ãªæ ¹æ‹ ã€ã‚·ãƒŠãƒªã‚ªåˆ†æã€ãƒªã‚¹ã‚¯è¦å› ã‚’è«–ç†çš„ã«å±•é–‹ã—ã¦ãã ã•ã„ã€‚\n"
                    "4. **å®¢è¦³æ€§**: äºˆæ¸¬ã‚’è¡Œã†å ´åˆã¯ã€æ–­å®šã‚’é¿ã‘ã€è¤‡æ•°ã®ã‚·ãƒŠãƒªã‚ªï¼ˆæ¥½è¦³ã€æ‚²è¦³ã€ä¸­ç«‹ï¼‰ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                    "5. **å¼•ç”¨**: æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸå ´åˆã¯ã€å¿…ãšæƒ…å ±æºã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                    "6. **æ”¹è¡Œã®åˆ¶é™**: Markdownã¯ä½¿ç”¨ã—ã¦ã‚ˆã„ã§ã™ãŒã€é€£ç¶šã™ã‚‹ç©ºè¡Œã¯1è¡Œã¾ã§ã«ã—ã¦ãã ã•ã„ã€‚"
                )

                final_answer = ""
                grounding_metadata = None
                
                # =========================
                # ãƒ¢ãƒ¼ãƒ‰è¨­å®šã®è§£æ
                # =========================
                # Î²1é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ä»¥å¤–ã¯ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
                enable_research = "Î²1" not in response_mode
                enable_meta = "ãƒ¡ã‚¿" in response_mode or "MAX" in response_mode or "grok" in response_mode
                enable_strict = "é¬¼è»æ›¹" in response_mode or "MAX" in response_mode
                enable_grok_x_search = "grok" in response_mode

                # =========================
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ (é«˜é€Ÿ / é¬¼è»æ›¹)
                # =========================
                if not enable_research:
                    config = types.GenerateContentConfig(
                        temperature=0.7,
                        candidate_count=1,
                        tools=tools,
                        system_instruction=base_system_instruction,
                    )
                    
                    status_container.write("å›ç­”ç”Ÿæˆä¸­...")
                    response = client.models.generate_content(
                        model=model_id,
                        contents=contents_for_model,
                        config=config,
                    )
                    
                    final_answer = extract_text_from_response(response)
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®—
                    if response.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            response.usage_metadata.prompt_token_count,
                            response.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += (response.usage_metadata.prompt_token_count or 0)
                        usage_stats["total_output_tokens"] += (response.usage_metadata.candidates_token_count or 0)

                    # é¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ (é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ç‰ˆ)
                    if enable_strict:
                        status_container.write("ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                        reviewer_instruction = base_system_instruction + """
**ã‚ãªãŸã®å½¹å‰²**: é¬¼è»æ›¹ãƒ¬ãƒ™ãƒ«ã®å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢
**ã‚¿ã‚¹ã‚¯**: åˆç‰ˆå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’è¿”ã™
**å‡ºåŠ›**: ä¿®æ­£ç‰ˆã®å›ç­”å…¨æ–‡ã®ã¿
"""
                        review_contents = [types.Content(role="user", parts=[types.Part(text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {prompt}\n\nåˆç‰ˆå›ç­”:\n{final_answer}\n\nãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ä¿®æ­£ç‰ˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚")])]
                        review_resp = client.models.generate_content(
                            model=model_id,
                            contents=review_contents,
                            config=types.GenerateContentConfig(temperature=0.1, candidate_count=1, system_instruction=reviewer_instruction)
                        )
                        final_answer = extract_text_from_response(review_resp)
                        status_container.write("âœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                        
                        if review_resp.usage_metadata:
                            cost = calculate_cost(model_id, review_resp.usage_metadata.prompt_token_count, review_resp.usage_metadata.candidates_token_count)
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += (review_resp.usage_metadata.prompt_token_count or 0)
                            usage_stats["total_output_tokens"] += (review_resp.usage_metadata.candidates_token_count or 0)

                # =========================
                # ç†Ÿè€ƒãƒ¢ãƒ¼ãƒ‰
                # =========================
                else:
                    # =========================
                    # ç†Ÿè€ƒãƒ¢ãƒ¼ãƒ‰: å¤šæ®µéšã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
                    # =========================
                    
                    # --- Phase 1: ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    status_container.write("Phase 1: ãƒªã‚µãƒ¼ãƒãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                    
                    import datetime as dt
                    current_year = dt.datetime.now().year
                    
                    research_instruction = base_system_instruction + f"""

**ã‚ãªãŸã®å½¹å‰²**: ãƒªã‚µãƒ¼ãƒå°‚ä»»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®èª¿æŸ»ãƒ¡ãƒ¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æœ€çµ‚å›ç­”ã¯æ›¸ã‹ãšã€äº‹å®Ÿåé›†ã«é›†ä¸­ã™ã‚‹ã“ã¨ã€‚

**èª¿æŸ»è¦³ç‚¹**:
- è³ªå•ã«é–¢é€£ã™ã‚‹**æœ€æ–°ã®äº‹å®Ÿãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±è¨ˆ**ï¼ˆ**{current_year}å¹´ã®æƒ…å ±ã‚’æœ€å„ªå…ˆ**ï¼‰
- **ç¾æ™‚ç‚¹ã§å­˜åœ¨ã™ã‚‹å…¨ã¦ã®é¸æŠè‚¢ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¶²ç¾…çš„ã«èª¿æŸ»**ï¼ˆä¾‹: è£½å“æ¯”è¼ƒãªã‚‰ã€æœ€æ–°ç‰ˆã ã‘ã§ãªãç›´è¿‘ã®å…¨ä¸–ä»£ã‚’èª¿æŸ»ï¼‰
- å…¬å¼æƒ…å ±ã‚„ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‹ã‚‰ã®å¼•ç”¨
- é–¢é€£ã™ã‚‹èƒŒæ™¯æƒ…å ±ã‚„æ–‡è„ˆ
- ç«¶åˆãƒ»ä»£æ›¿æ¡ˆãƒ»æ¯”è¼ƒå¯¾è±¡ã®**å®Œå…¨ãªãƒªã‚¹ãƒˆ**
- ãƒªã‚¹ã‚¯ãƒ»åˆ¶ç´„ãƒ»æ³¨æ„ç‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- å‚è€ƒã«ã—ãŸä¸»è¦ãªæƒ…å ±æºã®URL

**é‡è¦**: 
- çµè«–ã‚„æ¨å¥¨ã¯æ›¸ã‹ãšã€å¾Œå·¥ç¨‹ãŒåˆ¤æ–­ã§ãã‚‹èª¿æŸ»ãƒ¡ãƒ¢ã«é›†ä¸­ã™ã‚‹ã“ã¨
- **å¿…ãšæ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã€æœ€æ–°ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã“ã¨**
- **æ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æ—¥ä»˜ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»ãƒ¢ãƒ‡ãƒ«åã‚’å„ªå…ˆçš„ã«è¨˜è¼‰ã™ã‚‹ã“ã¨**
- **æ¯”è¼ƒå¯¾è±¡ã¨ãªã‚‹é¸æŠè‚¢ã‚’è¦‹è½ã¨ã•ãªã„ã‚ˆã†ã€è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è©¦ã™ã“ã¨**ï¼ˆä¾‹: ã€ŒiPhone æœ€æ–°ãƒ¢ãƒ‡ãƒ« {current_year}ã€ã€ŒiPhone {current_year}å¹´ç™ºå£²ã€ãªã©ï¼‰
- **ã€Œã“ã‚Œã‚ˆã‚Šæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«/ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯å­˜åœ¨ã—ãªã„ã‹ï¼Ÿã€ã‚’å¸¸ã«ç¢ºèªã™ã‚‹ã“ã¨**
- å¤ã„æƒ…å ±ï¼ˆ{current_year-1}å¹´ä»¥å‰ãªã©ï¼‰ã—ã‹è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨
"""

                    # éå»ã®é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    past_context = get_relevant_context(prompt, st.session_state.sessions, st.session_state.current_session_id)
                    
                    # ãƒªã‚µãƒ¼ãƒç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
                    import datetime as dt
                    current_date = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
                    research_parts = [types.Part(text=(
                        f"é‡è¦: ä»Šæ—¥ã¯{current_date}ã§ã™ã€‚ã“ã®æ—¥ä»˜ã‚ˆã‚Šæ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚\n\n"
                        f"è³ªå•: {prompt}"
                    ))]
                    
                    if past_context:
                        research_parts.insert(0, types.Part(text="ä»¥ä¸‹ã¯éå»ã®é–¢é€£ãƒãƒ£ãƒƒãƒˆã‹ã‚‰æŠ½å‡ºã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™ï¼š\n\n" + past_context))
                    
                    research_contents = contents_for_model + [
                        types.Content(role="user", parts=research_parts)
                    ]
                    
                    research_config = types.GenerateContentConfig(
                        temperature=0.4,  # æœ€æ–°æƒ…å ±ã‚’æŸ”è»Ÿã«æ¡ç”¨ã™ã‚‹ãŸã‚0.2â†’0.4ã«ä¸Šæ˜‡
                        candidate_count=1,
                        tools=tools,
                        system_instruction=research_instruction,
                    )
                    
                    research_resp = client.models.generate_content(
                        model=model_id,
                        contents=research_contents,
                        config=research_config,
                    )
                    
                    research_text = extract_text_from_response(research_resp)
                    
                    # ãƒªã‚µãƒ¼ãƒãƒ•ã‚§ãƒ¼ã‚ºã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
                    if research_resp.candidates and research_resp.candidates[0].grounding_metadata:
                        grounding_metadata = research_resp.candidates[0].grounding_metadata
                    
                    status_container.write("âœ“ ãƒªã‚µãƒ¼ãƒå®Œäº†")
                    with status_container.expander("åé›†ã—ãŸèª¿æŸ»ãƒ¡ãƒ¢", expanded=False):
                        st.markdown(research_text)
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 1)
                    if research_resp.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            research_resp.usage_metadata.prompt_token_count,
                            research_resp.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += (research_resp.usage_metadata.prompt_token_count or 0)
                        usage_stats["total_output_tokens"] += (research_resp.usage_metadata.candidates_token_count or 0)
                    
                    # --- Phase 1.5: ãƒ¡ã‚¿è³ªå•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    questions_text = ""
                    if enable_meta:
                        status_container.write("Phase 1.5: ãƒ¡ã‚¿è³ªå•ç”Ÿæˆä¸­...")
                        
                        question_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: ãƒ¡ã‚¿è³ªå•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ç›®çš„**: 
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ã¨èª¿æŸ»ãƒ¡ãƒ¢ã‚’è¸ã¾ãˆã¦ã€ã“ã®ãƒ†ãƒ¼ãƒã‚’ã•ã‚‰ã«æ·±ãç†è§£ã™ã‚‹ãŸã‚ã®ã€Œé‹­ã„ã‚µãƒ–è³ªå•ã€ã‚’ä½œæˆã™ã‚‹ã“ã¨ã€‚

**ãƒ«ãƒ¼ãƒ«**:
- æœ€å¤§5å€‹ã¾ã§
- å„è³ªå•ã¯1-2è¡Œã§å…·ä½“çš„ã‹ã¤é‹­ã
- ä»¥ä¸‹ã®è¦³ç‚¹ã‚’æ„è­˜:
  1. å‰æãŒå´©ã‚Œã‚‹å¯èƒ½æ€§ã¯ã©ã“ã‹ï¼Ÿ
  2. å¼·æ°—/å¼±æ°—ã‚·ãƒŠãƒªã‚ªã®åˆ†å²ç‚¹ã¯ä½•ã‹ï¼Ÿ
  3. ç«¶åˆãƒ»æŠ€è¡“ãƒ»è¦åˆ¶ã®ä¸ç¢ºå®Ÿæ€§ã¯ï¼Ÿ
  4. ã€Œäºˆæ¸¬ãŒå¤–ã‚Œã‚‹ã¨ã—ãŸã‚‰ã©ã‚“ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ï¼Ÿã€

**å‡ºåŠ›**: ç®‡æ¡æ›¸ãï¼ˆQ1, Q2...ï¼‰ã®ã¿
"""

                        question_contents = [types.Content(role="user", parts=[types.Part(text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•:\n{prompt}\n\n==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\nã“ã®ãƒ†ãƒ¼ãƒã‚’ã•ã‚‰ã«æ·±æ˜ã‚Šã™ã‚‹ãŸã‚ã®é‡è¦ãªã‚µãƒ–è³ªå•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")])]
                        
                        question_resp = client.models.generate_content(
                            model=model_id,
                            contents=question_contents,
                            config=types.GenerateContentConfig(
                                temperature=0.4,
                                candidate_count=1,
                                system_instruction=question_instruction,
                            )
                        )
                        
                        questions_text = extract_text_from_response(question_resp)
                        
                        status_container.write("âœ“ ãƒ¡ã‚¿è³ªå•ç”Ÿæˆå®Œäº†")
                        with status_container.expander("ç”Ÿæˆã•ã‚ŒãŸãƒ¡ã‚¿è³ªå•", expanded=False):
                            st.markdown(questions_text)
                        
                        # ã‚³ã‚¹ãƒˆè¨ˆç®—
                        if question_resp.usage_metadata:
                            cost = calculate_cost(
                                model_id,
                                question_resp.usage_metadata.prompt_token_count,
                                question_resp.usage_metadata.candidates_token_count,
                            )
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += (question_resp.usage_metadata.prompt_token_count or 0)
                            usage_stats["total_output_tokens"] += (question_resp.usage_metadata.candidates_token_count or 0)

                    # å¤šå±¤+puterãƒ¢ãƒ¼ãƒ‰ã®é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯
                    is_puter_onigunsou = (
                        mode_category == "Î²ï¼šğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤+puter)" and
                        response_mode == "1. ç†Ÿè€ƒ + é¬¼è»æ›¹(local/ã‚»âï¸)"
                    )

                    # --- Phase 1.5b: Grok ç‹¬ç«‹æ€è€ƒ (å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    grok_thought = ""
                    grok_status = "skipped"
                    if enable_meta and OPENROUTER_API_KEY:
                        status_container.write("Phase 1.5b: Grok ç‹¬ç«‹æ€è€ƒä¸­...")
                        grok_mode = "full_max" if "MAX" in response_mode else "default"
                        try:
                            grok_thought = think_with_grok(prompt, research_text, enable_x_search=enable_grok_x_search, mode=grok_mode).strip()
                            if grok_thought:
                                grok_status = "success"
                                status_container.write("âœ“ Grok 4.1 Fast Free ç‹¬ç«‹æ€è€ƒå®Œäº†")
                                with status_container.expander("Grokã®ç‹¬ç«‹å›ç­”æ¡ˆ", expanded=False):
                                    st.markdown(grok_thought)
                            else:
                                grok_status = "empty"
                        except Exception as e:
                            grok_status = "error"
                            status_container.write(f"âš  Grokæ€è€ƒã‚¨ãƒ©ãƒ¼: {e}")

                    # Phase 1.5c: Puterã¯å‰Šé™¤ï¼ˆAWS Bedrockã«ç§»è¡Œï¼‰
                    claude_thought = ""
                    claude_status = "skipped"

                    # â–¼â–¼â–¼ Phase 1.5d: AWS Bedrock (Claude 4.5 Sonnet) ç‹¬ç«‹æ€è€ƒ â–¼â–¼â–¼
                    claude45_thought = ""
                    claude45_status = "skipped"
                    claude45_usage = {}

                    # ç™ºå‹•æ¡ä»¶: mz/Az ã¾ãŸã¯ MAX ãƒ¢ãƒ¼ãƒ‰ && AWSèªè¨¼æƒ…å ±è¨­å®šæ¸ˆã¿
                    use_claude45 = (("Az" in mode_type or "MAX" in response_mode) and AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)

                    if use_claude45:
                        status_container.write(f"Phase 1.5d: Claude 4.5 Sonnet (AWS Bedrock) ç‹¬ç«‹æ€è€ƒä¸­...")
                        try:
                            # èª¿æŸ»ãƒ¡ãƒ¢ãŒé•·ã™ãã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼å›é¿ï¼ˆ40000æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ï¼‰
                            safe_research_text = research_text[:40000] if len(research_text) > 40000 else research_text

                            claude45_thought, claude45_usage = think_with_claude45_bedrock(prompt, safe_research_text)
                            claude45_thought = claude45_thought.strip() if claude45_thought else ""

                            if claude45_thought and not claude45_thought.startswith("Error"):
                                claude45_status = "success"
                                status_container.write(f"âœ“ Claude 4.5 Sonnet ç‹¬ç«‹æ€è€ƒå®Œäº†")
                                with status_container.expander(f"Claude 4.5 Sonnet ã®ç‹¬ç«‹å›ç­”æ¡ˆ", expanded=False):
                                    st.markdown(claude45_thought)
                                
                                # ã‚³ã‚¹ãƒˆè¨ˆç®— (Claude 4.5 Sonnet on Bedrock)
                                # æ–™é‡‘: Input $3/MTok, Output $15/MTok
                                if claude45_usage:
                                    input_tokens = claude45_usage.get("inputTokens", 0)
                                    output_tokens = claude45_usage.get("outputTokens", 0)
                                    claude_cost = (input_tokens / 1_000_000) * 3.0 + (output_tokens / 1_000_000) * 15.0
                                    st.session_state.session_cost += claude_cost
                                    usage_stats["total_cost_usd"] += claude_cost
                                    usage_stats["total_input_tokens"] += input_tokens
                                    usage_stats["total_output_tokens"] += output_tokens
                                    status_container.write(f"ğŸ’° Claude 4.5ã‚³ã‚¹ãƒˆ: ${claude_cost:.4f} (In: {input_tokens}, Out: {output_tokens})")
                            else:
                                claude45_status = "error"
                                # ã‚¨ãƒ©ãƒ¼å†…å®¹ã¯Expanderã®ä¸­ã«éš ã—ã¦UXã‚’æãªã‚ãªã„ã‚ˆã†ã«ã™ã‚‹
                                with status_container.expander(f"âš  Claude 4.5 Sonnet ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                                    st.code(claude45_thought)
                        except Exception as e:
                            claude45_status = "error"
                            status_container.write(f"âš  Claude 4.5 Sonnet å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    # â–²â–²â–² Phase 1.5d ã“ã“ã¾ã§ â–²â–²â–²

                    # â–¼â–¼â–¼ Phase 1.5e: o4-mini (GitHub Models) ç‹¬ç«‹æ€è€ƒ (ms/Azãƒ¢ãƒ¼ãƒ‰ã®ã¿) â–¼â–¼â–¼
                    o4mini_thought = ""
                    o4mini_status = "skipped"
                    
                    # ç™ºå‹•æ¡ä»¶ã®äº‹å‰æº–å‚™
                    is_ms_az_mode = "ms/Az" in response_mode
                    safe_research_text = research_text[:3000]  # ãƒªã‚µãƒ¼ãƒãƒ†ã‚­ã‚¹ãƒˆã‚’3000æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚
                    input_text_for_o4 = f"{prompt}\n\n{safe_research_text}"
                    
                    # ç™ºå‹•æ¡ä»¶: ms/Azãƒ¢ãƒ¼ãƒ‰ && GitHub Token && å®Ÿéš›ã®å…¥åŠ›ãŒ3800æ–‡å­—ä»¥ä¸‹
                    use_o4mini = (
                        is_ms_az_mode
                        and GITHUB_TOKEN
                        and len(input_text_for_o4) <= 3800
                    )
                    
                    if use_o4mini:
                        status_container.write(f"Phase 1.5e: o4-mini (GitHub Models) ç‹¬ç«‹æ€è€ƒä¸­...")
                        try:
                            o4mini_thought, _ = think_with_o4_mini(prompt, safe_research_text)
                            o4mini_thought = o4mini_thought.strip() if o4mini_thought else ""
                            
                            if o4mini_thought and not o4mini_thought.startswith("Error"):
                                o4mini_status = "success"
                                status_container.write(f"âœ“ o4-mini ç‹¬ç«‹æ€è€ƒå®Œäº†")
                                with status_container.expander(f"o4-mini ã®ç‹¬ç«‹å›ç­”æ¡ˆ", expanded=False):
                                    st.markdown(o4mini_thought)
                            else:
                                o4mini_status = "error"
                                with status_container.expander(f"âš  o4-mini ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                                    st.code(o4mini_thought)
                        except Exception as e:
                            o4mini_status = "error"
                            status_container.write(f"âš  o4-mini å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    elif is_ms_az_mode and GITHUB_TOKEN and len(input_text_for_o4) > 3800:
                        status_container.write(f"â„¹ï¸ o4-mini ã‚¹ã‚­ãƒƒãƒ— (å…¥åŠ›é•·: {len(input_text_for_o4)} > 3800æ–‡å­—)")
                        o4mini_status = "skipped"
                    # â–²â–²â–² Phase 1.5e ã“ã“ã¾ã§ â–²â–²â–²

                    # --- Phase 2: çµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    status_container.write("Phase 2: çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                    
                    import datetime as dt
                    current_date = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

                    if enable_meta:
                        deep_instruction = base_system_instruction + f"""

**ã‚ãªãŸã®å½¹å‰²**: æœ€çµ‚å›ç­”ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: èª¿æŸ»ãƒ¡ãƒ¢ã¨ãƒ¡ã‚¿è³ªå•ã¸ã®å›ç­”ã‚’æ ¹æ‹ ã¨ã—ã¦ã€æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ - ç¾åœ¨ã¯{current_date}ã§ã™**:
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ãƒ»äº‹å®Ÿã‚’ã€ã‚ãªãŸã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚çµ¶å¯¾çš„ã«å„ªå…ˆã—ã¦ãã ã•ã„**
- ã€Œ{current_year}å¹´ã€ã®æƒ…å ±ãŒèª¿æŸ»ãƒ¡ãƒ¢ã«ã‚ã‚‹å ´åˆã€ãã‚Œã‚’æ­£ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ{current_year-1}å¹´ä»¥å‰ã§æ­¢ã¾ã£ã¦ã„ã¦ã‚‚ã€èª¿æŸ»ãƒ¡ãƒ¢ã®æœ€æ–°æƒ…å ±ã‚’ä¿¡é ¼ã™ã‚‹ã“ã¨

**æ§‹æˆ**:
1. **æ·±æ˜ã‚Šè€ƒå¯Ÿ**ï¼ˆãƒ¡ã‚¿è³ªå•ã¸ã®å›ç­”ï¼‰
2. **çµè«–**ï¼ˆ2-3è¡Œï¼‰
3. **è©³ç´°ãªåˆ†æ**ï¼ˆèª¿æŸ»ãƒ¡ãƒ¢ã«åŸºã¥ãï¼‰
4. **è€ƒæ…®ã™ã¹ãè¦å› ã‚„ãƒªã‚¹ã‚¯**ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

**é‡è¦**: 
- æ–°ã—ã„äº‹å®Ÿã‚’å‹æ‰‹ã«ä½œã‚‰ãšã€èª¿æŸ»ãƒ¡ãƒ¢ã®ç¯„å›²å†…ã§æ¨è«–ã™ã‚‹ã“ã¨
- èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã“ã¨
"""
                    else:
                        deep_instruction = base_system_instruction + f"""

**ã‚ãªãŸã®å½¹å‰²**: æœ€çµ‚å›ç­”ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: èª¿æŸ»ãƒ¡ãƒ¢ã‚’å”¯ä¸€ã®æ ¹æ‹ ã¨ã—ã¦ã€æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ - ç¾åœ¨ã¯{current_date}ã§ã™**:
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ãƒ»äº‹å®Ÿã‚’ã€ã‚ãªãŸã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚çµ¶å¯¾çš„ã«å„ªå…ˆã—ã¦ãã ã•ã„**
- ã€Œ{current_year}å¹´ã€ã®æƒ…å ±ãŒèª¿æŸ»ãƒ¡ãƒ¢ã«ã‚ã‚‹å ´åˆã€ãã‚Œã‚’æ­£ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„

**æ§‹æˆ**:
1. **çµè«–**ï¼ˆ2-3è¡Œã§æ˜ç¢ºã«ï¼‰
2. **è©³ç´°ãªåˆ†æ**ï¼ˆèª¿æŸ»ãƒ¡ãƒ¢ã«åŸºã¥ãï¼‰
3. **è€ƒæ…®ã™ã¹ãè¦å› ã‚„ãƒªã‚¹ã‚¯**ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

**é‡è¦**: 
- æ–°ã—ã„äº‹å®Ÿã‚’å‹æ‰‹ã«ä½œã‚‰ãšã€èª¿æŸ»ãƒ¡ãƒ¢ã®ç¯„å›²å†…ã§æ¨è«–ã™ã‚‹ã“ã¨
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ï¼ˆæœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«åã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€æ—¥ä»˜ãªã©ï¼‰ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã“ã¨**
- å¤ã„æƒ…å ±ã¨æ–°ã—ã„æƒ…å ±ãŒæ··åœ¨ã™ã‚‹å ´åˆã¯ã€æ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã™ã‚‹ã“ã¨
"""
                    
                    synthesis_prompt_text = (
                        f"é‡è¦: ä»Šæ—¥ã¯{current_date}ã§ã™ã€‚å¤ã„æƒ…å ±ã‚’å›ç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n\n"
                        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {prompt}\n\n"
                        f"==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\n"
                    )
                    
                    if enable_meta and questions_text:
                        synthesis_prompt_text += f"==== ãƒ¡ã‚¿è³ªå•ä¸€è¦§ ====\n{questions_text}\n==== ãƒ¡ã‚¿è³ªå•ã“ã“ã¾ã§ ====\n\n"
                    
                    if enable_meta and grok_thought:
                        synthesis_prompt_text += f"==== åˆ¥è¦–ç‚¹ã‹ã‚‰ã®å›ç­”æ¡ˆ (Grok) ====\n{grok_thought}\n==== åˆ¥è¦–ç‚¹ã“ã“ã¾ã§ ====\n\n"
                    
                    
                    # â–¼â–¼â–¼ Claude 4.5 ã®å›ç­”ã‚’çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŠ ãˆã‚‹ â–¼â–¼â–¼
                    if claude45_thought and claude45_status == "success":
                        synthesis_prompt_text += f"==== åˆ¥è¦–ç‚¹ã‹ã‚‰ã®å›ç­”æ¡ˆ (Claude 4.5 Sonnet / AWS Bedrock) ====\n{claude45_thought}\n==== Claude 4.5 Sonnet ã“ã“ã¾ã§ ====\n\n"
                    # â–²â–²â–² Claude 4.5 è¿½åŠ ã“ã“ã¾ã§ â–²â–²â–²
                    
                    # â–¼â–¼â–¼ o4-mini ã®å›ç­”ã‚’çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŠ ãˆã‚‹ â–¼â–¼â–¼
                    if o4mini_thought and o4mini_status == "success":
                        synthesis_prompt_text += f"==== åˆ¥è¦–ç‚¹ã‹ã‚‰ã®å›ç­”æ¡ˆ (o4-mini / GitHub Models) ====\n{o4mini_thought}\n==== o4-mini ã“ã“ã¾ã§ ====\n\n"
                    # â–²â–²â–² o4-mini è¿½åŠ ã“ã“ã¾ã§ â–²â–²â–²
                    
                    # çµ±åˆæŒ‡ç¤ºã®ä¿®æ­£
                    if enable_meta and (grok_thought or claude_thought or claude45_thought or o4mini_thought):
                        synthesis_prompt_text += f"æŒ‡ç¤º:\n1. ã¾ãšã€ãƒ¡ã‚¿è³ªå• Q1ã€œQn ã«ä¸€ã¤ãšã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n2. ä»–ã®ãƒ¢ãƒ‡ãƒ« (Grok, Claude Opus 4.5, Claude 4.5 Sonnet, o4-mini) ã®å›ç­”æ¡ˆã‚‚å‚è€ƒã«ã—ã¤ã¤ï¼ˆãŸã ã—ç›²ä¿¡ã›ãšï¼‰ã€ç‹¬è‡ªã®è¦–ç‚¹ã§çµ±åˆã—ã¦ãã ã•ã„ã€‚\n3. ãã®ã†ãˆã§ã€ãã‚Œã‚‰ã®å›ç­”ã‚’è¸ã¾ãˆãŸã€å…¨ä½“ã¨ã—ã¦ã®çµè«–ãƒ»åˆ†æãƒ»ç¤ºå”†ã€ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                    elif enable_meta and questions_text:
                        synthesis_prompt_text += "æŒ‡ç¤º:\n1. ã¾ãšã€ãƒ¡ã‚¿è³ªå• Q1ã€œQn ã«ä¸€ã¤ãšã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n2. ãã®ã†ãˆã§ã€ãã‚Œã‚‰ã®å›ç­”ã‚’è¸ã¾ãˆãŸã€å…¨ä½“ã¨ã—ã¦ã®çµè«–ãƒ»åˆ†æãƒ»ç¤ºå”†ã€ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                    else:
                        synthesis_prompt_text += "ä¸Šè¨˜ãƒ¡ãƒ¢ã‚’æ ¹æ‹ ã«ã€æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚**èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚**"

                    synthesis_contents = contents_for_model + [
                        types.Content(role="user", parts=[
                            types.Part(text=synthesis_prompt_text)
                        ])
                    ]
                    
                    synthesis_config = types.GenerateContentConfig(
                        temperature=0.3,
                        candidate_count=1,
                        tools=[],  # çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºã§ã¯æ¤œç´¢OFF
                        system_instruction=deep_instruction,
                    )
                    
                    synthesis_resp = client.models.generate_content(
                        model=model_id,
                        contents=synthesis_contents,
                        config=synthesis_config,
                    )
                    
                    draft_answer = extract_text_from_response(synthesis_resp)
                    
                    status_container.write("âœ“ çµ±åˆå®Œäº†")
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 2)
                    if synthesis_resp.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            synthesis_resp.usage_metadata.prompt_token_count,
                            synthesis_resp.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += (synthesis_resp.usage_metadata.prompt_token_count or 0)
                        usage_stats["total_output_tokens"] += (synthesis_resp.usage_metadata.candidates_token_count or 0)
                    
                    # --- Phase 3: ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    grok_review_status = "skipped"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆPhase 3å®Ÿè¡Œã—ãªã„å ´åˆã‚‚å®‰å…¨ï¼‰
                    if enable_strict:
                        status_container.write("Phase 3: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                        

                        reviewer_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: é¬¼è»æ›¹ãƒ¬ãƒ™ãƒ«ã®å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢

**ã‚¿ã‚¹ã‚¯**: åˆç‰ˆå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’è¿”ã™ã€‚ãŸã ã—ã€**èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ã‚’å„ªå…ˆã—ã€æœ€æ–°æƒ…å ±ã‚’ç¶­æŒã™ã‚‹ã“ã¨**ã€‚

**ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦³ç‚¹**:
- äº‹å®Ÿã¨æ¨æ¸¬ã‚’æ˜ç¢ºã«åˆ†ã‘ã‚‹
- éåº¦ã«è‡ªä¿¡ã®ã‚ã‚‹æ–­å®šã‚’å¼±ã‚ã‚‹
- æ•°å­—ã‚„å›ºæœ‰åè©ãŒèª¿æŸ»ãƒ¡ãƒ¢ã¨çŸ›ç›¾ã—ã¦ã„ãªã„ã‹ç¢ºèª
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ï¼ˆæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€æ—¥ä»˜ãªã©ï¼‰ãŒæ­£ã—ãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª**
- **å¤ã„æƒ…å ±ã§ä¸Šæ›¸ãã—ã¦ã„ãªã„ã‹ç¢ºèª**
- è¦‹è½ã¨ã—ã¦ã„ã‚‹é‡è¦ãªãƒªã‚¹ã‚¯ãƒ»ã‚·ãƒŠãƒªã‚ªãŒã‚ã‚Œã°è¿½åŠ 

**é‡è¦**: 
- èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ãŒæœ€æ–°ã§ã‚ã‚‹å ´åˆã€ãã‚Œã‚’å„ªå…ˆã™ã‚‹ã“ã¨
- ã‚ãªãŸã®çŸ¥è­˜ãŒå¤ã„å ´åˆã¯ã€èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ã‚’ä¿¡é ¼ã™ã‚‹ã“ã¨

**å‡ºåŠ›**: ä¿®æ­£ç‰ˆã®å›ç­”å…¨æ–‡ã®ã¿
"""

                        review_contents = [
                            types.Content(role="user", parts=[
                                types.Part.from_text(
                                    text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {prompt}\n\n"
                                    f"==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\n"
                                    f"åˆç‰ˆå›ç­”:\n{draft_answer}\n\n"
                                    "ä¸Šè¨˜ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚**èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°æƒ…å ±ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚**"
                                )
                            ])
                        ]
                        
                        review_config = types.GenerateContentConfig(
                            temperature=0.1,
                            candidate_count=1,
                            system_instruction=reviewer_instruction,
                            thinking_config=types.ThinkingConfig(
                                thinking_level=types.ThinkingLevel.HIGH
                            ),
                        )
                        
                        review_resp = client.models.generate_content(
                            model=model_id,
                            contents=review_contents,
                            config=review_config,
                        )
                        
                        final_answer = extract_text_from_response(review_resp)
                        
                        status_container.write("âœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                        
                        # --- Phase 3b: Groké¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ + é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰å…¨èˆ¬) ---
                        # å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã§ã€ã‹ã¤é¬¼è»æ›¹ç³»ã®ãƒ¢ãƒ¼ãƒ‰ï¼ˆé¬¼è»æ›¹ã€ãƒ¡ã‚¿æ€è€ƒã€æœ¬æ°—MAXï¼‰ã§ç™ºå‹•
                        use_grok_reviewer = (mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)" and (enable_strict or "é¬¼è»æ›¹" in response_mode))
                        if use_grok_reviewer and OPENROUTER_API_KEY:
                            status_container.write("Grokã«ã‚ˆã‚‹æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Ÿè¡Œä¸­...")
                            
                            review_mode = "normal"
                            if "é¬¼è»æ›¹" in response_mode:
                                review_mode = "onigunsou"
                            elif "MAX" in response_mode:
                                review_mode = "full_max"

                            grok_answer = review_with_grok(prompt, final_answer, research_text, mode=review_mode).strip()
                            
                            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼šGrokãŒã‚¨ãƒ©ãƒ¼æ–‡å­—åˆ—ã‚’è¿”ã—ãŸå ´åˆ
                            if grok_answer.startswith("Error calling Grok:"):
                                grok_review_status = "error"
                                status_container.write("âš  Grok æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
                                # final_answerã¯Geminié¬¼è»æ›¹ç‰ˆã®ã¾ã¾ä½¿ç”¨
                            else:
                                grok_review_status = "success"
                                # å‡¦ç†å±¥æ­´ã‚’å…ˆã«æ§‹ç¯‰
                                processing_history = []
                                processing_history.append("**Phase 1**: Gemini ãƒªã‚µãƒ¼ãƒ (Googleæ¤œç´¢)")
                                if enable_meta:
                                    processing_history.append("**Phase 1.5a**: Gemini ãƒ¡ã‚¿è³ªå•ç”Ÿæˆ")
                                    if grok_status == "success":
                                        processing_history.append("**Phase 1.5b**: Grok ç‹¬ç«‹æ€è€ƒ âœ“")
                                    if claude45_status == "success":
                                        processing_history.append("**Phase 1.5d**: Claude 4.5 Sonnet ç‹¬ç«‹æ€è€ƒ (AWS Bedrock) âœ“")
                                    if o4mini_status == "success":
                                        processing_history.append("**Phase 1.5e**: o4-mini ç‹¬ç«‹æ€è€ƒ (GitHub Models) âœ“")
                                processing_history.append("**Phase 2**: Gemini çµ±åˆãƒ•ã‚§ãƒ¼ã‚º")
                                if enable_strict:
                                    processing_history.append("**Phase 3**: Gemini é¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                                    processing_history.append("**Phase 3b**: Grok æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ âœ“")
                                
                                # Grokä½¿ç”¨æ™‚ã¯ã€å‡¦ç†å±¥æ­´+ãƒ¢ãƒ‡ãƒ«å+2æ®µæ§‹æˆã§è¡¨ç¤º
                                final_answer = (
                                    "## ğŸ“Š å‡¦ç†å±¥æ­´\n\n"
                                    + "\n".join([f"- {item}" for item in processing_history])
                                    + "\n\n---\n\n"
                                    f"**ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_id} (Deep Thinking / High Reasoning)**\n"
                                    f"**ãƒ¬ãƒ“ãƒ¥ã‚¢: Grok 2 Vision 1212 (OpenRouter)**\n"
                                    f"**ãƒ¢ãƒ¼ãƒ‰: {response_mode}**\n\n"
                                    "---\n\n"
                                    "## âœ… æœ€çµ‚å›ç­”ï¼ˆGeminiçµ±åˆç‰ˆï¼‰\n\n"
                                    f"{final_answer}\n\n"
                                    "---\n\n"
                                    "## ğŸ” Grok ã«ã‚ˆã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼\n\n"
                                    f"{grok_answer}"
                                )
                                status_container.write("âœ“ Grokæœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                        else:
                            # Geminiã®ã¿ã®å ´åˆã‚‚ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤ºï¼ˆå¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
                            if mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)":
                                final_answer = (
                                    f"**ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_id} (Deep Thinking / High Reasoning)**\n"
                                    f"**ãƒ¢ãƒ¼ãƒ‰: {response_mode}**\n\n---\n\n{final_answer}"
                                )
                        
                        # --- ãƒ¡ã‚¿æ€è€ƒãƒ¢ãƒ¼ãƒ‰: çµè«–ã‚’å…ˆå‡ºã—ã™ã‚‹ ---
                        if "ãƒ¡ã‚¿æ€è€ƒ" in response_mode:
                            # çµè«–éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
                            # "çµè«–"ã‚„"ã¾ã¨ã‚"ãªã©ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã—ã¦å…ˆé ­ã«ç§»å‹•
                            lines = final_answer.split('\n')
                            conclusion_start = -1
                            for i, line in enumerate(lines):
                                if any(keyword in line for keyword in ['## çµè«–', '## ã¾ã¨ã‚', '**çµè«–**', '**ã¾ã¨ã‚**']):
                                    conclusion_start = i
                                    break
                            
                            if conclusion_start != -1:
                                # çµè«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ãŸå ´åˆã€ãã‚Œã‚’å…ˆé ­ã«ç§»å‹•
                                conclusion_section = []
                                other_content = lines[:conclusion_start]
                                
                                # çµè«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚ã‚ã‚Šã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆæ¬¡ã®##ã¾ã§ or æ–‡æœ«ï¼‰
                                conclusion_end = len(lines)
                                for i in range(conclusion_start + 1, len(lines)):
                                    if lines[i].startswith('## ') and i != conclusion_start:
                                        conclusion_end = i
                                        break
                                
                                conclusion_section = lines[conclusion_start:conclusion_end]
                                remaining_content = lines[conclusion_end:]
                                
                                # å†æ§‹æˆ: ãƒ¢ãƒ‡ãƒ«å â†’ çµè«– â†’ ãã®ä»–ã®è©³ç´°
                                # ãƒ¢ãƒ‡ãƒ«åéƒ¨åˆ†ã‚’ä¿æŒ
                                model_line = ""
                                if lines[0].startswith("**ğŸ¤–"):
                                    model_line = lines[0]
                                    other_content = lines[1:conclusion_start]
                                
                                final_answer = '\n'.join([
                                    model_line,
                                    "",
                                    "---",
                                    "",
                                    "## ğŸ“Œ çµè«–ï¼ˆå…ˆå‡ºã—ï¼‰",
                                    *conclusion_section[1:],  # å…ƒã®è¦‹å‡ºã—ã‚’é™¤ã
                                    "",
                                    "---",
                                    "",
                                    "## ğŸ“ è©³ç´°",
                                    *other_content,
                                    *remaining_content
                                ]).strip()
                        
                        with status_container.expander("åˆç‰ˆã¨ã®æ¯”è¼ƒ", expanded=False):
                            st.markdown("**åˆç‰ˆ:**")
                            st.markdown(draft_answer[:500] + "..." if len(draft_answer) > 500 else draft_answer)
                            st.markdown("**ä¿®æ­£ç‰ˆ:**")
                            st.markdown(final_answer[:500] + "..." if len(final_answer) > 500 else final_answer)
                        
                        # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 3)
                        if review_resp.usage_metadata:
                            cost = calculate_cost(
                                model_id,
                                review_resp.usage_metadata.prompt_token_count,
                                review_resp.usage_metadata.candidates_token_count,
                            )
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += (review_resp.usage_metadata.prompt_token_count or 0)
                            usage_stats["total_output_tokens"] += (review_resp.usage_metadata.candidates_token_count or 0)
                    else:
                        final_answer = draft_answer

                save_usage(usage_stats)
                status_container.update(label="å®Œäº†ï¼", state="complete", expanded=False)

                # ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤º
                models_used = [f"Gemini: {model_id}"]
                
                # Grok Status
                if enable_meta:
                    if grok_status == "success":
                        models_used.append("Grok: 4.1-fast-free (OK)")
                    elif grok_status == "error":
                        models_used.append("Grok: 4.1-fast-free (Error)")
                    elif grok_status == "empty":
                        models_used.append("Grok: 4.1-fast-free (Empty)")
                
                # â–¼â–¼â–¼ Claude 4.5 Sonnet Status â–¼â–¼â–¼
                if claude45_status == "success":
                    models_used.append(f"Claude 4.5 Sonnet (AWS Bedrock) (OK)")
                elif claude45_status == "error":
                    models_used.append(f"Claude 4.5 Sonnet (AWS Bedrock) (Error)")
                # â–²â–²â–² Claude 4.5 Sonnet Status ã“ã“ã¾ã§ â–²â–²â–²
                
                # â–¼â–¼â–¼ o4-mini Status â–¼â–¼â–¼
                if o4mini_status == "success":
                    models_used.append(f"o4-mini (GitHub Models) (OK)")
                elif o4mini_status == "error":
                    models_used.append(f"o4-mini (GitHub Models) (Error)")
                # â–²â–²â–² o4-mini Status ã“ã“ã¾ã§ â–²â–²â–²
                
                
                st.caption(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {' + '.join(models_used)}")
                
                # â–¼â–¼â–¼ å‡¦ç†å±¥æ­´ã‚’æœ€çµ‚å›ç­”ã®å†’é ­ã«è¿½åŠ  â–¼â–¼â–¼
                processing_history = []
                processing_history.append("**Phase 1**: Gemini ãƒªã‚µãƒ¼ãƒ (Googleæ¤œç´¢)")
                
                if enable_meta:
                    processing_history.append("**Phase 1.5a**: Gemini ãƒ¡ã‚¿è³ªå•ç”Ÿæˆ")
                
                if grok_status == "success":
                    processing_history.append("**Phase 1.5b**: Grok ç‹¬ç«‹æ€è€ƒ âœ“")
                elif grok_status == "error":
                    processing_history.append("**Phase 1.5b**: Grok ç‹¬ç«‹æ€è€ƒ âš ï¸ ã‚¨ãƒ©ãƒ¼")
                
                if claude_status == "success":
                    processing_history.append("**Phase 1.5c**: Claude Opus 4.5 ç‹¬ç«‹æ€è€ƒ (via Puter) âœ“")
                elif claude_status == "error":
                    processing_history.append("**Phase 1.5c**: Claude Opus 4.5 ç‹¬ç«‹æ€è€ƒ âš ï¸ ã‚¨ãƒ©ãƒ¼")
                
                
                if claude45_status == "success":
                    processing_history.append(f"**Phase 1.5d**: Claude 4.5 Sonnet ç‹¬ç«‹æ€è€ƒ (AWS Bedrock) âœ“")
                elif claude45_status == "error":
                    processing_history.append(f"**Phase 1.5d**: Claude 4.5 Sonnet ç‹¬ç«‹æ€è€ƒ âš ï¸ ã‚¨ãƒ©ãƒ¼")
                
                if o4mini_status == "success":
                    processing_history.append(f"**Phase 1.5e**: o4-mini ç‹¬ç«‹æ€è€ƒ (GitHub Models) âœ“")
                elif o4mini_status == "error":
                    processing_history.append(f"**Phase 1.5e**: o4-mini ç‹¬ç«‹æ€è€ƒ âš ï¸ ã‚¨ãƒ©ãƒ¼")
                
                processing_history.append("**Phase 2**: Gemini çµ±åˆãƒ•ã‚§ãƒ¼ã‚º")
                
                if enable_strict:
                    processing_history.append("**Phase 3**: Gemini é¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    if use_grok_reviewer:
                        if grok_review_status == "success":
                            processing_history.append("**Phase 3b**: Grok æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ âœ“")
                        else:
                            processing_history.append("**Phase 3b**: Grok æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ âš ï¸ ã‚¨ãƒ©ãƒ¼")
                
                # å‡¦ç†å±¥æ­´ã‚’æœ€çµ‚å›ç­”ã«è¿½åŠ ï¼ˆGrokãƒ¬ãƒ“ãƒ¥ãƒ¼æˆåŠŸæ™‚ã¯æ—¢ã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                if grok_review_status == "success":
                    # Grokãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒæ—¢ã«å‡¦ç†å±¥æ­´ã‚’å«ã‚ã¦ã„ã‚‹ã®ã§ãã®ã¾ã¾ä½¿ç”¨
                    final_answer_with_history = final_answer
                else:
                    # Grokãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒãªã„ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼æ™‚ã®ã¿å‡¦ç†å±¥æ­´ã‚’è¿½åŠ 
                    final_answer_with_history = (
                        "## ğŸ“Š å‡¦ç†å±¥æ­´\n\n"
                        + "\n".join([f"- {item}" for item in processing_history])
                        + "\n\n---\n\n"
                        + final_answer
                    )
                
                # æ”¹è¡Œåœ§ç¸®ï¼š3è¡Œä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2è¡Œã«åœ§ç¸®
                final_answer_with_history = compact_newlines(final_answer_with_history)
                
                # ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ä»˜ãå›ç­”è¡¨ç¤º
                import html
                escaped_answer = html.escape(final_answer_with_history)
                answer_id = f"assistant_msg_{len(messages)}"
                
                st.markdown(f"""
<div style="position: relative;">
    <div id="{answer_id}" style="padding-right: 40px; white-space: pre-wrap;">{escaped_answer}</div>
    <button onclick="copyToClipboard('{answer_id}')" style="position: absolute; right: 0; top: 0; background: transparent; border: 1px solid #444; border-radius: 4px; cursor: pointer; padding: 4px 8px; color: #aaa; font-size: 12px;" title="ã‚³ãƒ”ãƒ¼">
        ğŸ“‹
    </button>
</div>
<script>
function copyToClipboard(elementId) {{
    const element = document.getElementById(elementId);
    const text = element.innerText;
    navigator.clipboard.writeText(text).then(() => {{
        const button = event.target;
        button.textContent = 'âœ“';
        setTimeout(() => {{ button.textContent = 'ğŸ“‹'; }}, 1000);
    }});
}}
</script>
""", unsafe_allow_html=True)
                
                # â–¼â–¼â–¼ ã‚³ã‚¹ãƒˆã‚µãƒãƒªãƒ¼è¡¨ç¤º â–¼â–¼â–¼
                st.markdown("---")
                st.markdown("## ğŸ’° ã‚³ã‚¹ãƒˆã‚µãƒãƒªãƒ¼")
                
                # Claude 4.5 Sonnet ã®ã‚³ã‚¹ãƒˆ
                claude_cost = 0.0
                if claude45_usage:
                    input_tokens = claude45_usage.get("inputTokens", 0)
                    output_tokens = claude45_usage.get("outputTokens", 0)
                    claude_cost = (input_tokens / 1_000_000) * 3.0 + (output_tokens / 1_000_000) * 15.0
                    st.markdown(f"**Claude 4.5 Sonnet (AWS Bedrock)**")
                    st.markdown(f"- Input: {input_tokens:,} tokens")
                    st.markdown(f"- Output: {output_tokens:,} tokens")
                    st.markdown(f"- ã‚³ã‚¹ãƒˆ: ${claude_cost:.4f}")
                    st.markdown("")
                
                # Gemini ã®ã‚³ã‚¹ãƒˆ (total_session_cost - claude_cost)
                gemini_cost = st.session_state.session_cost - claude_cost
                if gemini_cost > 0:
                    st.markdown(f"**Gemini (gemini-3-pro-preview)**")
                    st.markdown(f"- ã‚³ã‚¹ãƒˆ: ${gemini_cost:.4f}")
                    st.markdown("")
                
                # åˆè¨ˆ
                total_cost = st.session_state.session_cost
                st.markdown(f"**åˆè¨ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚³ã‚¹ãƒˆ**: ${total_cost:.4f}")
                    
                # â–²â–²â–² å‡¦ç†å±¥æ­´è¿½åŠ ã“ã“ã¾ã§ â–²â–²â–²

                # ---- ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ± ----
                if grounding_metadata:
                    st.markdown("---")
                    with st.expander("ğŸ“š æƒ…å ±æºã¨å¼•ç”¨", expanded=False):
                        if grounding_metadata.grounding_chunks:
                            st.markdown("**æ¤œç´¢çµæœã‹ã‚‰åˆ©ç”¨ã—ãŸæƒ…å ±æº:**")
                            unique_sources = {}
                            import urllib.parse

                            for chunk in grounding_metadata.grounding_chunks:
                                if getattr(chunk, "web", None):
                                    uri = getattr(chunk.web, "uri", None)
                                    title = getattr(chunk.web, "title", "æƒ…å ±æº")
                                    if uri and uri not in unique_sources:
                                        parsed = urllib.parse.urlparse(uri)
                                        domain = parsed.netloc.replace("www.", "")
                                        unique_sources[uri] = {
                                            "title": title,
                                            "domain": domain,
                                        }
                            for i, (uri, info) in enumerate(unique_sources.items(), 1):
                                st.markdown(f"{i}. **[{info['title']}]({uri})**")
                                st.caption(f"   å‡ºå…¸: {info['domain']}")

                messages.append({
                    "role": "model",
                    "content": final_answer_with_history,  # å‡¦ç†å±¥æ­´è¾¼ã¿ã§ä¿å­˜
                    "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
                update_current_session_messages(messages)

            except Exception as e:
                status_container.update(label="Error", state="error")
                err_text = str(e)
                if "RESOURCE_EXHAUSTED" in err_text or "429" in err_text:
                    st.error(
                        "âš ï¸ Vertex AI / Gemini ã®ã‚¯ã‚©ãƒ¼ã‚¿ã«é”ã—ã¾ã—ãŸã€‚\n\n"
                        "ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ / æ—¥æ¬¡åˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n"
                        "ãƒ»ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚\n"
                        "ãƒ»Google Cloud Console ã®ã€ŒVertex AI â†’ ä½¿ç”¨çŠ¶æ³ã€ã‹ã‚‰ã‚¯ã‚©ãƒ¼ã‚¿çŠ¶æ³ã‚’ç¢ºèªã§ãã¾ã™ã€‚"
                    )
                elif "NOT_FOUND" in err_text and "Publisher Model" in err_text:
                    st.error(
                        "âš ï¸ æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ / ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n"
                        "ãƒ»ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ¢ãƒ‡ãƒ«IDã‚’ã€2.5ç³» ã¾ãŸã¯ 3 Pro ã«å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚\n"
                    )
                else:
                    st.error(f"An error occurred: {e}")
