import os
import uuid
import datetime
import streamlit as st
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
import io
from logic import (
    USAGE_FILE, SESSIONS_FILE, MAX_BUDGET_USD, PRICING,
    VERTEX_PROJECT, VERTEX_LOCATION,
    load_usage, save_usage, calculate_cost, get_mime_type,
    extract_youtube_id, get_youtube_transcript, get_relevant_context,
    extract_text_from_response, load_sessions, save_sessions, get_client
)

try:
    from st_img_pastebutton import paste
    import_error_msg = None
except ImportError as e:
    paste = None
    import_error_msg = str(e)

# =========================
# ç’°å¢ƒå¤‰æ•° & å®šæ•°
# =========================

load_dotenv()



st.set_page_config(page_title="Gemini 3 Web Studio", layout="wide")

# ğŸ” ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ­ãƒƒã‚¯ (198501) + URLãƒˆãƒ¼ã‚¯ãƒ³æ°¸ç¶šåŒ–
SECRET_TOKEN = "access_granted_198501"

# 1. URLãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
query_params = st.query_params
url_token = query_params.get("auth", None)

if url_token == SECRET_TOKEN:
    st.session_state.authenticated = True
elif "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# 2. æœªèªè¨¼ãªã‚‰ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ç”»é¢
if not st.session_state.authenticated:
    st.title("Gemini 3 Studio")
    st.write("ã“ã®ã‚¢ãƒ—ãƒªã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™ã€‚")

    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")

    if st.button("ãƒ­ã‚°ã‚¤ãƒ³"):
        if password == "198501":
            st.session_state.authenticated = True
            # URLã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä»˜ä¸ã—ã¦ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã§ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯å¯èƒ½ã«ãªã‚‹ï¼‰
            st.query_params["auth"] = SECRET_TOKEN
            st.rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™ã€‚")

    st.stop()

# =========================
# Helper Functions
# =========================

# =========================
# Helper Functions
# =========================
# Moved to logic.py

# =========================
# Grok Review Function (OpenRouter API)
# =========================

import requests

# OpenRouter API Keyã®å–å¾— (st.secretså„ªå…ˆã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°)
if "OPENROUTER_API_KEY" in st.secrets:
    OPENROUTER_API_KEY = st.secrets["OPENROUTER_API_KEY"]
else:
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")

def review_with_grok(user_question: str, gemini_answer: str) -> str:
    """
    Grok 4.1 Fast Free ã‚’ä½¿ã£ã¦ã€Geminiã®å›ç­”ã‚’æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹
    """
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    data = {
        "model": "x-ai/grok-4.1-fast:free",
        "messages": [
            {
                "role": "system",
                "content": (
                    "ã‚ãªãŸã¯å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ã§ã™ã€‚"
                    "äº‹å®Ÿèª¤èªãƒ»è«–ç†ã®é£›èºãƒ»æŠœã‘æ¼ã‚Œã‚’å®¹èµ¦ãªãæŒ‡æ‘˜ã—ã€"
                    "å¿…è¦ãªã‚‰å›ç­”ã‚’å…¨æ–‡æ›¸ãç›´ã—ã¦ãã ã•ã„ã€‚"
                ),
            },
            {
                "role": "user",
                "content": (
                    f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{user_question}\n\n"
                    f"Gemini ã®å›ç­”:\n{gemini_answer}\n\n"
                    "1. æ˜ç¢ºãªå•é¡Œç‚¹ã® bullet list\n"
                    "2. å•é¡Œã‚’ä¿®æ­£ã—ãŸæœ€çµ‚å›ç­”ï¼ˆå…¨æ–‡ï¼‰\n"
                    "ã ã‘ã‚’æ—¥æœ¬èªã§å‡ºã—ã¦ãã ã•ã„ã€‚"
                ),
            },
        ],
    }
    try:
        resp = requests.post(url, headers=headers, json=data, timeout=60)
        resp.raise_for_status()
        j = resp.json()
        return j["choices"][0]["message"]["content"]
    except Exception as e:
        return f"[Grokãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}]\n\n{gemini_answer}"

# =========================
# Session Management
# =========================



def create_new_session():
    current_sessions = load_sessions()
    new_id = str(uuid.uuid4())
    new_session = {
        "id": new_id,
        "title": "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ",
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": [],
    }
    current_sessions.insert(0, new_session)
    save_sessions(current_sessions)
    st.session_state.sessions = current_sessions
    st.session_state.current_session_id = new_id
    st.rerun()

def switch_session(session_id):
    st.session_state.current_session_id = session_id
    st.rerun()

def update_current_session_messages(messages):
    if st.session_state.current_session_id:
        current_sessions = load_sessions()
        for session in current_sessions:
            if session["id"] == st.session_state.current_session_id:
                session["messages"] = messages
                if session["title"] == "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ" and len(messages) > 0:
                    first_msg = messages[0]["content"]
                    session["title"] = (first_msg[:20] + "...") if len(first_msg) > 20 else first_msg
                session["timestamp"] = datetime.datetime.now().isoformat()
                break
        save_sessions(current_sessions)
        st.session_state.sessions = current_sessions

def get_current_messages():
    if st.session_state.current_session_id:
        for session in st.session_state.sessions:
            if session["id"] == st.session_state.current_session_id:
                return session["messages"]
    return []

def delete_session(session_id):
    current_sessions = load_sessions()
    current_sessions = [s for s in current_sessions if s["id"] != session_id]
    save_sessions(current_sessions)
    st.session_state.sessions = current_sessions
    if st.session_state.current_session_id == session_id:
        st.session_state.current_session_id = None
        if st.session_state.sessions:
            st.session_state.current_session_id = st.session_state.sessions[0]["id"]
    st.rerun()

def branch_session():
    """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’åˆ†å²"""
    current_messages = get_current_messages()
    current_sessions = load_sessions()
    
    # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    current_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
    for session in current_sessions:
        if session["id"] == st.session_state.current_session_id:
            current_title = session["title"]
            break
    
    # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    new_id = str(uuid.uuid4())
    new_session = {
        "id": new_id,
        "title": f"{current_title} (åˆ†å²)",
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": current_messages.copy(),  # ç¾åœ¨ã®å±¥æ­´ã‚’ã‚³ãƒ”ãƒ¼
    }
    current_sessions.insert(0, new_session)
    save_sessions(current_sessions)
    
    st.session_state.sessions = current_sessions
    st.session_state.current_session_id = new_id
    st.session_state.session_cost = 0.0  # ã‚³ã‚¹ãƒˆãƒªã‚»ãƒƒãƒˆ
    st.rerun()

# =========================
# Initialization
# =========================

if "sessions" not in st.session_state:
    st.session_state.sessions = load_sessions()

if "current_session_id" not in st.session_state:
    # Always create a new session when the app starts
    create_new_session()

if "session_cost" not in st.session_state:
    st.session_state.session_cost = 0.0

usage_stats = load_usage()

# ==========================
# ã‚³ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã®å‰ã«å®Ÿè¡Œï¼‰
# ==========================
usage_stats = load_usage()
stop_generation = usage_stats["total_cost_usd"] >= MAX_BUDGET_USD

# =========================
# Sidebar
# =========================

with st.sidebar:
    # ğŸ” ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³
    if st.button("ğŸ”’ ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", use_container_width=True):
        st.session_state.authenticated = False
        st.query_params.clear()  # URLãƒˆãƒ¼ã‚¯ãƒ³ã‚‚å‰Šé™¤
        st.rerun()

    st.markdown("""
    <div style="text-align: center; padding: 0; margin: 0; margin-top: -1rem;">
        <h1 style="font-size: 18px; font-weight: 700; margin: 0; padding: 0; letter-spacing: 1px;">
            Gemini 3<br/>Studio
        </h1>
    </div>
    """, unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• æ–°è¦", use_container_width=True):
            create_new_session()
    with col2:
        if st.button("ğŸŒ± åˆ†å²", use_container_width=True):
            branch_session()

    # ---- å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ ----
    # ---- å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ ----
    current_messages = get_current_messages()
    if current_messages:
        with st.expander("ğŸ”— å…±æœ‰ãƒªãƒ³ã‚¯ä½œæˆ", expanded=False):
            export_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
            for s in st.session_state.sessions:
                if s["id"] == st.session_state.current_session_id:
                    export_title = s["title"]
                    break

            export_md = f"# {export_title}\n\n"
            export_md += f"**ä½œæˆæ—¥æ™‚**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
            for msg in current_messages:
                role_label = "ğŸ§‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg["role"] == "user" else "ğŸ¤– AI"
                export_md += f"## {role_label}\n\n{msg['content']}\n\n---\n\n"

            if st.button("ãƒªãƒ³ã‚¯ç”Ÿæˆ", use_container_width=True):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    try:
                        import urllib.request
                        import urllib.parse

                        data = urllib.parse.urlencode(
                            {"content": export_md, "expiry_days": 7, "syntax": "md"}
                        ).encode()

                        req = urllib.request.Request("https://dpaste.org/api/", data=data)
                        req.add_header("User-Agent", "Mozilla/5.0 (Gemini3Studio)")

                        with urllib.request.urlopen(req) as response:
                            share_url = response.read().decode("utf-8").strip()
                            st.success("ä½œæˆå®Œäº†ï¼")
                            st.code(share_url)
                            st.caption("â€»7æ—¥é–“æœ‰åŠ¹")
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    st.markdown("---")

    # ---- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ ----
    # ---- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ ----
    with st.expander("ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«", expanded=False):
        uploaded_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«",
            accept_multiple_files=True,
            type=["png", "jpg", "jpeg", "mp4", "mov", "txt", "pdf", "csv"],
            label_visibility="collapsed"
        )

    # ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ï¼ˆExpanderã®å¤–ã«å‡ºã™ï¼‰
    if paste:
        if "paste_key" not in st.session_state:
            st.session_state.paste_key = 0
        try:
            pasted_image_bytes = paste(
                label="ğŸ“‹ ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰è²¼ä»˜",
                key=f"paste_btn_{st.session_state.paste_key}",
            )
        except Exception as e:
            st.error(f"Error: {e}")
        
        if pasted_image_bytes:
            st.success("è²¼ä»˜å®Œäº†")
            st.image(pasted_image_bytes, caption="ç”»åƒ", use_container_width=True)
            if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", key="clear_paste"):
                st.session_state.paste_key += 1
                st.rerun()
    else:
        st.warning("Clipboard lib missing")

    # ---- YouTube URL ----
    with st.expander("ğŸ“º YouTubeåˆ†æ", expanded=False):
        youtube_url = st.text_input("URL", placeholder="https://youtu.be/...", label_visibility="collapsed")

    st.markdown("---")
    
    st.markdown("---")
    
    # ---- ãƒ¢ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªé¸æŠ ----
    mode_category = st.radio(
        "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰",
        ["ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)", "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(é€šå¸¸)"],
        index=0,
        horizontal=True,
    )
    
    # ---- å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ ----
    if mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)":
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(å¤šå±¤)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["é¸æŠ1 (å®Œå…¨ç‰ˆ)", "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)", "ãƒ™ãƒ¼ã‚¿ç‰ˆ"],
                index=0,
                horizontal=True,
                label_visibility="collapsed"
            )
            
            if mode_type == "é¸æŠ1 (å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "2. ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "3. ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                    ],
                    index=1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ãƒ¡ã‚¿æ€è€ƒã«å¤‰æ›´
                )
            elif mode_type == "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                    ],
                    index=0
                )
            else:
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    # ---- é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ ----
    else:
        with st.expander("ãƒ¢ãƒ¼ãƒ‰è¨­å®š(é€šå¸¸)", expanded=True):
            mode_type = st.radio(
                "ã‚¿ã‚¤ãƒ—",
                ["é¸æŠ1 (å®Œå…¨ç‰ˆ)", "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)", "ãƒ™ãƒ¼ã‚¿ç‰ˆ"],
                index=0,
                horizontal=True,
                label_visibility="collapsed"
            )
            
            if mode_type == "é¸æŠ1 (å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ + é¬¼è»æ›¹",
                        "2. ç†Ÿè€ƒ (ãƒ¡ã‚¿æ€è€ƒ)",
                        "3. ç†Ÿè€ƒ (æœ¬æ°—MAX)",
                    ],
                    index=0
                )
            elif mode_type == "é¸æŠ2 (ä¸å®Œå…¨ç‰ˆ)":
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "1. ç†Ÿè€ƒ (ãƒªã‚µãƒ¼ãƒ)",
                    ],
                    index=0
                )
            else:
                response_mode = st.radio(
                    "ãƒ¢ãƒ¼ãƒ‰",
                    [
                        "Î²1. é€šå¸¸ (é«˜é€Ÿ)",
                    ],
                    index=0
                )
    
    strict_mode = False
    
    # ---- è¨­å®š (ãƒ¢ãƒ‡ãƒ«ãªã©) ----
    with st.expander("âš™ï¸ è¨­å®š", expanded=False):
        model_options = [
            "gemini-3-pro-preview",
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.0-flash",
        ]
        model_id = st.selectbox("ãƒ¢ãƒ‡ãƒ«ID", options=model_options, index=0)

        use_search = st.toggle("Googleæ¤œç´¢", value=True)
        candidate_count = st.slider("å€™è£œæ•°", min_value=1, max_value=3, value=3)

    st.markdown("---")

    # ---- å±¥æ­´æ¤œç´¢ ----
    search_query = st.text_input("ğŸ” å±¥æ­´æ¤œç´¢", placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰...")
    if search_query:
        filtered_sessions = []
        for s in st.session_state.sessions:
            if search_query.lower() in s["title"].lower():
                filtered_sessions.append(s)
                continue
            found = False
            for m in s["messages"]:
                if search_query.lower() in m["content"].lower():
                    filtered_sessions.append(s)
                    found = True
                    break
            if not found:
                pass
    else:
        # æ¤œç´¢ã—ã¦ã„ãªã„å ´åˆ: ç©ºã®ã€Œæ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã€ã‚’é™¤å¤–ï¼ˆç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯é™¤ãï¼‰
        filtered_sessions = []
        for s in st.session_state.sessions:
            if s["id"] == st.session_state.current_session_id:
                filtered_sessions.append(s)
                continue
            if s["title"] == "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ" and len(s["messages"]) == 0:
                continue
            filtered_sessions.append(s)

    # CSSã§è¦‹ã‚„ã™ãä½¿ã„ã‚„ã™ãæœ€é©åŒ–
    st.markdown("""
    <style>
    /* å…¨ä½“ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºç¸®å° */
    .stApp, .stMarkdown, .stButton, .stSelectbox, .stTextInput, .stTextArea {
        font-size: 12px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä½™ç™½ã‚’æ¥µé™ã¾ã§è©°ã‚ã‚‹ */
    section[data-testid="stSidebar"] .block-container {
        padding-top: 0.3rem !important;
        padding-bottom: 0.3rem !important;
        padding-left: 0.5rem !important;
        padding-right: 0.5rem !important;
    }
    /* å„è¦ç´ é–“ã®éš™é–“ã‚’è©°ã‚ã‚‹ */
    div[data-testid="stVerticalBlock"] > div {
        gap: 0.1rem !important;
    }
    /* Expanderã®ä½™ç™½å‰Šæ¸› */
    .streamlit-expanderHeader {
        font-size: 11px !important;
        padding-top: 0px !important;
        padding-bottom: 0px !important;
        min-height: 1.3rem !important;
    }
    .streamlit-expanderContent {
        padding-top: 0px !important;
        padding-bottom: 0px !important;
    }
    /* éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ã‚¿ã‚¤ãƒˆãƒ« */
    div[data-testid="stExpander"] summary p {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ */
    section[data-testid="stSidebar"] button p {
        font-size: 10px !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã« */
    section[data-testid="stSidebar"] button {
        padding: 0rem 0.2rem !important;
        min-height: 1.4rem !important;
        font-size: 10px !important;
        margin-bottom: 0px !important;
    }
    /* ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®ãƒ©ãƒ™ãƒ«ã‚’å°ã•ã */
    section[data-testid="stSidebar"] .stRadio label {
        font-size: 10px !important;
    }
    section[data-testid="stSidebar"] .stRadio > label > div {
        font-size: 10px !important;
    }
    /* ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ã‚‹ï¼ˆæ”¹è¡Œã•ã›ãªã„ï¼‰ */
    section[data-testid="stSidebar"] label, 
    section[data-testid="stSidebar"] p,
    section[data-testid="stSidebar"] span {
        white-space: nowrap !important;
        overflow: hidden !important;
        text-overflow: ellipsis !important;
    }
    /* åŒºåˆ‡ã‚Šç·š */
    hr {
        margin-top: 0.1rem !important;
        margin-bottom: 0.1rem !important;
    }
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ç¸®å° */
    h1, h2, h3 {
        padding-top: 0rem !important;
        padding-bottom: 0rem !important;
        margin-top: 0rem !important;
        margin-bottom: 0rem !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ç›´è¿‘5ä»¶ã¨éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«åˆ†å‰²
    recent_sessions = filtered_sessions[:5] if len(filtered_sessions) > 5 else filtered_sessions
    archive_sessions = filtered_sessions[5:] if len(filtered_sessions) > 5 else []
    
    # ç›´è¿‘5ä»¶ï¼ˆå¸¸ã«å±•é–‹ï¼‰
    if recent_sessions:
        st.markdown("**ğŸ“Œ ç›´è¿‘ã®ãƒãƒ£ãƒƒãƒˆ**")
        for session in recent_sessions:
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                if st.button(session["title"], key=f"btn_{session['id']}", use_container_width=True):
                    switch_session(session["id"])
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_{session['id']}"):
                    delete_session(session["id"])
    
    # éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
    if archive_sessions:
        with st.expander(f"ğŸ“œ éå»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– ({len(archive_sessions)}ä»¶)", expanded=False):
            for session in archive_sessions:
                col1, col2 = st.columns([0.8, 0.2])
                with col1:
                    if st.button(session["title"], key=f"btn_{session['id']}", use_container_width=True):
                        switch_session(session["id"])
                with col2:
                    if st.button("ğŸ—‘ï¸", key=f"del_{session['id']}"):
                        delete_session(session["id"])

    st.markdown("---")

    # ---- ç”»åƒç”Ÿæˆ ----
    with st.expander("ğŸ¨ ç”»åƒç”Ÿæˆ", expanded=False):
        img_prompt = st.text_area("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", placeholder="æœªæ¥çš„ãªéƒ½å¸‚...")
        aspect_ratio = st.selectbox("æ¯”ç‡", ["16:9", "1:1", "4:3", "3:4", "9:16"])

        generate_img_btn = st.button("ç”Ÿæˆ", type="primary", disabled=not img_prompt)
        if generate_img_btn and img_prompt:
            st.session_state.generate_image_trigger = {
                "prompt": img_prompt,
                "aspect_ratio": aspect_ratio,
            }

    st.markdown("---")

    # ---- è©•ä¾¡åˆ†æ ----
    with st.expander("ğŸ“Š å“è³ªåˆ†æ", expanded=False):
        total_ratings = 0
        positive_ratings = 0
        model_ratings = {}

        for s in st.session_state.sessions:
            for m in s["messages"]:
                if "rating" in m and m["rating"] is not None:
                    total_ratings += 1
                    if m["rating"] == 1:
                        positive_ratings += 1
                    if "metadata" in m and "model" in m["metadata"]:
                        mod = m["metadata"]["model"]
                        if mod not in model_ratings:
                            model_ratings[mod] = {"total": 0, "positive": 0}
                        model_ratings[mod]["total"] += 1
                        if m["rating"] == 1:
                            model_ratings[mod]["positive"] += 1

        if total_ratings > 0:
            approval_rate = (positive_ratings / total_ratings) * 100
            st.metric("Positive Ratings", f"{positive_ratings}/{total_ratings}", f"{approval_rate:.1f}%")
            best_model = "N/A"
            best_rate = -1
            for mod, stats in model_ratings.items():
                rate = stats["positive"] / stats["total"]
                if rate > best_rate:
                    best_rate = rate
                    best_model = mod
            if best_model != "N/A":
                st.caption(f"ğŸ† Best: **{best_model}** ({best_rate*100:.0f}%)")
        else:
            st.caption("ãƒ‡ãƒ¼ã‚¿ãªã—")

    st.markdown("---")

    # ---- ã‚³ã‚¹ãƒˆè¡¨ç¤º ----
    from logic import load_manual_cost, save_manual_cost, MAX_BUDGET_JPY, TRIAL_LIMIT_JPY, TRIAL_EXPIRY
    
    st.subheader("ğŸ’° Cost")
    st.caption(f"äºˆç®—: Â¥{MAX_BUDGET_JPY:,.0f}")
    st.caption(f"ä¸Šé™: Â¥{TRIAL_LIMIT_JPY:,.0f}")
    st.caption(f"æœ‰åŠ¹æœŸé™: {TRIAL_EXPIRY}")
    
    # æ‰‹å‹•ã‚³ã‚¹ãƒˆå…¥åŠ›ï¼ˆæ°¸ç¶šåŒ–ï¼‰
    current_manual_cost = load_manual_cost()
    manual_cost = st.number_input(
        "æ‰‹å‹•å…¥åŠ› (Â¥)",
        min_value=0.0,
        value=current_manual_cost,
        step=10.0,
        format="%.0f",
        key="manual_cost_persistent",
        help="Google Cloud Consoleã§ç¢ºèªã—ãŸå®Ÿéš›ã®ã‚³ã‚¹ãƒˆï¼ˆå††ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ã“ã®å€¤ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¦ã‚‚ä¿æŒã•ã‚Œã¾ã™ã€‚"
    )
    
    # å€¤ãŒå¤‰æ›´ã•ã‚ŒãŸã‚‰ä¿å­˜
    if manual_cost != current_manual_cost:
        save_manual_cost(manual_cost)

    st.markdown("---")
    st.code(f"PROJECT: {VERTEX_PROJECT}\nLOCATION: {VERTEX_LOCATION} (Vertex AI)")

# =========================
# Main UI
# =========================

current_session_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
for s in st.session_state.sessions:
    if s["id"] == st.session_state.current_session_id:
        current_session_title = s["title"]
        break

st.header(current_session_title)
st.markdown(
    "ä»¥ä¸‹ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€YouTubeåˆ†æã€æ¤œç´¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
)

# ---- Vertex AI Client ----



client = get_client()

# ---- å±¥æ­´è¡¨ç¤º ----

messages = get_current_messages()
for idx, msg in enumerate(messages):
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg["role"] == "model":
            col1, col2, col3 = st.columns([0.1, 0.1, 0.8])
            current_rating = msg.get("rating")
            with col1:
                if st.button("ğŸ‘", key=f"up_{idx}"):
                    messages[idx]["rating"] = 1
                    update_current_session_messages(messages)
                    st.rerun()
            with col2:
                if st.button("ğŸ‘", key=f"down_{idx}"):
                    messages[idx]["rating"] = -1
                    update_current_session_messages(messages)
                    st.rerun()
            with col3:
                if current_rating == 1:
                    st.caption("âœ… é«˜è©•ä¾¡")
                elif current_rating == -1:
                    st.caption("âŒ ä½è©•ä¾¡")

# =========================
# ç”»åƒç”Ÿæˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
# =========================

if hasattr(st.session_state, "generate_image_trigger") and st.session_state.generate_image_trigger:
    img_data = st.session_state.generate_image_trigger
    del st.session_state.generate_image_trigger

    with st.chat_message("user"):
        st.markdown(f"ğŸ¨ ç”»åƒç”Ÿæˆ: {img_data['prompt']}")
        st.caption(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”: {img_data['aspect_ratio']}")

    messages.append({"role": "user", "content": f"ğŸ¨ ç”»åƒç”Ÿæˆ: {img_data['prompt']}"})
    update_current_session_messages(messages)

    with st.chat_message("assistant"):
        with st.status("ç”»åƒã‚’ç”Ÿæˆä¸­...", expanded=True) as status:
            try:
                config = types.GenerateContentConfig(
                    response_modalities=["IMAGE"],
                    image_config=types.ImageConfig(
                        aspect_ratio=img_data["aspect_ratio"]
                    ),
                )
                status.write("Gemini 3 Pro Imageã§ç”Ÿæˆä¸­...")
                response = client.models.generate_content(
                    model="gemini-3-pro-image-preview",
                    contents=img_data["prompt"],
                    config=config,
                )

                generated_image = None

                # ç”»åƒã‚’å–ã‚Šå‡ºã™
                if getattr(response, "candidates", None):
                    for candidate in response.candidates:
                        if getattr(candidate, "content", None) and candidate.content.parts:
                            for part in candidate.content.parts:
                                if getattr(part, "inline_data", None):
                                    try:
                                        generated_image = part.as_image()
                                    except AttributeError:
                                        image_bytes_raw = part.inline_data.data
                                        generated_image = Image.open(io.BytesIO(image_bytes_raw))
                                    break
                        if generated_image is not None:
                            break

                if generated_image is not None:
                    st.image(generated_image, caption=img_data["prompt"])
                    buf = io.BytesIO()
                    generated_image.save(buf, format="PNG")
                    image_bytes = buf.getvalue()
                    st.download_button(
                        label="ğŸ’¾ ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=image_bytes,
                        file_name=f"generated_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                        mime="image/png",
                    )
                    messages.append(
                        {"role": "model", "content": f"âœ… ç”»åƒã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {img_data['prompt']}"}
                    )
                    update_current_session_messages(messages)
                    status.update(label="âœ… ç”»åƒç”Ÿæˆå®Œäº†", state="complete")
                else:
                    st.error("ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error")

            except Exception as e:
                status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error")
                st.error(f"ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

# =========================
# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
# =========================

prompt = st.chat_input("ä½•ã‹èã„ã¦ãã ã•ã„...", disabled=stop_generation)

if prompt:
    if stop_generation:
        st.error("äºˆç®—ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚")
    else:
        # ---- ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€è¡¨ç¤º ----
        with st.chat_message("user"):
            st.markdown(prompt)
            if uploaded_files:
                for uf in uploaded_files:
                    st.caption(f"ğŸ“ æ·»ä»˜: {uf.name}")
            if youtube_url:
                st.caption(f"ğŸ“º YouTube: {youtube_url}")
            if pasted_image_bytes:
                st.caption("ğŸ“‹ ç”»åƒãŒè²¼ã‚Šä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ")

        messages.append({"role": "user", "content": prompt})
        update_current_session_messages(messages)

        # ---- ãƒ¢ãƒ‡ãƒ«å¿œç­” ----
        with st.chat_message("assistant"):
            status_container = st.status("æ€è€ƒä¸­...", expanded=True)
            try:
                # ä¼šè©±å±¥æ­´
                model_history = []
                for msg in messages[:-1]:
                    model_history.append(
                        types.Content(
                            role=msg["role"],
                            parts=[types.Part.from_text(text=msg["content"])],
                        )
                    )

                # ç¾åœ¨ã®ã‚¿ãƒ¼ãƒ³
                current_parts = [types.Part.from_text(text=prompt)]

                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
                for uploaded_file in uploaded_files or []:
                    try:
                        mime_type = get_mime_type(uploaded_file.name)
                        bytes_data = uploaded_file.getvalue()
                        part = types.Part.from_bytes(data=bytes_data, mime_type=mime_type)
                        current_parts.append(part)
                        status_container.write(f"ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™å®Œäº†: {uploaded_file.name}")
                    except Exception as e:
                        status_container.error(
                            f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {uploaded_file.name} - {e}"
                        )

                # è²¼ã‚Šä»˜ã‘ç”»åƒ
                if pasted_image_bytes:
                    import base64

                    status_container.write("è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã‚’å‡¦ç†ä¸­...")
                    try:
                        if isinstance(pasted_image_bytes, str):
                            if pasted_image_bytes.startswith("data:"):
                                base64_str = pasted_image_bytes.split(",", 1)[1]
                                image_bytes_decoded = base64.b64decode(base64_str)
                            else:
                                image_bytes_decoded = base64.b64decode(pasted_image_bytes)
                        else:
                            image_bytes_decoded = pasted_image_bytes
                        part = types.Part.from_bytes(
                            data=image_bytes_decoded, mime_type="image/png"
                        )
                        current_parts.append(part)
                        status_container.write("è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã®æº–å‚™å®Œäº†")
                    except Exception as e:
                        status_container.error(f"è²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

                # YouTube å­—å¹•
                if youtube_url:
                    vid_id = extract_youtube_id(youtube_url)
                    if vid_id:
                        status_container.write("YouTubeã®å­—å¹•ã‚’å–å¾—ä¸­...")
                        transcript_text = get_youtube_transcript(vid_id)
                        current_parts.append(
                            types.Part.from_text(text=f"YouTube Transcript:\n{transcript_text}")
                        )
                    else:
                        status_container.write("ç„¡åŠ¹ãªYouTube URLã§ã™ã€‚")

                contents_for_model = model_history + [
                    types.Content(role="user", parts=current_parts)
                ]

                # ---- Tool / Config ----
                tools = []
                final_candidate_count = candidate_count
                if use_search:
                    tools.append(types.Tool(google_search=types.GoogleSearch()))
                    final_candidate_count = 1

                # â˜… System Instruction ã®æ”¹å–„: ãƒ¡ã‚¿ç™ºè¨€ç¦æ­¢ã¨ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªæŒ¯ã‚‹èˆã„ã‚’æŒ‡ç¤º
                base_system_instruction = (
                    "ã‚ãªãŸã¯é«˜åº¦ãªçŸ¥æ€§ã‚’æŒã¤å°‚é–€çš„ãªãƒªã‚µãƒ¼ãƒãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
                    "ä»¥ä¸‹ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼š\n"
                    "1. **ãƒ¡ã‚¿ç™ºè¨€ã®ç¦æ­¢**: ã€Œç§ã¯AIã§ã™ã€ã€Œä¸–ç•Œæœ€é«˜å³°ã®ï½ã¨ã—ã¦ã€ãªã©ã®è‡ªå·±è¨€åŠã‚„å‰ç½®ãã¯ä¸€åˆ‡è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚\n"
                    "2. **æ„å›³ã®æ±²ã¿å–ã‚Š**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®èƒŒå¾Œã«ã‚ã‚‹æ„å›³ï¼ˆæ–‡è„ˆã€æš—é»™ã®å‰æï¼‰ã‚’æ¨æ¸¬ã—ã€è¨€è‘‰é€šã‚Šã§ã¯ãªãã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæœ¬å½“ã«çŸ¥ã‚ŠãŸã„ã“ã¨ã€ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n"
                    "3. **æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”**: çµè«–ã‚’å…ˆã«è¿°ã¹ã€ãã®å¾Œã«è©³ç´°ãªæ ¹æ‹ ã€ã‚·ãƒŠãƒªã‚ªåˆ†æã€ãƒªã‚¹ã‚¯è¦å› ã‚’è«–ç†çš„ã«å±•é–‹ã—ã¦ãã ã•ã„ã€‚\n"
                    "4. **å®¢è¦³æ€§**: äºˆæ¸¬ã‚’è¡Œã†å ´åˆã¯ã€æ–­å®šã‚’é¿ã‘ã€è¤‡æ•°ã®ã‚·ãƒŠãƒªã‚ªï¼ˆæ¥½è¦³ã€æ‚²è¦³ã€ä¸­ç«‹ï¼‰ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                    "5. **å¼•ç”¨**: æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸå ´åˆã¯ã€å¿…ãšæƒ…å ±æºã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"
                )

                final_answer = ""
                grounding_metadata = None
                
                # =========================
                # ãƒ¢ãƒ¼ãƒ‰è¨­å®šã®è§£æ
                # =========================
                enable_research = True  # å…¨ãƒ¢ãƒ¼ãƒ‰ã§ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
                enable_meta = "ãƒ¡ã‚¿" in response_mode or "MAX" in response_mode
                enable_strict = "é¬¼è»æ›¹" in response_mode or "MAX" in response_mode

                # =========================
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ (é«˜é€Ÿ / é¬¼è»æ›¹)
                # =========================
                if not enable_research:
                    config = types.GenerateContentConfig(
                        temperature=0.7,
                        candidate_count=1,
                        tools=tools,
                        system_instruction=base_system_instruction,
                    )
                    
                    status_container.write("å›ç­”ç”Ÿæˆä¸­...")
                    response = client.models.generate_content(
                        model=model_id,
                        contents=contents_for_model,
                        config=config,
                    )
                    
                    final_answer = extract_text_from_response(response)
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®—
                    if response.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            response.usage_metadata.prompt_token_count,
                            response.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += response.usage_metadata.prompt_token_count
                        usage_stats["total_output_tokens"] += response.usage_metadata.candidates_token_count

                    # é¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ (é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ç‰ˆ)
                    if enable_strict:
                        status_container.write("ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                        reviewer_instruction = base_system_instruction + """
**ã‚ãªãŸã®å½¹å‰²**: é¬¼è»æ›¹ãƒ¬ãƒ™ãƒ«ã®å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢
**ã‚¿ã‚¹ã‚¯**: åˆç‰ˆå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’è¿”ã™
**å‡ºåŠ›**: ä¿®æ­£ç‰ˆã®å›ç­”å…¨æ–‡ã®ã¿
"""
                        review_contents = [types.Content(role="user", parts=[types.Part(text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {prompt}\n\nåˆç‰ˆå›ç­”:\n{final_answer}\n\nãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ä¿®æ­£ç‰ˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚")])]
                        review_resp = client.models.generate_content(
                            model=model_id,
                            contents=review_contents,
                            config=types.GenerateContentConfig(temperature=0.1, candidate_count=1, system_instruction=reviewer_instruction)
                        )
                        final_answer = extract_text_from_response(review_resp)
                        status_container.write("âœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                        
                        if review_resp.usage_metadata:
                            cost = calculate_cost(model_id, review_resp.usage_metadata.prompt_token_count, review_resp.usage_metadata.candidates_token_count)
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += review_resp.usage_metadata.prompt_token_count
                            usage_stats["total_output_tokens"] += review_resp.usage_metadata.candidates_token_count

                # =========================
                # ç†Ÿè€ƒãƒ¢ãƒ¼ãƒ‰
                # =========================
                else:
                    # =========================
                    # ç†Ÿè€ƒãƒ¢ãƒ¼ãƒ‰: å¤šæ®µéšã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
                    # =========================
                    
                    # --- Phase 1: ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    status_container.write("Phase 1: ãƒªã‚µãƒ¼ãƒãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                    
                    research_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: ãƒªã‚µãƒ¼ãƒå°‚ä»»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®èª¿æŸ»ãƒ¡ãƒ¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æœ€çµ‚å›ç­”ã¯æ›¸ã‹ãšã€äº‹å®Ÿåé›†ã«é›†ä¸­ã™ã‚‹ã“ã¨ã€‚

**èª¿æŸ»è¦³ç‚¹**:
- è³ªå•ã«é–¢é€£ã™ã‚‹**æœ€æ–°ã®äº‹å®Ÿãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±è¨ˆ**ï¼ˆ**2025å¹´ã®æƒ…å ±ã‚’æœ€å„ªå…ˆ**ï¼‰
- **ç¾æ™‚ç‚¹ã§å­˜åœ¨ã™ã‚‹å…¨ã¦ã®é¸æŠè‚¢ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¶²ç¾…çš„ã«èª¿æŸ»**ï¼ˆä¾‹: è£½å“æ¯”è¼ƒãªã‚‰ã€æœ€æ–°ç‰ˆã ã‘ã§ãªãç›´è¿‘ã®å…¨ä¸–ä»£ã‚’èª¿æŸ»ï¼‰
- å…¬å¼æƒ…å ±ã‚„ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‹ã‚‰ã®å¼•ç”¨
- é–¢é€£ã™ã‚‹èƒŒæ™¯æƒ…å ±ã‚„æ–‡è„ˆ
- ç«¶åˆãƒ»ä»£æ›¿æ¡ˆãƒ»æ¯”è¼ƒå¯¾è±¡ã®**å®Œå…¨ãªãƒªã‚¹ãƒˆ**
- ãƒªã‚¹ã‚¯ãƒ»åˆ¶ç´„ãƒ»æ³¨æ„ç‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- å‚è€ƒã«ã—ãŸä¸»è¦ãªæƒ…å ±æºã®URL

**é‡è¦**: 
- çµè«–ã‚„æ¨å¥¨ã¯æ›¸ã‹ãšã€å¾Œå·¥ç¨‹ãŒåˆ¤æ–­ã§ãã‚‹èª¿æŸ»ãƒ¡ãƒ¢ã«é›†ä¸­ã™ã‚‹ã“ã¨
- **å¿…ãšæ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã€æœ€æ–°ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã“ã¨**
- **æ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æ—¥ä»˜ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»ãƒ¢ãƒ‡ãƒ«åã‚’å„ªå…ˆçš„ã«è¨˜è¼‰ã™ã‚‹ã“ã¨**
- **æ¯”è¼ƒå¯¾è±¡ã¨ãªã‚‹é¸æŠè‚¢ã‚’è¦‹è½ã¨ã•ãªã„ã‚ˆã†ã€è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è©¦ã™ã“ã¨**ï¼ˆä¾‹: ã€ŒiPhone æœ€æ–°ãƒ¢ãƒ‡ãƒ« 2025ã€ã€ŒiPhone 17ã€ã€ŒiPhone 2025å¹´ç™ºå£²ã€ãªã©ï¼‰
- **ã€Œã“ã‚Œã‚ˆã‚Šæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«/ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯å­˜åœ¨ã—ãªã„ã‹ï¼Ÿã€ã‚’å¸¸ã«ç¢ºèªã™ã‚‹ã“ã¨**
- å¤ã„æƒ…å ±ï¼ˆ2024å¹´ä»¥å‰ãªã©ï¼‰ã—ã‹è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨
"""

                    # éå»ã®é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    past_context = get_relevant_context(prompt, st.session_state.sessions, st.session_state.current_session_id)
                    
                    # ãƒªã‚µãƒ¼ãƒç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
                    research_parts = [types.Part(text=f"ã“ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®èª¿æŸ»ãƒ¡ãƒ¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š\n{prompt}")]
                    
                    if past_context:
                        research_parts.insert(0, types.Part(text="ä»¥ä¸‹ã¯éå»ã®é–¢é€£ãƒãƒ£ãƒƒãƒˆã‹ã‚‰æŠ½å‡ºã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™ï¼š\n\n" + past_context))
                    
                    research_contents = contents_for_model + [
                        types.Content(role="user", parts=research_parts)
                    ]
                    
                    research_config = types.GenerateContentConfig(
                        temperature=0.2,
                        candidate_count=1,
                        tools=tools,
                        system_instruction=research_instruction,
                    )
                    
                    research_resp = client.models.generate_content(
                        model=model_id,
                        contents=research_contents,
                        config=research_config,
                    )
                    
                    research_text = extract_text_from_response(research_resp)
                    
                    # ãƒªã‚µãƒ¼ãƒãƒ•ã‚§ãƒ¼ã‚ºã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
                    if research_resp.candidates and research_resp.candidates[0].grounding_metadata:
                        grounding_metadata = research_resp.candidates[0].grounding_metadata
                    
                    status_container.write("âœ“ ãƒªã‚µãƒ¼ãƒå®Œäº†")
                    with status_container.expander("åé›†ã—ãŸèª¿æŸ»ãƒ¡ãƒ¢", expanded=False):
                        st.markdown(research_text)
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 1)
                    if research_resp.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            research_resp.usage_metadata.prompt_token_count,
                            research_resp.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += research_resp.usage_metadata.prompt_token_count
                        usage_stats["total_output_tokens"] += research_resp.usage_metadata.candidates_token_count
                    
                    # --- Phase 1.5: ãƒ¡ã‚¿è³ªå•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    questions_text = ""
                    if enable_meta:
                        status_container.write("Phase 1.5: ãƒ¡ã‚¿è³ªå•ç”Ÿæˆä¸­...")
                        
                        question_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: ãƒ¡ã‚¿è³ªå•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ç›®çš„**: 
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ã¨èª¿æŸ»ãƒ¡ãƒ¢ã‚’è¸ã¾ãˆã¦ã€ã“ã®ãƒ†ãƒ¼ãƒã‚’ã•ã‚‰ã«æ·±ãç†è§£ã™ã‚‹ãŸã‚ã®ã€Œé‹­ã„ã‚µãƒ–è³ªå•ã€ã‚’ä½œæˆã™ã‚‹ã“ã¨ã€‚

**ãƒ«ãƒ¼ãƒ«**:
- æœ€å¤§5å€‹ã¾ã§
- å„è³ªå•ã¯1-2è¡Œã§å…·ä½“çš„ã‹ã¤é‹­ã
- ä»¥ä¸‹ã®è¦³ç‚¹ã‚’æ„è­˜:
  1. å‰æãŒå´©ã‚Œã‚‹å¯èƒ½æ€§ã¯ã©ã“ã‹ï¼Ÿ
  2. å¼·æ°—/å¼±æ°—ã‚·ãƒŠãƒªã‚ªã®åˆ†å²ç‚¹ã¯ä½•ã‹ï¼Ÿ
  3. ç«¶åˆãƒ»æŠ€è¡“ãƒ»è¦åˆ¶ã®ä¸ç¢ºå®Ÿæ€§ã¯ï¼Ÿ
  4. ã€Œäºˆæ¸¬ãŒå¤–ã‚Œã‚‹ã¨ã—ãŸã‚‰ã©ã‚“ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ï¼Ÿã€

**å‡ºåŠ›**: ç®‡æ¡æ›¸ãï¼ˆQ1, Q2...ï¼‰ã®ã¿
"""

                        question_contents = [types.Content(role="user", parts=[types.Part(text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•:\n{prompt}\n\n==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\nã“ã®ãƒ†ãƒ¼ãƒã‚’ã•ã‚‰ã«æ·±æ˜ã‚Šã™ã‚‹ãŸã‚ã®é‡è¦ãªã‚µãƒ–è³ªå•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")])]
                        
                        question_resp = client.models.generate_content(
                            model=model_id,
                            contents=question_contents,
                            config=types.GenerateContentConfig(
                                temperature=0.4,
                                candidate_count=1,
                                system_instruction=question_instruction,
                            )
                        )
                        
                        questions_text = extract_text_from_response(question_resp)
                        
                        status_container.write("âœ“ ãƒ¡ã‚¿è³ªå•ç”Ÿæˆå®Œäº†")
                        with status_container.expander("ç”Ÿæˆã•ã‚ŒãŸãƒ¡ã‚¿è³ªå•", expanded=False):
                            st.markdown(questions_text)
                        
                        # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 1.5)
                        if question_resp.usage_metadata:
                            cost = calculate_cost(
                                model_id,
                                question_resp.usage_metadata.prompt_token_count,
                                question_resp.usage_metadata.candidates_token_count,
                            )
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += question_resp.usage_metadata.prompt_token_count
                            usage_stats["total_output_tokens"] += question_resp.usage_metadata.candidates_token_count

                    # --- Phase 2: çµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ---
                    status_container.write("Phase 2: çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                    
                    if enable_meta:
                        deep_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: æœ€çµ‚å›ç­”ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: èª¿æŸ»ãƒ¡ãƒ¢ã¨ãƒ¡ã‚¿è³ªå•ã¸ã®å›ç­”ã‚’æ ¹æ‹ ã¨ã—ã¦ã€æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**æ§‹æˆ**:
1. **æ·±æ˜ã‚Šè€ƒå¯Ÿ**ï¼ˆãƒ¡ã‚¿è³ªå•ã¸ã®å›ç­”ï¼‰
2. **çµè«–**ï¼ˆ2-3è¡Œï¼‰
3. **è©³ç´°ãªåˆ†æ**ï¼ˆèª¿æŸ»ãƒ¡ãƒ¢ã«åŸºã¥ãï¼‰
4. **è€ƒæ…®ã™ã¹ãè¦å› ã‚„ãƒªã‚¹ã‚¯**ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

**é‡è¦**: 
- æ–°ã—ã„äº‹å®Ÿã‚’å‹æ‰‹ã«ä½œã‚‰ãšã€èª¿æŸ»ãƒ¡ãƒ¢ã®ç¯„å›²å†…ã§æ¨è«–ã™ã‚‹ã“ã¨
- èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã“ã¨
"""
                    else:
                        deep_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: æœ€çµ‚å›ç­”ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**ã‚¿ã‚¹ã‚¯**: èª¿æŸ»ãƒ¡ãƒ¢ã‚’å”¯ä¸€ã®æ ¹æ‹ ã¨ã—ã¦ã€æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**æ§‹æˆ**:
1. **çµè«–**ï¼ˆ2-3è¡Œã§æ˜ç¢ºã«ï¼‰
2. **è©³ç´°ãªåˆ†æ**ï¼ˆèª¿æŸ»ãƒ¡ãƒ¢ã«åŸºã¥ãï¼‰
3. **è€ƒæ…®ã™ã¹ãè¦å› ã‚„ãƒªã‚¹ã‚¯**ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

**é‡è¦**: 
- æ–°ã—ã„äº‹å®Ÿã‚’å‹æ‰‹ã«ä½œã‚‰ãšã€èª¿æŸ»ãƒ¡ãƒ¢ã®ç¯„å›²å†…ã§æ¨è«–ã™ã‚‹ã“ã¨
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ï¼ˆæœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«åã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€æ—¥ä»˜ãªã©ï¼‰ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã“ã¨**
- å¤ã„æƒ…å ±ã¨æ–°ã—ã„æƒ…å ±ãŒæ··åœ¨ã™ã‚‹å ´åˆã¯ã€æ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã™ã‚‹ã“ã¨
"""
                    
                    synthesis_prompt_text = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {prompt}\n\n==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\n"
                    
                    if enable_meta and questions_text:
                        synthesis_prompt_text += f"==== ãƒ¡ã‚¿è³ªå•ä¸€è¦§ ====\n{questions_text}\n==== ãƒ¡ã‚¿è³ªå•ã“ã“ã¾ã§ ====\n\næŒ‡ç¤º:\n1. ã¾ãšã€ãƒ¡ã‚¿è³ªå• Q1ã€œQn ã«ä¸€ã¤ãšã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n2. ãã®ã†ãˆã§ã€ãã‚Œã‚‰ã®å›ç­”ã‚’è¸ã¾ãˆãŸã€å…¨ä½“ã¨ã—ã¦ã®çµè«–ãƒ»åˆ†æãƒ»ç¤ºå”†ã€ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                    else:
                        synthesis_prompt_text += "ä¸Šè¨˜ãƒ¡ãƒ¢ã‚’æ ¹æ‹ ã«ã€æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚**èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚**"

                    synthesis_contents = contents_for_model + [
                        types.Content(role="user", parts=[
                            types.Part(text=synthesis_prompt_text)
                        ])
                    ]
                    
                    synthesis_config = types.GenerateContentConfig(
                        temperature=0.3,
                        candidate_count=1,
                        tools=[],  # çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºã§ã¯æ¤œç´¢OFF
                        system_instruction=deep_instruction,
                    )
                    
                    synthesis_resp = client.models.generate_content(
                        model=model_id,
                        contents=synthesis_contents,
                        config=synthesis_config,
                    )
                    
                    draft_answer = extract_text_from_response(synthesis_resp)
                    
                    status_container.write("âœ“ çµ±åˆå®Œäº†")
                    
                    # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 2)
                    if synthesis_resp.usage_metadata:
                        cost = calculate_cost(
                            model_id,
                            synthesis_resp.usage_metadata.prompt_token_count,
                            synthesis_resp.usage_metadata.candidates_token_count,
                        )
                        st.session_state.session_cost += cost
                        usage_stats["total_cost_usd"] += cost
                        usage_stats["total_input_tokens"] += synthesis_resp.usage_metadata.prompt_token_count
                        usage_stats["total_output_tokens"] += synthesis_resp.usage_metadata.candidates_token_count
                    
                    # --- Phase 3: ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰ã®ã¿) ---
                    if enable_strict:
                        status_container.write("Phase 3: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œä¸­...")
                        

                        reviewer_instruction = base_system_instruction + """

**ã‚ãªãŸã®å½¹å‰²**: é¬¼è»æ›¹ãƒ¬ãƒ™ãƒ«ã®å³æ ¼ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢

**ã‚¿ã‚¹ã‚¯**: åˆç‰ˆå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’è¿”ã™ã€‚ãŸã ã—ã€**èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ã‚’å„ªå…ˆã—ã€æœ€æ–°æƒ…å ±ã‚’ç¶­æŒã™ã‚‹ã“ã¨**ã€‚

**ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦³ç‚¹**:
- äº‹å®Ÿã¨æ¨æ¸¬ã‚’æ˜ç¢ºã«åˆ†ã‘ã‚‹
- éåº¦ã«è‡ªä¿¡ã®ã‚ã‚‹æ–­å®šã‚’å¼±ã‚ã‚‹
- æ•°å­—ã‚„å›ºæœ‰åè©ãŒèª¿æŸ»ãƒ¡ãƒ¢ã¨çŸ›ç›¾ã—ã¦ã„ãªã„ã‹ç¢ºèª
- **èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°ã®æƒ…å ±ï¼ˆæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€æ—¥ä»˜ãªã©ï¼‰ãŒæ­£ã—ãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª**
- **å¤ã„æƒ…å ±ã§ä¸Šæ›¸ãã—ã¦ã„ãªã„ã‹ç¢ºèª**
- è¦‹è½ã¨ã—ã¦ã„ã‚‹é‡è¦ãªãƒªã‚¹ã‚¯ãƒ»ã‚·ãƒŠãƒªã‚ªãŒã‚ã‚Œã°è¿½åŠ 

**é‡è¦**: 
- èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ãŒæœ€æ–°ã§ã‚ã‚‹å ´åˆã€ãã‚Œã‚’å„ªå…ˆã™ã‚‹ã“ã¨
- ã‚ãªãŸã®çŸ¥è­˜ãŒå¤ã„å ´åˆã¯ã€èª¿æŸ»ãƒ¡ãƒ¢ã®æƒ…å ±ã‚’ä¿¡é ¼ã™ã‚‹ã“ã¨

**å‡ºåŠ›**: ä¿®æ­£ç‰ˆã®å›ç­”å…¨æ–‡ã®ã¿
"""

                        review_contents = [
                            types.Content(role="user", parts=[
                                types.Part.from_text(
                                    text=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {prompt}\n\n"
                                    f"==== èª¿æŸ»ãƒ¡ãƒ¢ ====\n{research_text}\n==== èª¿æŸ»ãƒ¡ãƒ¢ã“ã“ã¾ã§ ====\n\n"
                                    f"åˆç‰ˆå›ç­”:\n{draft_answer}\n\n"
                                    "ä¸Šè¨˜ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€å¿…è¦ãªã‚‰ä¿®æ­£ç‰ˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚**èª¿æŸ»ãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹æœ€æ–°æƒ…å ±ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚**"
                                )
                            ])
                        ]
                        
                        review_config = types.GenerateContentConfig(
                            temperature=0.1,
                            candidate_count=1,
                            system_instruction=reviewer_instruction,
                            thinking_config=types.ThinkingConfig(
                                thinking_level=types.ThinkingLevel.HIGH
                            ),
                        )
                        
                        review_resp = client.models.generate_content(
                            model=model_id,
                            contents=review_contents,
                            config=review_config,
                        )
                        
                        final_answer = extract_text_from_response(review_resp)
                        
                        status_container.write("âœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                        
                        # --- Phase 3b: Groké¬¼è»æ›¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ + é¬¼è»æ›¹ãƒ¢ãƒ¼ãƒ‰å…¨èˆ¬) ---
                        # å¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã§ã€ã‹ã¤é¬¼è»æ›¹ç³»ã®ãƒ¢ãƒ¼ãƒ‰ï¼ˆé¬¼è»æ›¹ã€ãƒ¡ã‚¿æ€è€ƒã€æœ¬æ°—MAXï¼‰ã§ç™ºå‹•
                        use_grok_reviewer = (mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)" and enable_strict)
                        
                        if use_grok_reviewer and OPENROUTER_API_KEY:
                            status_container.write("Phase 3b: Grok 4.1 Fast ã§æœ€çµ‚ãƒã‚§ãƒƒã‚¯ä¸­...")
                            try:
                                grok_answer = review_with_grok(prompt, final_answer)
                                # Grokä½¿ç”¨æ™‚ã¯ã€ãƒ¢ãƒ‡ãƒ«åã‚’æ˜ç¤º
                                final_answer = f"**ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: Gemini 3 Pro (High) â†’ Grok 4.1 Fast**\n**ãƒ¢ãƒ¼ãƒ‰: {response_mode}**\n\n---\n\n{grok_answer}"
                                status_container.write("âœ“ Grokæœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†")
                            except Exception as e:
                                status_container.write(f"âš  Grokãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                        else:
                            # Geminiã®ã¿ã®å ´åˆã‚‚ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤ºï¼ˆå¤šå±¤ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
                            if mode_category == "ğŸ¯ å›ç­”ãƒ¢ãƒ¼ãƒ‰(å¤šå±¤)":
                                final_answer = f"**ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: Gemini 3 Pro (High)**\n**ãƒ¢ãƒ¼ãƒ‰: {response_mode}**\n\n---\n\n{final_answer}"
                        
                        # --- ãƒ¡ã‚¿æ€è€ƒãƒ¢ãƒ¼ãƒ‰: çµè«–ã‚’å…ˆå‡ºã—ã™ã‚‹ ---
                        if "ãƒ¡ã‚¿æ€è€ƒ" in response_mode:
                            # çµè«–éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
                            # "çµè«–"ã‚„"ã¾ã¨ã‚"ãªã©ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã—ã¦å…ˆé ­ã«ç§»å‹•
                            lines = final_answer.split('\n')
                            conclusion_start = -1
                            for i, line in enumerate(lines):
                                if any(keyword in line for keyword in ['## çµè«–', '## ã¾ã¨ã‚', '**çµè«–**', '**ã¾ã¨ã‚**']):
                                    conclusion_start = i
                                    break
                            
                            if conclusion_start != -1:
                                # çµè«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ãŸå ´åˆã€ãã‚Œã‚’å…ˆé ­ã«ç§»å‹•
                                conclusion_section = []
                                other_content = lines[:conclusion_start]
                                
                                # çµè«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚ã‚ã‚Šã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆæ¬¡ã®##ã¾ã§ or æ–‡æœ«ï¼‰
                                conclusion_end = len(lines)
                                for i in range(conclusion_start + 1, len(lines)):
                                    if lines[i].startswith('## ') and i != conclusion_start:
                                        conclusion_end = i
                                        break
                                
                                conclusion_section = lines[conclusion_start:conclusion_end]
                                remaining_content = lines[conclusion_end:]
                                
                                # å†æ§‹æˆ: ãƒ¢ãƒ‡ãƒ«å â†’ çµè«– â†’ ãã®ä»–ã®è©³ç´°
                                # ãƒ¢ãƒ‡ãƒ«åéƒ¨åˆ†ã‚’ä¿æŒ
                                model_line = ""
                                if lines[0].startswith("**ğŸ¤–"):
                                    model_line = lines[0]
                                    other_content = lines[1:conclusion_start]
                                
                                final_answer = '\n'.join([
                                    model_line,
                                    "",
                                    "---",
                                    "",
                                    "## ğŸ“Œ çµè«–ï¼ˆå…ˆå‡ºã—ï¼‰",
                                    *conclusion_section[1:],  # å…ƒã®è¦‹å‡ºã—ã‚’é™¤ã
                                    "",
                                    "---",
                                    "",
                                    "## ğŸ“ è©³ç´°",
                                    *other_content,
                                    *remaining_content
                                ]).strip()
                        
                        with status_container.expander("åˆç‰ˆã¨ã®æ¯”è¼ƒ", expanded=False):
                            st.markdown("**åˆç‰ˆ:**")
                            st.markdown(draft_answer[:500] + "..." if len(draft_answer) > 500 else draft_answer)
                            st.markdown("**ä¿®æ­£ç‰ˆ:**")
                            st.markdown(final_answer[:500] + "..." if len(final_answer) > 500 else final_answer)
                        
                        # ã‚³ã‚¹ãƒˆè¨ˆç®— (Phase 3)
                        if review_resp.usage_metadata:
                            cost = calculate_cost(
                                model_id,
                                review_resp.usage_metadata.prompt_token_count,
                                review_resp.usage_metadata.candidates_token_count,
                            )
                            st.session_state.session_cost += cost
                            usage_stats["total_cost_usd"] += cost
                            usage_stats["total_input_tokens"] += review_resp.usage_metadata.prompt_token_count
                            usage_stats["total_output_tokens"] += review_resp.usage_metadata.candidates_token_count
                    else:
                        final_answer = draft_answer

                save_usage(usage_stats)
                status_container.update(label="å®Œäº†ï¼", state="complete", expanded=False)

                # ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤º
                st.caption(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_id}")
                st.markdown(final_answer)

                # ---- ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ± ----
                if grounding_metadata:
                    st.markdown("---")
                    with st.expander("ğŸ“š æƒ…å ±æºã¨å¼•ç”¨", expanded=False):
                        if grounding_metadata.grounding_chunks:
                            st.markdown("**æ¤œç´¢çµæœã‹ã‚‰åˆ©ç”¨ã—ãŸæƒ…å ±æº:**")
                            unique_sources = {}
                            import urllib.parse

                            for chunk in grounding_metadata.grounding_chunks:
                                if getattr(chunk, "web", None):
                                    uri = getattr(chunk.web, "uri", None)
                                    title = getattr(chunk.web, "title", "æƒ…å ±æº")
                                    if uri and uri not in unique_sources:
                                        parsed = urllib.parse.urlparse(uri)
                                        domain = parsed.netloc.replace("www.", "")
                                        unique_sources[uri] = {
                                            "title": title,
                                            "domain": domain,
                                        }
                            for i, (uri, info) in enumerate(unique_sources.items(), 1):
                                st.markdown(f"{i}. **[{info['title']}]({uri})**")
                                st.caption(f"   å‡ºå…¸: {info['domain']}")

                messages.append({"role": "model", "content": final_answer})
                update_current_session_messages(messages)

            except Exception as e:
                status_container.update(label="Error", state="error")
                err_text = str(e)
                if "RESOURCE_EXHAUSTED" in err_text or "429" in err_text:
                    st.error(
                        "âš ï¸ Vertex AI / Gemini ã®ã‚¯ã‚©ãƒ¼ã‚¿ã«é”ã—ã¾ã—ãŸã€‚\n\n"
                        "ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ / æ—¥æ¬¡åˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n"
                        "ãƒ»ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚\n"
                        "ãƒ»Google Cloud Console ã®ã€ŒVertex AI â†’ ä½¿ç”¨çŠ¶æ³ã€ã‹ã‚‰ã‚¯ã‚©ãƒ¼ã‚¿çŠ¶æ³ã‚’ç¢ºèªã§ãã¾ã™ã€‚"
                    )
                elif "NOT_FOUND" in err_text and "Publisher Model" in err_text:
                    st.error(
                        "âš ï¸ æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ / ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n"
                        "ãƒ»ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ¢ãƒ‡ãƒ«IDã‚’ã€2.5ç³» ã¾ãŸã¯ 3 Pro ã«å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚\n"
                    )
                else:
                    st.error(f"An error occurred: {e}")
