"""
JSON Intermediate Representation (IR) for research results.
Phase B: Structured data extraction from Phase 1 research.
"""

from typing import TypedDict, Literal, List, Optional
from datetime import datetime
import json


# ========================================
# Type Definitions
# ========================================

class FactIR(TypedDict):
    """Individual fact extracted from research"""
    statement: str  # Fact content
    source: Literal["web", "youtube", "model"]  # Source type
    source_detail: str  # Specific URL or model name
    date: Optional[str]  # Information date (YYYY-MM-DD format)
    confidence: Literal["high", "medium", "low", "unknown"]  # Confidence level


class OptionIR(TypedDict):
    """Option or alternative approach"""
    name: str  # Option name (e.g., "Plan A: On-premise migration")
    pros: List[str]  # Advantages
    cons: List[str]  # Disadvantages
    conditions: List[str]  # Conditions for success
    estimated_cost: Optional[str]  # Cost estimate if available


class RiskIR(TypedDict):
    """Risk identified during research"""
    statement: str  # Risk description
    severity: Literal["high", "medium", "low", "unknown"]  # Severity level
    timeframe: Literal["short", "medium", "long", "unknown"]  # Impact timeframe
    mitigation: Optional[str]  # Mitigation strategy if available


class UnknownIR(TypedDict):
    """Unknown or unclear point"""
    question: str  # Unknown question
    why_unknown: Literal[
        "insufficient_data",  # Insufficient data
        "conflicting_data",  # Conflicting data
        "grey_area",  # Grey area (legal, etc.)
        "future_dependent",  # Future-dependent
        "unknown"  # Unknown reason
    ]
    impact: Literal["high", "medium", "low", "unknown"]  # Impact of not knowing


class ResearchMetadataIR(TypedDict):
    """Metadata about the research process"""
    question: str  # Original user question
    language: str  # Language (e.g., "ja", "en")
    created_at: str  # ISO 8601 timestamp
    models: List[str]  # Models used in research
    sources_count: int  # Number of sources consulted
    search_queries: List[str]  # Search queries used


class ResearchIR(TypedDict):
    """Top-level research intermediate representation"""
    facts: List[FactIR]
    options: List[OptionIR]
    risks: List[RiskIR]
    unknowns: List[UnknownIR]
    metadata: ResearchMetadataIR


# ========================================
# Validation and Normalization
# ========================================

def validate_research_ir(ir: dict) -> tuple[ResearchIR, List[str]]:
    """
    Validate and normalize ResearchIR schema.
    
    Args:
        ir: Raw dictionary to validate
    
    Returns:
        Tuple of (normalized_ir, warnings)
        - normalized_ir: Validated and normalized ResearchIR
        - warnings: List of warning messages
    """
    warnings: List[str] = []
    
    # Ensure top-level keys exist
    normalized: ResearchIR = {
        "facts": ir.get("facts", []),
        "options": ir.get("options", []),
        "risks": ir.get("risks", []),
        "unknowns": ir.get("unknowns", []),
        "metadata": ir.get("metadata", {})
    }
    
    # Normalize facts
    normalized_facts: List[FactIR] = []
    for i, fact in enumerate(normalized["facts"]):
        if not isinstance(fact, dict):
            warnings.append(f"Fact {i} is not a dict, skipping")
            continue
        
        normalized_fact: FactIR = {
            "statement": str(fact.get("statement", "")),
            "source": fact.get("source", "model") if fact.get("source") in ["web", "youtube", "model"] else "model",
            "source_detail": str(fact.get("source_detail", "")),
            "date": fact.get("date"),
            "confidence": fact.get("confidence", "unknown") if fact.get("confidence") in ["high", "medium", "low", "unknown"] else "unknown"
        }
        
        if not normalized_fact["statement"]:
            warnings.append(f"Fact {i} has empty statement")
        
        normalized_facts.append(normalized_fact)
    
    normalized["facts"] = normalized_facts
    
    # Normalize options
    normalized_options: List[OptionIR] = []
    for i, option in enumerate(normalized["options"]):
        if not isinstance(option, dict):
            warnings.append(f"Option {i} is not a dict, skipping")
            continue
        
        normalized_option: OptionIR = {
            "name": str(option.get("name", f"Option {i+1}")),
            "pros": [str(p) for p in option.get("pros", [])],
"cons": [str(c) for c in option.get("cons", [])],
            "conditions": [str(c) for c in option.get("conditions", [])],
            "estimated_cost": option.get("estimated_cost")
        }
        
        normalized_options.append(normalized_option)
    
    normalized["options"] = normalized_options
    
    # Normalize risks
    normalized_risks: List[RiskIR] = []
    for i, risk in enumerate(normalized["risks"]):
        if not isinstance(risk, dict):
            warnings.append(f"Risk {i} is not a dict, skipping")
            continue
        
        normalized_risk: RiskIR = {
            "statement": str(risk.get("statement", "")),
            "severity": risk.get("severity", "unknown") if risk.get("severity") in ["high", "medium", "low", "unknown"] else "unknown",
            "timeframe": risk.get("timeframe", "unknown") if risk.get("timeframe") in ["short", "medium", "long", "unknown"] else "unknown",
            "mitigation": risk.get("mitigation")
        }
        
        if not normalized_risk["statement"]:
            warnings.append(f"Risk {i} has empty statement")
        
        normalized_risks.append(normalized_risk)
    
    normalized["risks"] = normalized_risks
    
    # Normalize unknowns
    normalized_unknowns: List[UnknownIR] = []
    for i, unknown in enumerate(normalized["unknowns"]):
        if not isinstance(unknown, dict):
            warnings.append(f"Unknown {i} is not a dict, skipping")
            continue
        
        valid_reasons = ["insufficient_data", "conflicting_data", "grey_area", "future_dependent", "unknown"]
        normalized_unknown: UnknownIR = {
            "question": str(unknown.get("question", "")),
            "why_unknown": unknown.get("why_unknown", "unknown") if unknown.get("why_unknown") in valid_reasons else "unknown",
            "impact": unknown.get("impact", "unknown") if unknown.get("impact") in ["high", "medium", "low", "unknown"] else "unknown"
        }
        
        if not normalized_unknown["question"]:
            warnings.append(f"Unknown {i} has empty question")
        
        normalized_unknowns.append(normalized_unknown)
    
    normalized["unknowns"] = normalized_unknowns
    
    # Normalize metadata
    metadata = normalized["metadata"]
    normalized_metadata: ResearchMetadataIR = {
        "question": str(metadata.get("question", "")),
        "language": str(metadata.get("language", "ja")),
        "created_at": str(metadata.get("created_at", datetime.now().isoformat())),
        "models": [str(m) for m in metadata.get("models", [])],
        "sources_count": int(metadata.get("sources_count", 0)),
        "search_queries": [str(q) for q in metadata.get("search_queries", [])]
    }
    
    normalized["metadata"] = normalized_metadata
    
    # Final validation
    if not normalized["facts"]:
        warnings.append("No facts extracted (empty facts list)")
    
    return normalized, warnings


# ========================================
# Synthesis Prompt Builder
# ========================================

def build_synthesis_prompt_from_ir(ir: ResearchIR, original_question: str) -> str:
    """
    Build Phase 2 synthesis prompt from JSON IR.
    
    Args:
        ir: Structured research IR
        original_question: Original user question
    
    Returns:
        Formatted prompt string for Phase 2 integration
    """
    
    # Facts section
    facts_section = "ã€ç¢ºèªã•ã‚ŒãŸäº‹å®Ÿã€‘\n"
    if ir["facts"]:
        confidence_marks = {
            "high": "âœ“",
            "medium": "â–³",
            "low": "?",
            "unknown": "Â·"
        }
        
        for fact in ir["facts"]:
            mark = confidence_marks.get(fact["confidence"], "Â·")
            facts_section += f"{mark} {fact['statement']}\n"
            if fact["source_detail"]:
                facts_section += f"  å‡ºå…¸: {fact['source_detail']} ({fact['confidence']}ä¿¡é ¼åº¦)\n"
            if fact.get("date"):
                facts_section += f"  æ—¥ä»˜: {fact['date']}\n"
    else:
        facts_section += "ï¼ˆæŠ½å‡ºã•ã‚ŒãŸäº‹å®Ÿãªã—ï¼‰\n"
    
    # Options section
    options_section = ""
    if ir["options"]:
        options_section = "\nã€æ¤œè¨ã™ã¹ãé¸æŠè‚¢ã€‘\n"
        for opt in ir["options"]:
            options_section += f"\n## {opt['name']}\n"
            if opt["pros"]:
                options_section += f"ãƒ¡ãƒªãƒƒãƒˆ: {', '.join(opt['pros'])}\n"
            if opt["cons"]:
                options_section += f"ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: {', '.join(opt['cons'])}\n"
            if opt["conditions"]:
                options_section += f"æˆç«‹æ¡ä»¶: {', '.join(opt['conditions'])}\n"
            if opt.get("estimated_cost"):
                options_section += f"ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š: {opt['estimated_cost']}\n"
    
    # Risks section
    risks_section = ""
    if ir["risks"]:
        risks_section = "\nã€ç‰¹å®šã•ã‚ŒãŸãƒªã‚¹ã‚¯ã€‘\n"
        severity_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢", "unknown": "âšª"}
        
        # Sort by severity
        severity_order = {"high": 0, "medium": 1, "low": 2, "unknown": 3}
        sorted_risks = sorted(ir["risks"], key=lambda x: severity_order.get(x["severity"], 3))
        
        for risk in sorted_risks:
            emoji = severity_emoji.get(risk["severity"], "âšª")
            timeframe_ja = {"short": "çŸ­æœŸ", "medium": "ä¸­æœŸ", "long": "é•·æœŸ", "unknown": "ä¸æ˜"}
            timeframe = timeframe_ja.get(risk["timeframe"], "ä¸æ˜")
            
            risks_section += f"{emoji} {risk['statement']} ({timeframe})\n"
            if risk.get("mitigation"):
                risks_section += f"  å¯¾ç­–: {risk['mitigation']}\n"
    
    # Unknowns section
    unknowns_section = ""
    if ir["unknowns"]:
        unknowns_section = "\nã€ä¸æ˜ç‚¹ãƒ»è¦ç¢ºèªäº‹é …ã€‘\n"
        reason_map = {
            "insufficient_data": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
            "conflicting_data": "æƒ…å ±ãŒçŸ›ç›¾",
            "grey_area": "ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³",
            "future_dependent": "å°†æ¥ã®çŠ¶æ³æ¬¡ç¬¬",
            "unknown": "ç†ç”±ä¸æ˜"
        }
        
        for unknown in ir["unknowns"]:
            reason = reason_map.get(unknown["why_unknown"], "ç†ç”±ä¸æ˜")
            unknowns_section += f"? {unknown['question']}\n"
            unknowns_section += f"  ç†ç”±: {reason}\n"
    
    # Metadata section
    metadata_section = f"""
ã€èª¿æŸ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€‘
- èª¿æŸ»æ—¥æ™‚: {ir['metadata']['created_at']}
- æƒ…å ±æºæ•°: {ir['metadata']['sources_count']}
- ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {', '.join(ir['metadata']['models'])}
"""
    
    # Final synthesis prompt
    synthesis_prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:
{original_question}

{facts_section}
{options_section}
{risks_section}
{unknowns_section}
{metadata_section}

ã€çµ±åˆã‚¿ã‚¹ã‚¯ã€‘
ä¸Šè¨˜ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«ã€æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªåˆ¶ç´„:**
1. ã€Œâœ“ é«˜ä¿¡é ¼åº¦ã€ã®äº‹å®Ÿã¯å¼·ãä¸»å¼µã§ãã¾ã™
2. ã€Œâ–³ ä¸­ä¿¡é ¼åº¦ã€ã€Œ? ä½ä¿¡é ¼åº¦ã€ã®äº‹å®Ÿã¯ã€Œã€œã¨ã•ã‚Œã‚‹ã€ã€Œã€œå¯èƒ½æ€§ãŒã‚ã‚‹ã€ã¨å¼±ã‚ã¦ãã ã•ã„
3. ã€Œä¸æ˜ç‚¹ã€ã«è©²å½“ã™ã‚‹äº‹é …ã¯ã€å‹æ‰‹ã«åŸ‹ã‚ãšã«ã€Œç¾æ™‚ç‚¹ã§ã¯ä¸æ˜ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„
4. ãƒªã‚¹ã‚¯ã¯æ·±åˆ»åº¦é †ï¼ˆğŸ”´â†’ğŸŸ¡â†’ğŸŸ¢ï¼‰ã«è¨€åŠã—ã¦ãã ã•ã„
5. å‡ºå…¸æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯é©å®œå‚ç…§ã—ã¦ãã ã•ã„
"""
    
    return synthesis_prompt
